Directory structure:
└── FF/
    ├── BMA.py
    ├── BMA_val.py
    ├── MAIN2.py
    ├── config.yml
    ├── errors.py
    ├── evaluate_validation.py
    ├── export_test_data.py
    ├── export_val_data.py
    ├── hp.py
    ├── huc.py
    ├── huc2_spatial_nse_uncertainty.py
    ├── kill_gpu.sh
    ├── losses.py
    ├── metrics.py
    ├── models.py
    ├── notes.txt
    ├── run_all_models.sh
    ├── run_evaluation.sh
    ├── spatial_uncertainty_analysis.py
    ├── streamflow.def
    ├── submit_jobs.sh
    ├── test_sig.py
    ├── utils.py
    ├── val_data/
    └── .ipynb_checkpoints/
        ├── BMA-checkpoint.py
        ├── BMA_val-checkpoint.py
        ├── MAIN2-checkpoint.py
        ├── config-checkpoint.yml
        ├── evaluate_validation-checkpoint.py
        ├── export_test_data-checkpoint.py
        ├── export_val_data-checkpoint.py
        ├── hp-checkpoint.py
        ├── huc2_spatial_nse_uncertainty-checkpoint.py
        ├── losses-checkpoint.py
        ├── metrics-checkpoint.py
        ├── notes-checkpoint.txt
        ├── run_all_models-checkpoint.sh
        ├── run_evaluation-checkpoint.sh
        ├── spatial_uncertainty_analysis-checkpoint.py
        └── submit_jobs-checkpoint.sh

================================================
File: BMA.py
================================================
import os
import glob
import pandas as pd
import numpy as np
import xarray as xr
from metrics import calculate_metrics, get_available_metrics  # Import metrics functions

# Base directory containing the model output directories
base_dir = '/scratch/kdahal3/camels_losses'

# Define the forecast horizons you want to process
forecast_horizons = [1, 7, 30]

# Define the metrics list, excluding Peak-Timing and Missed-Peaks
metrics_list = get_available_metrics()
metrics_list = [m for m in metrics_list if m not in ["Peak-Timing", "Missed-Peaks"]]

# Function to process data for a given forecast horizon
def process_forecast_horizon(forecast_horizon):
    print(f"\nProcessing Forecast Horizon: {forecast_horizon} day(s)")

    # Find all model directories ending with f'_{forecast_horizon}day'
    model_dirs = glob.glob(os.path.join(base_dir, f'*_{forecast_horizon}day'))

    if not model_dirs:
        print(f"No model directories found for forecast horizon {forecast_horizon} day(s).")
        return

    # Get a list of all station IDs from the CSV filenames in the first model directory
    first_model_dir = model_dirs[0]
    csv_pattern = 'camels_*_combined_forecast_vs_observed.csv'
    csv_files = glob.glob(os.path.join(first_model_dir, csv_pattern))

    if not csv_files:
        print(f"No CSV files found in the first model directory: {first_model_dir}")
        return

    station_ids = [os.path.basename(f).split('_')[1] for f in csv_files]

    # Prepare a DataFrame to store results
    bma_results = []

    # Keep track of all possible model names for consistent columns
    all_model_names = set()

    # Loop over each station
    for station_id in station_ids:
        print(f"\nProcessing Station ID: {station_id}")
        station_data = {}
        station_data['Station_ID'] = station_id
        model_forecasts = {}
        observed_data = None
        models_with_data = []

        # Loop over each model directory to collect forecasts for the station
        for model_dir in model_dirs:
            model_name = os.path.basename(model_dir)
            csv_filename = f'camels_{station_id}_combined_forecast_vs_observed.csv'
            csv_file_path = os.path.join(model_dir, csv_filename)

            if not os.path.isfile(csv_file_path):
                # Skip if the station data is not available in this model
                print(f"Model {model_name}: No data for station {station_id}")
                continue

            print(f"Reading data for model {model_name}, station {station_id}")
            # Read the CSV file
            df = pd.read_csv(csv_file_path)

            # For forecast horizons >1, select only the desired Forecast_Horizon
            if forecast_horizon > 1:
                df = df[df['Forecast_Horizon'] == forecast_horizon]
                if df.empty:
                    print(f"Warning: No data for Forecast_Horizon={forecast_horizon} in station {station_id}, model {model_name}")
                    continue

            # Convert 'Date' column to datetime
            df['Date'] = pd.to_datetime(df['Date'])
            df = df.sort_values('Date').reset_index(drop=True)

            # Store the observed data (assuming it's the same across models)
            if observed_data is None:
                observed_data = df[['Date', 'Observed']].copy()
            else:
                # Check if observed data matches
                if not observed_data['Observed'].equals(df['Observed']):
                    print(f"Warning: Observed data mismatch for station {station_id} in model {model_name}")

            # Store the model's forecast
            model_forecasts[model_name] = df['Forecasted'].values
            models_with_data.append(model_name)
            all_model_names.add(model_name)  # Add to the set of all model names

        if not models_with_data:
            # No models have data for this station
            print(f"No models have data for station {station_id} at Forecast_Horizon={forecast_horizon}")
            continue

        # Create a DataFrame with observed and forecasts
        combined_df = observed_data.copy()
        for model_name in models_with_data:
            combined_df[model_name] = model_forecasts[model_name]

        # Remove any rows with NaN values (if any models are missing forecasts for certain dates)
        combined_df = combined_df.dropna()

        # Extract observed and forecasts
        observed = combined_df['Observed'].values
        forecasts = combined_df[models_with_data].values  # Shape: (n_samples, n_models)

        print(f"Number of samples: {len(observed)}")
        print(f"Models with data: {models_with_data}")

        n_models = forecasts.shape[1]
        n_samples = forecasts.shape[0]

        # Initialize a list to store NSE values for weighting
        nse_values = []
        model_metrics = {}

        # Calculate metrics for individual models
        for idx, model_name in enumerate(models_with_data):
            model_forecast = forecasts[:, idx]

            # Convert to xarray DataArrays for metric calculations
            obs_da = xr.DataArray(observed)
            sim_da = xr.DataArray(model_forecast)

            # Calculate metrics
            print(f"Calculating metrics for model {model_name}")
            metrics = calculate_metrics(obs_da, sim_da, metrics=metrics_list)

            # Store metrics
            for metric_name, metric_value in metrics.items():
                station_data[f'{metric_name}_{model_name}'] = metric_value

            # Store NSE value for weighting
            nse_model = metrics['NSE']
            nse_values.append(nse_model)
            model_metrics[model_name] = metrics

        # Convert NSE values to a numpy array for calculations
        nse_values = np.array(nse_values)

        # Implement proper Bayesian Model Averaging

        # Calculate prior probabilities (uniform priors)
        prior_probabilities = np.full(n_models, 1 / n_models)

        # Compute log-likelihoods for each model
        log_likelihoods = []
        for idx in range(n_models):
            residuals = observed - forecasts[:, idx]
            sigma_k_squared = np.var(residuals, ddof=1)
            n = len(observed)
            # Handle case where sigma_k_squared is zero
            if sigma_k_squared == 0:
                # Assign a very small variance
                sigma_k_squared = 1e-6
            log_likelihood = -0.5 * n * np.log(2 * np.pi * sigma_k_squared) - (np.sum(residuals ** 2) / (2 * sigma_k_squared))
            log_likelihoods.append(log_likelihood)

        log_likelihoods = np.array(log_likelihoods)

        # Compute log posterior probabilities (since priors are equal, they cancel out in log space)
        log_posteriors = log_likelihoods

        # To prevent numerical underflow, subtract the maximum log posterior
        max_log_posterior = np.max(log_posteriors)
        log_posteriors -= max_log_posterior

        # Convert to posterior probabilities
        posteriors = np.exp(log_posteriors)
        posterior_probabilities = posteriors / np.sum(posteriors)

        print(f"Posterior probabilities for models: {dict(zip(models_with_data, posterior_probabilities))}")

        # Compute the BMA forecast
        bma_forecast = np.dot(forecasts, posterior_probabilities)

        # Convert to xarray DataArrays for metric calculations
        bma_forecast_da = xr.DataArray(bma_forecast)
        obs_da = xr.DataArray(observed)

        # Calculate metrics for BMA forecast
        print("Calculating metrics for BMA forecast")
        metrics_bma = calculate_metrics(obs_da, bma_forecast_da, metrics=metrics_list)
        for metric_name, metric_value in metrics_bma.items():
            station_data[f'{metric_name}_BMA'] = metric_value

        # Store the posterior probabilities as weights
        for idx, model_name in enumerate(models_with_data):
            station_data[f'Weight_{model_name}'] = posterior_probabilities[idx]

        # Ensure that weights for all possible models are included, even if zero
        for model_name in all_model_names:
            if f'Weight_{model_name}' not in station_data:
                station_data[f'Weight_{model_name}'] = 0.0

        # Calculate Mean Forecast
        mean_forecast = forecasts.mean(axis=1)

        # Convert to xarray DataArrays for metric calculations
        mean_forecast_da = xr.DataArray(mean_forecast)
        obs_da = xr.DataArray(observed)

        # Calculate metrics for Mean Forecast
        print("Calculating metrics for Mean Forecast")
        metrics_mean = calculate_metrics(obs_da, mean_forecast_da, metrics=metrics_list)
        for metric_name, metric_value in metrics_mean.items():
            station_data[f'{metric_name}_Mean_Forecast'] = metric_value

        # Calculate actual coverage based on observed data and forecast range (min-max bounds from all models)
        lower_bound_bma = forecasts.min(axis=1)
        upper_bound_bma = forecasts.max(axis=1)
        actual_coverage = np.sum((observed >= lower_bound_bma) & (observed <= upper_bound_bma)) / n_samples
        station_data['Actual_Coverage_BMA'] = actual_coverage

        # Append the results
        bma_results.append(station_data)

    if not bma_results:
        print(f"No results to display for Forecast_Horizon={forecast_horizon} day(s).")
        return

    # Create a DataFrame from the results, ensuring all columns are included
    bma_results_df = pd.DataFrame(bma_results)

    # Display the results
    print(bma_results_df)

    # Optionally, save the results to a CSV file
    output_filename = f'bma_results_{forecast_horizon}day_with_metrics.csv'
    bma_results_df.to_csv(output_filename, index=False)
    print(f"Results saved to {output_filename}")

# Process each forecast horizon
for fh in forecast_horizons:
    process_forecast_horizon(fh)



================================================
File: BMA_val.py
================================================



================================================
File: MAIN2.py
================================================
#MAIN2.py

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress INFO and WARNING logs from TensorFlow

import warnings
warnings.filterwarnings('ignore')  # Suppress all Python warnings

import logging
logging.getLogger('tensorflow').setLevel(logging.ERROR)  # Suppress INFO and WARNING logs from TensorFlow

import yaml
import argparse
import glob
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from keras.callbacks import EarlyStopping, TensorBoard, LearningRateScheduler
import datetime
import gc
from keras import backend as K
from keras import mixed_precision
import xarray as xr

import losses
import models
from metrics import calculate_metrics, get_available_metrics

# Enable mixed precision for memory efficiency
mixed_precision.set_global_policy('mixed_float16')

# Enable GPU memory growth to avoid pre-allocating all GPU memory at once
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# Learning rate schedule
def lr_schedule(epoch, lr):
    if epoch < 20:
        return 1e-3
    elif 20 <= epoch < 25:
        return 5e-4
    else:
        return 1e-4

# Create sequences from the data
def create_sequences(data, n_steps_in, n_steps_out):
    X, y = [], []
    for i in range(len(data)):
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out
        if out_end_ix > len(data):
            break
        seq_x, seq_y = data[i:end_ix], data[end_ix:out_end_ix, -1]  # Assuming the last column is the target
        X.append(seq_x)
        y.append(seq_y)
    return np.array(X), np.array(y)

def create_time_sequences(data, n_steps_in, n_steps_out):
    X = []
    for i in range(len(data)):
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out
        if out_end_ix > len(data):
            break
        seq_x = data[i:end_ix]
        X.append(seq_x)
    return np.array(X)

# Parse command-line arguments
parser = argparse.ArgumentParser(description='Run forecast with a specific config file.')
parser.add_argument('--config', type=str, default='config.yml', help='Path to the config file.')
args = parser.parse_args()

# Load the specified configuration file
with open(args.config, 'r') as f:
    config = yaml.safe_load(f)

# Create output directory based on the output_prefix
output_dir = config['output_prefix']
os.makedirs(output_dir, exist_ok=True)

# Prepare directory for saving evaluation metrics
evaluation_folder = os.path.join(output_dir, config.get('evaluation_folder', 'evaluation_metrics'))
os.makedirs(evaluation_folder, exist_ok=True)

# Helper function to load and process datasets
def load_and_process_data(file_paths, for_transfer_learning=False):
    data_list = []
    for path in file_paths:
        if os.path.isdir(path):
            csv_files = glob.glob(os.path.join(path, "*.csv"))
        else:
            csv_files = [path]

        for csv_file in csv_files:
            dataset = pd.read_csv(csv_file, header=0, index_col=0)
            dataset.index = pd.to_datetime(dataset.index)
            dataset.replace('Min', np.nan, inplace=True)
            dataset = dataset.asfreq('D')
            dataset = dataset.apply(pd.to_numeric, errors='coerce')
            dataset = dataset.loc[dataset['streamflow'].first_valid_index():]
            dataset = dataset.loc[:dataset['streamflow'].last_valid_index()]
            dataset = dataset.interpolate(method='linear').ffill().bfill()
            if for_transfer_learning:
                data_list.append(dataset)
            else:
                data_list.append((csv_file, dataset))
    return data_list

# Load main datasets
print("Loading main datasets...")
main_datasets = load_and_process_data(config['data_paths'])

# Load transfer learning datasets (if applicable)
if 'transfer_learning_paths' in config:
    print("Loading transfer learning datasets...")
    transfer_datasets = load_and_process_data(config['transfer_learning_paths'], for_transfer_learning=True)
else:
    transfer_datasets = []

# Identify all feature names (excluding 'streamflow') across all main datasets
print("Identifying all feature names...")
all_features = set()
for _, dataset in main_datasets:
    all_features.update([col for col in dataset.columns if col != 'streamflow'])
all_features = sorted(all_features)  # Ensure consistent ordering
number_of_features = len(all_features) + 1  # +1 for 'streamflow' column

# Compute global min and max for each column across all main and transfer learning datasets
print("Computing global min and max for each column...")
min_max_dict = {}
# Include both main and transfer datasets for scaling
for dataset in main_datasets:
    _, ds = dataset
    for col in ds.columns:
        col_min = ds[col].min()
        col_max = ds[col].max()
        if col not in min_max_dict:
            min_max_dict[col] = {'min': col_min, 'max': col_max}
        else:
            if col_min < min_max_dict[col]['min']:
                min_max_dict[col]['min'] = col_min
            if col_max > min_max_dict[col]['max']:
                min_max_dict[col]['max'] = col_max

for ds in transfer_datasets:
    for col in ds.columns:
        col_min = ds[col].min()
        col_max = ds[col].max()
        if col not in min_max_dict:
            min_max_dict[col] = {'min': col_min, 'max': col_max}
        else:
            if col_min < min_max_dict[col]['min']:
                min_max_dict[col]['min'] = col_min
            if col_max > min_max_dict[col]['max']:
                min_max_dict[col]['max'] = col_max

# Convert min_max_dict to a DataFrame and save as CSV
min_max_df = pd.DataFrame(min_max_dict).T  # Transpose for easier CSV format
min_max_csv_path = os.path.join(output_dir, 'min_max.csv')
min_max_df.to_csv(min_max_csv_path)
print(f"Global min and max saved to {min_max_csv_path}")

# Function to scale data using global min and max
def scale_data(df, min_max_dict):
    scaled_df = pd.DataFrame(index=df.index)
    for col in df.columns:
        min_val = min_max_dict[col]['min']
        max_val = min_max_dict[col]['max']
        if max_val > min_val:
            scaled_df[col] = (df[col] - min_val) / (max_val - min_val)
        else:
            scaled_df[col] = 0.0  # Handle case where min == max to avoid division by zero
    return scaled_df

# Data generator for tf.data.Dataset
def data_generator(datasets, n_steps_in, n_steps_out, min_max_dict, features, split='train'):
    for dataset_info in datasets:
        csv_file, dataset = dataset_info
        station_name = os.path.splitext(os.path.basename(csv_file))[0] if csv_file else "transfer_learning"
        print(f"Processing station: {station_name} for split: {split}")

        # Ensure the dataset has all required features
        missing_features = set(features) - set(dataset.columns)
        if missing_features:
            print(f"Warning: Missing features {missing_features} in station {station_name}. Filling with NaN.")
            for mf in missing_features:
                dataset[mf] = np.nan

        # Reorder the dataset columns to match all_features + 'streamflow'
        dataset = dataset[features + ['streamflow']]

        # Extract time features
        dataset['year'] = dataset.index.year
        dataset['month'] = dataset.index.month
        dataset['day'] = dataset.index.day
        dataset['day_of_week'] = dataset.index.dayofweek + 1  # Monday=1, Sunday=7
        dataset['day_of_year'] = dataset.index.dayofyear

        time_features = ['year', 'month', 'day', 'day_of_week', 'day_of_year']

        # Scale all features using global min and max
        scaled_features = scale_data(dataset[features], min_max_dict)
        # Scale the target variable (streamflow) using global min and max
        scaled_target = scale_data(dataset[['streamflow']], min_max_dict)

        # Combine scaled features and target
        scaled = np.hstack((scaled_features.values, scaled_target.values))  # 'streamflow' is the last column

        # Prepare time features (without scaling, as temporal encoding will handle it)
        time_data = dataset[time_features].values  # Shape: (num_samples, num_time_features)

        # Create sequences
        X, y = create_sequences(scaled, n_steps_in, n_steps_out)
        time_sequences = create_time_sequences(time_data, n_steps_in, n_steps_out)  # Updated call

        # Determine split indices
        total_sequences = len(X)
        train_end = int(total_sequences * 0.7)
        val_end = int(total_sequences * 0.8)

        if split == 'train':
            sequences_X = X[:train_end]
            sequences_y = y[:train_end]
            sequences_time = time_sequences[:train_end]
        elif split == 'val':
            sequences_X = X[train_end:val_end]
            sequences_y = y[train_end:val_end]
            sequences_time = time_sequences[train_end:val_end]
        else:
            continue  # Skip processing for 'test' split

        # Yield data for the specified split
        for i in range(len(sequences_X)):
            yield (
                (sequences_X[i].astype(np.float32), sequences_time[i].astype(np.float32)),
                sequences_y[i].astype(np.float32)
            )  # Ensure float32 dtype for TensorFlow

# Create separate generators for training and validation
train_generator = lambda: data_generator(
    [(None, ds) for ds in transfer_datasets] + main_datasets,  # Include transfer datasets in train split
    config['n_steps_in'],
    config['n_steps_out'],
    min_max_dict,
    all_features,
    split='train'
)

val_generator = lambda: data_generator(
    [(None, ds) for ds in transfer_datasets] + main_datasets,  # Include transfer datasets in val split
    config['n_steps_in'],
    config['n_steps_out'],
    min_max_dict,
    all_features,
    split='val'
)

# Create dataset from generator for training
train_dataset = tf.data.Dataset.from_generator(
    train_generator,
    output_signature=(
        (
            tf.TensorSpec(shape=(config['n_steps_in'], number_of_features), dtype=tf.float32),  # Input features
            tf.TensorSpec(shape=(config['n_steps_in'], 5), dtype=tf.float32)  # Time features
        ),
        tf.TensorSpec(shape=(config['n_steps_out'],), dtype=tf.float32)  # Target
    )
)

# Create dataset from generator for validation
val_dataset = tf.data.Dataset.from_generator(
    val_generator,
    output_signature=(
        (
            tf.TensorSpec(shape=(config['n_steps_in'], number_of_features), dtype=tf.float32),  # Input features
            tf.TensorSpec(shape=(config['n_steps_in'], 5), dtype=tf.float32)  # Time features
        ),
        tf.TensorSpec(shape=(config['n_steps_out'],), dtype=tf.float32)  # Target
    )
)

# Shuffle, batch, and prefetch the training dataset
train_dataset = train_dataset.shuffle(buffer_size=10000) \
                             .batch(config['batch_size']) \
                             .prefetch(tf.data.AUTOTUNE)

# Batch and prefetch the validation dataset
val_dataset = val_dataset.batch(config['batch_size']) \
                         .prefetch(tf.data.AUTOTUNE)

# Build the model using models.py
output_prefix = config['output_prefix']  # e.g., "run_1"
log_dir = os.path.join(
    "logs",
    "fit",
    f"{output_prefix}_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}"
)

# Callbacks
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)
early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
lr_scheduler = LearningRateScheduler(lr_schedule)  # Uncomment if using learning rate scheduler

# Build the model using models.py
model = models.get_model(config, number_of_features)

# Get loss function and metrics
loss_function = losses.get_loss_function(
    config['loss_function'], config.get('loss_params', {})
)
metrics = [
    losses.get_metric_function(metric_name)
    for metric_name in config.get('metrics', [])
]

# Compile the model
learning_rate = config.get('learning_rate', 0.001)
optimizer_name = config.get('optimizer', 'Adam').lower()
clipvalue = config.get('clipvalue', 1.0)

if optimizer_name == 'adam':
    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipvalue=clipvalue)
elif optimizer_name == 'sgd':
    opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)
else:
    raise ValueError(f"Unsupported optimizer: {optimizer_name}")

model.compile(optimizer=opt, loss=loss_function, metrics=metrics)

model.summary()

# Train the model using the separate training and validation datasets
history = model.fit(
    train_dataset,
    epochs=config['epochs'],
    validation_data=val_dataset,
    verbose=1,
    callbacks=[early_stopping, tensorboard_callback]#, lr_scheduler]  # Add lr_scheduler if needed
)

# Save the model if required
if config.get('export_model', False):
    model_path = os.path.join(output_dir, "combined_model.keras")
    model.save(model_path)
    print(f"Model saved at {model_path}")

# Clear GPU memory
gc.collect()
K.clear_session()

# ========================= EVALUATION ================================
# Load test data
test_data_folder = config.get('test_data_folder', 'test_data')
test_csv_files = glob.glob(os.path.join(test_data_folder, "*_test_data.csv"))

for test_csv_file in test_csv_files:
    station_name = os.path.splitext(os.path.basename(test_csv_file))[0].replace('_test_data', '')
    print(f"Testing on station: {station_name}")

    test_dataset = pd.read_csv(test_csv_file, header=0, index_col=0)
    test_dataset.index = pd.to_datetime(test_dataset.index)

    # Ensure the dataset has all required features
    missing_features = set(all_features + ['streamflow']) - set(test_dataset.columns)
    if missing_features:
        print(f"Warning: Missing features {missing_features} in station {station_name}. Filling with NaN.")
        for mf in missing_features:
            test_dataset[mf] = np.nan

    # Reorder the dataset columns to match all_features + 'streamflow'
    test_dataset = test_dataset[all_features + ['streamflow']]

    # Extract time features
    test_dataset['year'] = test_dataset.index.year
    test_dataset['month'] = test_dataset.index.month
    test_dataset['day'] = test_dataset.index.day
    test_dataset['day_of_week'] = test_dataset.index.dayofweek + 1
    test_dataset['day_of_year'] = test_dataset.index.dayofyear
    time_features = ['year', 'month', 'day', 'day_of_week', 'day_of_year']
    time_data = test_dataset[time_features].values

    # Scale all features using global min and max
    scaled_features = scale_data(test_dataset[all_features], min_max_dict)
    # Scale the target variable (streamflow) using global min and max
    scaled_target = scale_data(test_dataset[['streamflow']], min_max_dict)

    # Combine scaled features and target
    scaled = np.hstack((scaled_features.values, scaled_target.values))  # 'streamflow' is the last column

    # Create sequences for LSTM
    test_X, test_y = create_sequences(scaled, config['n_steps_in'], config['n_steps_out'])
    test_time_sequences = create_time_sequences(time_data, config['n_steps_in'], config['n_steps_out'])

    # Create a tf.data.Dataset for testing
    test_dataset_tf = tf.data.Dataset.from_tensor_slices(
        ((test_X.astype(np.float32), test_time_sequences.astype(np.float32)), test_y.astype(np.float32))
    )
    test_dataset_tf = test_dataset_tf.batch(config['batch_size']).prefetch(tf.data.AUTOTUNE)

    # Predict on the test dataset
    y_pred = model.predict(test_dataset_tf)

    # Inverse transform the predictions and actual values using global min and max
    # Assuming 'streamflow' is the last column in min_max_dict
    streamflow_min = min_max_dict['streamflow']['min']
    streamflow_max = min_max_dict['streamflow']['max']
    y_pred_inv = y_pred * (streamflow_max - streamflow_min) + streamflow_min
    test_y_inv = test_y * (streamflow_max - streamflow_min) + streamflow_min

    # Ensure that the lengths of y_pred_inv and test_y_inv match before constructing the DataFrame
    min_length = min(len(y_pred_inv), len(test_y_inv))
    y_pred_inv = y_pred_inv[:min_length]  # Truncate to the minimum length
    test_y_inv = test_y_inv[:min_length]  # Truncate to the minimum length

    # Create the date range for the test data
    test_start_date = test_dataset.index[config['n_steps_in'] + config['n_steps_out'] - 1]
    test_dates = pd.date_range(start=test_start_date, periods=min_length)

    # Repeat each date for each forecast horizon and shift by the horizon
    forecast_horizons = np.arange(1, config['n_steps_out'] + 1)  # e.g., [1, 2, 3] for n_steps_out=3
    repeated_dates = np.repeat(test_dates, config['n_steps_out'])
    shifted_dates = repeated_dates + pd.to_timedelta(np.tile(forecast_horizons, min_length), unit='D')

    # Flatten the observed and forecasted values
    forecasted_values = y_pred_inv.flatten()
    observed_values = test_y_inv.flatten()

    # Construct DataFrame with correctly aligned dates
    df_result = pd.DataFrame({
        'Date': shifted_dates,
        'Forecast_Horizon': np.tile(forecast_horizons, min_length),
        'Observed': observed_values,
        'Forecasted': forecasted_values
    })

    # Save the forecast vs observed results to a CSV file
    result_csv_path = os.path.join(output_dir, f"{station_name}_forecast_vs_observed.csv")
    df_result.to_csv(result_csv_path, index=False)
    print(f"Results saved for {station_name} at {result_csv_path}")

    # ======================= METRICS CALCULATION ===========================
    print(f"Calculating metrics for station: {station_name}")

    # Convert to xarray DataArrays for metric calculations
    y_pred_da = xr.DataArray(y_pred_inv)
    test_y_da = xr.DataArray(test_y_inv)

    # Define the metrics list, excluding Peak-Timing and Missed-Peaks
    metrics_list = get_available_metrics()
    metrics_list = [m for m in metrics_list if m not in ["Peak-Timing", "Missed-Peaks"]]

    # Dictionary to store metrics per forecast day
    metrics_per_day = {}

    # Calculate metrics for each forecast day
    for day in range(config['n_steps_out']):
        # Extract the observations and simulations for the current forecast horizon
        obs = test_y_da[:, day] if test_y_da.ndim > 1 else test_y_da
        sim = y_pred_da[:, day] if y_pred_da.ndim > 1 else y_pred_da

        # Calculate metrics
        day_metrics = calculate_metrics(obs, sim, metrics=metrics_list)
        metrics_per_day[day + 1] = day_metrics  # Store metrics for each forecast day

    # Convert metrics_per_day into a DataFrame and save as CSV
    metrics_df = pd.DataFrame(metrics_per_day).T  # Transpose for better readability
    metrics_csv_path = os.path.join(evaluation_folder, f"{station_name}_daywise_metrics.csv")
    metrics_df.to_csv(metrics_csv_path, index=True)
    print(f"Metrics saved for {station_name} at {metrics_csv_path}")



================================================
File: config.yml
================================================
# config.yml

data_paths:
  - "/scratch/kdahal3/Caravan-stat-dyna/camels"

output_prefix: "test-run"

n_steps_in: 365  # Number of input steps
n_steps_out: 1   # Number of output steps

epochs: 150        # Number of training epochs
batch_size: 256   # Batch size for training
learning_rate: 0.0001  # Initial learning rate

# # Uncomment and add paths as needed
# transfer_learning_paths:
#   - "/scratch/kdahal3/Caravan-stat-dyna/camels"

export_sequences: false
export_model: true
save_plots: true
evaluation_folder: "evaluation_metrics"  # Folder to store evaluation metrics for each CSV

optimizer: 'Adam'

model_type: "LSTM"  # Options: "GRU", "LSTM", "TRANSFORMER", "CNN_RNN", "SEQ2SEQ" etc.

loss_function: "nse_loss"  # Options: "nse_loss", "kge_loss", "weighted_mse_loss", "huber_loss", "quantile_loss", "expectile_loss", "mae_loss"

# # Parameters for loss functions
# loss_params:
#   expectile: 0.15  # Used for expectile_loss

metrics:
  - 'kge_metric'
  - 'nse_metric'

test_data_folder: "test_data"
export_test_data: True  # Set to False if you don't want to export test data

model_params:
  num_time_features: 5 #useful for transformers only
  num_heads: 4
  head_size: 32
  ff_dim: 128
  num_transformer_blocks: 4
  dropout_rate: 0.4
  mlp_units: [128]
  mlp_dropout_rate: 0.4
  hidden_dim: 128
  time_feature_sizes: [2020, 12, 31, 7, 366]  # Adjust these values according to your data



================================================
File: errors.py
================================================
class NoTrainDataError(Exception):
    """Raised, when basin contains no valid samples in training period"""

class NoEvaluationDataError(Exception):
    """Raised, when basin contains no valid samples in validation or test period""" 

class AllNaNError(Exception):
    """Raised by `calculate_(all_)metrics` if all observations or all simulations are NaN. """



================================================
File: evaluate_validation.py
================================================
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import warnings
warnings.filterwarnings('ignore')

import logging
logging.getLogger('tensorflow').setLevel(logging.ERROR)

import yaml
import argparse
import glob
import numpy as np
import pandas as pd
import tensorflow as tf
from keras import backend as K
from keras import mixed_precision
import xarray as xr

# Import all custom objects the models were trained with.
from losses import (QuantileLoss, KgeLoss, NseLoss, ExpectileLoss, 
                    HuberLoss, WeightedMSELoss, MaeLoss, LogNseLoss,
                    kge_metric, nse_metric)

from metrics import calculate_metrics, get_available_metrics

mixed_precision.set_global_policy('mixed_float16')

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# --- Helper Functions (No changes) ---
def create_sequences(data, n_steps_in, n_steps_out):
    X, y = [], []
    for i in range(len(data)):
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out
        if out_end_ix > len(data):
            break
        seq_x, seq_y = data[i:end_ix], data[end_ix:out_end_ix, -1]
        X.append(seq_x)
        y.append(seq_y)
    return np.array(X), np.array(y)

def create_time_sequences(data, n_steps_in, n_steps_out):
    X = []
    for i in range(len(data)):
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out
        if out_end_ix > len(data):
            break
        seq_x = data[i:end_ix]
        X.append(seq_x)
    return np.array(X)

def scale_data(df, min_max_dict):
    scaled_df = pd.DataFrame(index=df.index)
    for col in df.columns:
        if col in min_max_dict:
            min_val = min_max_dict[col]['min']
            max_val = min_max_dict[col]['max']
            if max_val > min_val:
                scaled_df[col] = (df[col] - min_val) / (max_val - min_val)
            else:
                scaled_df[col] = 0.0
        else:
            print(f"Warning: Column '{col}' not found in min_max dictionary. Skipping scaling.")
            scaled_df[col] = df[col]
    return scaled_df

# --- Main Evaluation Script ---
def main():
    parser = argparse.ArgumentParser(description='Evaluate a pre-trained model on validation data.')
    parser.add_argument('--config', type=str, required=True, help='Path to the config file used for training.')
    parser.add_argument('--model-path', type=str, required=True, help='Path to the saved .keras model file.')
    parser.add_argument('--min-max-path', type=str, required=True, help='Path to the min_max.csv file from training.')
    parser.add_argument('--val-data-folder', type=str, required=True, help='Path to the folder containing validation CSV files.')
    parser.add_argument('--output-dir', type=str, required=True, help='Directory to save evaluation results and metrics.')
    args = parser.parse_args()

    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)

    os.makedirs(args.output_dir, exist_ok=True)
    evaluation_folder = os.path.join(args.output_dir, 'evaluation_metrics')
    os.makedirs(evaluation_folder, exist_ok=True)

    print(f"Loading min/max scaling values from {args.min_max_path}...")
    min_max_df = pd.read_csv(args.min_max_path, index_col=0)
    min_max_dict = min_max_df.to_dict('index')

    all_features = [col for col in min_max_df.index if col != 'streamflow']
    print(f"Using {len(all_features)} features for evaluation.")

    print(f"Loading model from {args.model_path}...")
    
    # ==================================================================
    # === FIX STARTS HERE: Corrected Monkey Patching Logic           ===
    # ==================================================================
    custom_objects = {
        "QuantileLoss": QuantileLoss, "ExpectileLoss": ExpectileLoss,
        "HuberLoss": HuberLoss, "NseLoss": NseLoss, "KgeLoss": KgeLoss,
        "WeightedMSELoss": WeightedMSELoss, "MaeLoss": MaeLoss, "LogNseLoss": LogNseLoss,
        "kge_metric": kge_metric, "nse_metric": nse_metric
    }

    # --- Monkey Patching Section ---
    original_from_configs = {}
    classes_to_patch_from_config = [QuantileLoss, ExpectileLoss, HuberLoss]
    
    for loss_class in classes_to_patch_from_config:
        if hasattr(loss_class, 'from_config'):
            original_from_configs[loss_class.__name__] = loss_class.from_config
            
            @classmethod
            def patched_from_config(cls, config, original_method=original_from_configs[loss_class.__name__]):
                config.pop('reduction', None)
                return original_method(config)
            
            loss_class.from_config = patched_from_config

    # Corrected patching for classes without from_config (like NseLoss, KgeLoss)
    original_inits = {}
    classes_to_patch_init = [NseLoss, KgeLoss] 
    
    def create_patched_init(original_init):
        def patched_init(self, *args, **kwargs):
            kwargs.pop('reduction', None)
            original_init(self, *args, **kwargs)
        return patched_init

    for loss_class in classes_to_patch_init:
        original_inits[loss_class.__name__] = loss_class.__init__
        loss_class.__init__ = create_patched_init(original_inits[loss_class.__name__])

    # Load the model with the patched classes
    model = tf.keras.models.load_model(args.model_path, custom_objects=custom_objects)
    
    # Restore original methods
    for loss_class in classes_to_patch_from_config:
        if loss_class.__name__ in original_from_configs:
            loss_class.from_config = original_from_configs[loss_class.__name__]
    for loss_class in classes_to_patch_init:
        if loss_class.__name__ in original_inits:
            loss_class.__init__ = original_inits[loss_class.__name__]
    # ==================================================================
    # === FIX ENDS HERE ================================================
    
    model.summary()

    val_csv_files = glob.glob(os.path.join(args.val_data_folder, "*.csv"))
    if not val_csv_files:
        print(f"Error: No CSV files found in {args.val_data_folder}")
        return

    for val_csv_file in val_csv_files:
        station_name = os.path.splitext(os.path.basename(val_csv_file))[0].replace('_val_data', '')
        print(f"\n--- Evaluating on station: {station_name} ---")

        # ... (rest of the file is unchanged) ...
        val_dataset = pd.read_csv(val_csv_file, header=0, index_col=0)
        val_dataset.index = pd.to_datetime(val_dataset.index)
        val_dataset.replace('Min', np.nan, inplace=True)
        val_dataset = val_dataset.asfreq('D')
        val_dataset = val_dataset.apply(pd.to_numeric, errors='coerce')
        val_dataset = val_dataset.interpolate(method='linear').ffill().bfill()

        required_cols = all_features + ['streamflow']
        missing_features = set(required_cols) - set(val_dataset.columns)
        if missing_features:
            print(f"Warning: Missing features {missing_features} in {station_name}. Filling with NaN and interpolating.")
            for mf in missing_features:
                val_dataset[mf] = np.nan
            val_dataset = val_dataset.interpolate(method='linear').ffill().bfill()

        val_dataset = val_dataset[required_cols]

        val_dataset['year'] = val_dataset.index.year
        val_dataset['month'] = val_dataset.index.month
        val_dataset['day'] = val_dataset.index.day
        val_dataset['day_of_week'] = val_dataset.index.dayofweek + 1
        val_dataset['day_of_year'] = val_dataset.index.dayofyear
        time_features = ['year', 'month', 'day', 'day_of_week', 'day_of_year']
        time_data = val_dataset[time_features].values

        scaled_features = scale_data(val_dataset[all_features], min_max_dict)
        scaled_target = scale_data(val_dataset[['streamflow']], min_max_dict)
        scaled = np.hstack((scaled_features.values, scaled_target.values))

        val_X, val_y = create_sequences(scaled, config['n_steps_in'], config['n_steps_out'])
        val_time_sequences = create_time_sequences(time_data, config['n_steps_in'], config['n_steps_out'])

        if len(val_X) == 0:
            print(f"Not enough data in {station_name} to create sequences. Skipping.")
            continue

        val_dataset_tf = tf.data.Dataset.from_tensor_slices(
            ((val_X.astype(np.float32), val_time_sequences.astype(np.float32)),)
        ).batch(config['batch_size']).prefetch(tf.data.AUTOTUNE)

        print("Running predictions...")
        y_pred_scaled = model.predict(val_dataset_tf)

        streamflow_min = min_max_dict['streamflow']['min']
        streamflow_max = min_max_dict['streamflow']['max']
        y_pred_inv = y_pred_scaled * (streamflow_max - streamflow_min) + streamflow_min
        val_y_inv = val_y * (streamflow_max - streamflow_min) + streamflow_min

        min_length = min(len(y_pred_inv), len(val_y_inv))
        y_pred_inv = y_pred_inv[:min_length]
        val_y_inv = val_y_inv[:min_length]

        val_start_date = val_dataset.index[config['n_steps_in']]
        val_dates = pd.date_range(start=val_start_date, periods=min_length)

        results_list = []
        for i in range(min_length):
            for j in range(config['n_steps_out']):
                forecast_day = j + 1
                date = val_dates[i] + pd.Timedelta(days=j)
                observed = val_y_inv[i, j]
                forecasted = y_pred_inv[i, j]
                results_list.append([date, forecast_day, observed, forecasted])

        df_result = pd.DataFrame(results_list, columns=['Date', 'Forecast_Horizon', 'Observed', 'Forecasted'])
        result_csv_path = os.path.join(args.output_dir, f"{station_name}_validation_forecast_vs_observed.csv")
        df_result.to_csv(result_csv_path, index=False)
        print(f"Validation results saved for {station_name} at {result_csv_path}")

        print(f"Calculating metrics for station: {station_name}")
        y_pred_da = xr.DataArray(y_pred_inv, dims=('time', 'horizon'))
        val_y_da = xr.DataArray(val_y_inv, dims=('time', 'horizon'))

        metrics_list = get_available_metrics()
        metrics_list = [m for m in metrics_list if m not in ["Peak-Timing", "Missed-Peaks"]]
        metrics_per_day = {}

        for day in range(config['n_steps_out']):
            obs = val_y_da.isel(horizon=day)
            sim = y_pred_da.isel(horizon=day)
            day_metrics = calculate_metrics(obs, sim, metrics=metrics_list)
            metrics_per_day[day + 1] = day_metrics

        metrics_df = pd.DataFrame(metrics_per_day).T
        metrics_df.index.name = 'Forecast_Day'
        metrics_csv_path = os.path.join(evaluation_folder, f"{station_name}_validation_daywise_metrics.csv")
        metrics_df.to_csv(metrics_csv_path)
        print(f"Validation metrics saved for {station_name} at {metrics_csv_path}")

    K.clear_session()
    print("\nEvaluation complete for this model run.")

if __name__ == '__main__':
    main()


================================================
File: export_test_data.py
================================================
import os
import yaml
import argparse
import numpy as np
import pandas as pd
import glob

# Parse command-line arguments
parser = argparse.ArgumentParser(description='Export test data for evaluation.')
parser.add_argument('--config', type=str, default='config.yml', help='Path to the config file.')
args = parser.parse_args()

# Load the specified configuration file
with open(args.config, 'r') as f:
    config = yaml.safe_load(f)

# Create output directory for test data
test_data_folder = config.get('test_data_folder', 'test_data')
os.makedirs(test_data_folder, exist_ok=True)

# Helper function to load and process datasets
def load_and_process_data(file_paths):
    data_list = []
    for path in file_paths:
        if os.path.isdir(path):
            csv_files = glob.glob(os.path.join(path, "*.csv"))
        else:
            csv_files = [path]

        for csv_file in csv_files:
            dataset = pd.read_csv(csv_file, header=0, index_col=0)
            dataset.index = pd.to_datetime(dataset.index)
            dataset.replace('Min', np.nan, inplace=True)
            dataset = dataset.asfreq('D')
            dataset = dataset.apply(pd.to_numeric, errors='coerce')
            dataset = dataset.loc[dataset['streamflow'].first_valid_index():]
            dataset = dataset.loc[:dataset['streamflow'].last_valid_index()]
            dataset = dataset.interpolate(method='linear').ffill().bfill()
            data_list.append((csv_file, dataset))
    return data_list

# Load main datasets
print("Loading main datasets...")
main_datasets = load_and_process_data(config['data_paths'])

# Export test data
for csv_file, dataset in main_datasets:
    station_name = os.path.splitext(os.path.basename(csv_file))[0]
    # Split the dataset into train, validation, and test
    total_records = len(dataset)
    train_end = int(total_records * 0.7)
    val_end = int(total_records * 0.8)
    test_data = dataset.iloc[val_end:]
    # Save the test data to CSV
    test_data_path = os.path.join(test_data_folder, f"{station_name}_test_data.csv")
    test_data.to_csv(test_data_path)
    print(f"Test data saved for {station_name} at {test_data_path}")



================================================
File: export_val_data.py
================================================
import os
import yaml
import argparse
import numpy as np
import pandas as pd
import glob

# Parse command-line arguments
parser = argparse.ArgumentParser(description='Export validation data for evaluation.')
parser.add_argument('--config', type=str, default='config.yml', help='Path to the config file.')
args = parser.parse_args()

# Load the specified configuration file
with open(args.config, 'r') as f:
    config = yaml.safe_load(f)

# Create output directory for validation data
val_data_folder = config.get('val_data_folder', 'val_data')
os.makedirs(val_data_folder, exist_ok=True)

# Helper function to load and process datasets
def load_and_process_data(file_paths):
    data_list = []
    for path in file_paths:
        if os.path.isdir(path):
            csv_files = glob.glob(os.path.join(path, "*.csv"))
        else:
            csv_files = [path]

        for csv_file in csv_files:
            dataset = pd.read_csv(csv_file, header=0, index_col=0)
            dataset.index = pd.to_datetime(dataset.index)
            dataset.replace('Min', np.nan, inplace=True)
            dataset = dataset.asfreq('D')
            dataset = dataset.apply(pd.to_numeric, errors='coerce')
            dataset = dataset.loc[dataset['streamflow'].first_valid_index():]
            dataset = dataset.loc[:dataset['streamflow'].last_valid_index()]
            dataset = dataset.interpolate(method='linear').ffill().bfill()
            data_list.append((csv_file, dataset))
    return data_list

# Load main datasets
print("Loading main datasets...")
main_datasets = load_and_process_data(config['data_paths'])

# Export validation data
for csv_file, dataset in main_datasets:
    station_name = os.path.splitext(os.path.basename(csv_file))[0]
    # Split the dataset into train, validation, and test
    total_records = len(dataset)
    train_end = int(total_records * 0.7)
    val_end = int(total_records * 0.8)
    # Select the validation data slice
    val_data = dataset.iloc[train_end:val_end]
    # Save the validation data to CSV
    val_data_path = os.path.join(val_data_folder, f"{station_name}_val_data.csv")
    val_data.to_csv(val_data_path)
    print(f"Validation data saved for {station_name} at {val_data_path}")


================================================
File: hp.py
================================================
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress INFO and WARNING logs from TensorFlow

import warnings
warnings.filterwarnings('ignore')  # Suppress all Python warnings

import logging
logging.getLogger('tensorflow').setLevel(logging.ERROR)  # Suppress INFO and WARNING logs from TensorFlow


import keras_tuner as kt
import tensorflow as tf
from models import get_model
import yaml
import numpy as np
import os
import argparse
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import glob

# from tensorflow.keras import mixed_precision
# mixed_precision.set_global_policy('mixed_float16')

# tf.config.optimizer.set_jit(False)  # Disables XLA compilation


# Function to load the configuration file
def load_config(config_path):
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

# Load and process data (similar to main.py)
def load_and_process_data(file_paths):
    data_list = []
    for path in file_paths:
        csv_files = [path] if os.path.isfile(path) else glob.glob(os.path.join(path, "*.csv"))
        for csv_file in csv_files:
            dataset = pd.read_csv(csv_file, header=0, index_col=0)
            dataset.index = pd.to_datetime(dataset.index)
            dataset.replace('Min', np.nan, inplace=True)
            dataset = dataset.asfreq('D')
            dataset = dataset.apply(pd.to_numeric, errors='coerce')
            dataset = dataset.interpolate(method='linear').ffill().bfill()
            data_list.append(dataset)
    return data_list

# Define hypermodel for tuning
def build_hyper_model(hp):
    model_params = {
        'num_heads': hp.Int('num_heads', min_value=2, max_value=8, step=2),
        'head_size': hp.Int('head_size', min_value=8, max_value=64, step=8),
        'ff_dim': hp.Int('ff_dim', min_value=64, max_value=512, step=64),
        'num_transformer_blocks': hp.Int('num_transformer_blocks', min_value=2, max_value=6, step=1),
        'dropout_rate': hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1),
        'mlp_units': [hp.Int('mlp_units1', min_value=64, max_value=256, step=64),
                      hp.Int('mlp_units2', min_value=64, max_value=256, step=64)],
        'mlp_dropout_rate': hp.Float('mlp_dropout_rate', min_value=0.1, max_value=0.5, step=0.1),
        'hidden_dim': hp.Int('hidden_dim', min_value=64, max_value=256, step=64),
        'num_time_features': 5,  # Ensure this is always included
        'time_feature_sizes': [2020, 12, 31, 7, 366]
    }

    # Build and compile model with dynamically selected parameters
    model = get_model({
        'model_type': 'TRANSFORMER',
        'n_steps_in': config['n_steps_in'],
        'n_steps_out': config['n_steps_out'],
        'model_params': model_params
    }, number_of_features)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=config.get('learning_rate', 0.001)),
        loss='mse',
        metrics=['mae']
    )
    return model

# Function to create sequences for main features
def create_sequences(data, n_steps_in, n_steps_out):
    X, y = [], []
    for i in range(len(data)):
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out
        if out_end_ix > len(data):
            break
        seq_x, seq_y = data[i:end_ix], data[end_ix:out_end_ix, -1]  # Assuming last column is target
        X.append(seq_x)
        y.append(seq_y)
    return np.array(X), np.array(y)

# Function to create sequences for time features
def create_time_sequences(data, n_steps_in, n_steps_out):
    X = []
    for i in range(len(data)):
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out
        if out_end_ix > len(data):
            break
        seq_x = data[i:end_ix]
        X.append(seq_x)
    return np.array(X)

def run_hyperparameter_tuning():
    # Load and process the dataset
    main_datasets = load_and_process_data(config['data_paths'])
    scaler = MinMaxScaler()

    # Prepare data for tuning
    X_train, y_train, X_time_train = [], [], []
    for dataset in main_datasets:
        # Scale and prepare the main features
        scaled_data = scaler.fit_transform(dataset)
        X, y = create_sequences(scaled_data, config['n_steps_in'], config['n_steps_out'])
        X_train.append(X)
        y_train.append(y)

        # Prepare time features from the dataset index
        index = dataset.index  # Directly use the index
        time_features = pd.DataFrame({
            'year': index.year,
            'month': index.month,
            'day': index.day,
            'day_of_week': index.dayofweek + 1,  # Monday=1, Sunday=7
            'day_of_year': index.dayofyear
        }).values

        # Create time sequences
        X_time = create_time_sequences(time_features, config['n_steps_in'], config['n_steps_out'])
        X_time_train.append(X_time)

    # Concatenate all training data
    X_train = np.concatenate(X_train)
    y_train = np.concatenate(y_train)
    X_time_train = np.concatenate(X_time_train)

    # Verify shapes before calling tuner.search
    print(f"Final X_train shape (main features): {X_train.shape}")       # Expecting (56968, 365, 49)
    print(f"Final X_time_train shape (time features): {X_time_train.shape}") # Expecting (56968, 365, 5)
    print(f"Final y_train shape (target): {y_train.shape}")             # Expecting (56968, 3)

    # Define tuner
    tuner = kt.Hyperband(
        build_hyper_model,
        objective='val_mae',
        max_epochs=10,
        factor=3,
        directory=config['output_prefix'],
        project_name='hyperparameter_tuning'
    )

    # Run search with both main and time inputs
    tuner.search((X_train, X_time_train), y_train, validation_split=0.2, epochs=5, batch_size=config['batch_size'])

    # Get best hyperparameters and save them
    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
    print("Best hyperparameters found:")
    for param in best_hps.values:
        print(f"{param}: {best_hps.get(param)}")

    # Save best hyperparameters to file
    best_params = {param: best_hps.get(param) for param in best_hps.values}
    with open(os.path.join(config['output_prefix'], 'best_params.yml'), 'w') as file:
        yaml.dump(best_params, file)

# Load config and run tuning
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Run hyperparameter tuning with a specific config file.')
    parser.add_argument('--config', type=str, default='config.yml', help='Path to the config file.')
    args = parser.parse_args()
    
    config = load_config(args.config)
    number_of_features = 49
    run_hyperparameter_tuning()



================================================
File: huc.py
================================================
import os
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cfeature
from descartes import PolygonPatch

# Define base directory and CSV paths
base_dir = '/scratch/kdahal3/camels_losses'
forecast_horizons = [1, 7, 30]
csv_files = {
    1: os.path.join(base_dir, 'bma_nse_results_1day_with_uncertainty.csv'),
    7: os.path.join(base_dir, 'bma_nse_results_7day_with_uncertainty.csv'),
    30: os.path.join(base_dir, 'bma_nse_results_30day_with_uncertainty.csv')
}

# Path to station coordinates
coords_file = '/scratch/kdahal3/Caravan/attributes/camels/attributes_other_camels.csv'

# Read station coordinates
coords_df = pd.read_csv(coords_file)
print("Coords dataframe loaded.")
print("Original coords_df shape:", coords_df.shape)

coords_df['Station_ID'] = coords_df['gauge_id'].apply(lambda x: x.split('_')[1])
coords_df['Station_ID'] = coords_df['Station_ID'].astype(str).str.zfill(8)

print("Coords_df unique Station_ID count BEFORE drop_duplicates:", coords_df['Station_ID'].nunique())

# FIX: Drop duplicates
coords_df = coords_df.drop_duplicates(subset=['Station_ID'])

print("Coords_df shape AFTER drop_duplicates:", coords_df.shape)
print("Coords_df unique Station_ID count AFTER drop_duplicates:", coords_df['Station_ID'].nunique())
print("--------------------------------------------------------")

# Station ID to highlight
selected_station_id = ''

merged_data_list = []

for fh in forecast_horizons:
    file_path = csv_files.get(fh)

    if not os.path.isfile(file_path):
        print(f"CSV file for {fh}-day forecast not found at {file_path}. Skipping.")
        continue

    # Read forecast performance data
    performance_df = pd.read_csv(file_path)
    print(f"\nPerformance dataframe loaded for {fh}-day forecast.")
    print("Original performance_df shape:", performance_df.shape)

    performance_df['Station_ID'] = performance_df['Station_ID'].astype(str).str.zfill(8)

    print("Performance_df unique Station_ID count BEFORE drop_duplicates:", performance_df['Station_ID'].nunique())

    # FIX: Drop duplicates
    performance_df = performance_df.drop_duplicates(subset=['Station_ID'])

    print("Performance_df shape AFTER drop_duplicates:", performance_df.shape)
    print("Performance_df unique Station_ID count AFTER drop_duplicates:", performance_df['Station_ID'].nunique())

    # Merge
    merged_df = pd.merge(performance_df, coords_df, on='Station_ID', how='inner')

    print("Merged_df shape AFTER merge:", merged_df.shape)
    print("Merged_df unique Station_ID count:", merged_df['Station_ID'].nunique())

    # Drop missing data
    merged_df = merged_df.dropna(subset=['gauge_lat', 'gauge_lon', 'NSE_BMA'])

    print("Merged_df shape AFTER dropna:", merged_df.shape)

    # Add forecast horizon column
    merged_df['Forecast_Horizon'] = f"{fh}-Day"

    merged_data_list.append(merged_df)

print("\nMerging and cleaning complete.")
# Combine all merged data
all_merged_df = pd.concat(merged_data_list, ignore_index=True)

print("All merged dataframe shape:", all_merged_df.shape)
print("Unique Station_ID count in all_merged_df:", all_merged_df['Station_ID'].nunique())
print("Total records per Forecast Horizon:")
print(all_merged_df['Forecast_Horizon'].value_counts())
print("--------------------------------------------------------")

# Define forecast horizons for plotting
available_forecasts = all_merged_df['Forecast_Horizon'].unique()
available_forecasts = sorted(available_forecasts, key=lambda x: int(x.split('-')[0]))

# Color map
cmap = 'Blues'

# Get selected station
selected_station = all_merged_df[all_merged_df['Station_ID'].astype(str) == str(selected_station_id)]
if not selected_station.empty:
    selected_lat = selected_station['gauge_lat'].iloc[0]
    selected_lon = selected_station['gauge_lon'].iloc[0]
    print(f"Selected Station found: Latitude = {selected_lat}, Longitude = {selected_lon}")
else:
    print(f"Selected Station {selected_station_id} not found.")
    selected_lat, selected_lon = None, None

# Load shapefile
shapefile_path = '/scratch/kdahal3/camels_losses/huc2.shp'
shape_gdf = gpd.read_file(shapefile_path)

# Create plots
fig, axes = plt.subplots(
    len(available_forecasts), 1,
    figsize=(5, 7.5),
    subplot_kw={'projection': ccrs.PlateCarree()},
    constrained_layout=True
)

if len(available_forecasts) == 1:
    axes = [axes]

for ax, fh in zip(axes, available_forecasts):
    fh_df = all_merged_df[all_merged_df['Forecast_Horizon'] == fh]

    # Map features
    # ax.add_feature(cfeature.COASTLINE, linewidth=1)
    # ax.add_feature(cfeature.BORDERS, linestyle=':', linewidth=0.5)
    # ax.add_feature(cfeature.STATES, linestyle=':', linewidth=0.5)
    # ax.add_feature(cfeature.RIVERS, linewidth=0.5)

    ax.set_extent([-125, -66.5, 24, 49.5], crs=ccrs.PlateCarree())

    # Plot shapefile
    shape_gdf.plot(ax=ax, facecolor='none', edgecolor='black')

    scatter = ax.scatter(
        fh_df['gauge_lon'],
        fh_df['gauge_lat'],
        c=fh_df['NSE_BMA'],
        cmap=cmap,
        marker='o',
        s=20,
        edgecolor='k',
        alpha=0.7,
        vmin=0,
        vmax=1
    )

    cbar = fig.colorbar(scatter, ax=ax, orientation='vertical', pad=0.02, shrink=0.6)
    cbar.set_label('NSE (BMA)', fontsize=10)

    if selected_lat is not None and selected_lon is not None:
        ax.scatter(
            selected_lon,
            selected_lat,
            facecolors='none',
            edgecolors='red',
            s=100,
            linewidths=2,
            marker='o',
            label='Selected Station'
        )
        # if fh == available_forecasts[0]:
        #     ax.legend(loc='upper right', fontsize=10)

    ax.set_title(f"{fh} NSE Forecast", fontsize=12)

    gl = ax.gridlines(draw_labels=True, linestyle='--', alpha=0.5)
    gl.top_labels = False
    gl.right_labels = False
    gl.left_labels = True
    gl.bottom_labels = True
    gl.xlabel_style = {'size': 10}
    gl.ylabel_style = {'size': 10}

output_plot_path = os.path.join(base_dir, 'bma_performance_maps_1_7_30day_with_huc2.png')
plt.savefig(output_plot_path, dpi=300)
print(f"Performance maps saved to {output_plot_path}")

plt.show()














import os
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cfeature

# Define base directory and CSV paths
base_dir = '/scratch/kdahal3/camels_losses'
forecast_horizons = [1, 7, 30]
csv_files = {
    1: os.path.join(base_dir, 'bma_nse_results_1day_with_uncertainty.csv'),
    7: os.path.join(base_dir, 'bma_nse_results_7day_with_uncertainty.csv'),
    30: os.path.join(base_dir, 'bma_nse_results_30day_with_uncertainty.csv')
}

# Path to station coordinates
coords_file = '/scratch/kdahal3/Caravan/attributes/camels/attributes_other_camels.csv'

# Read station coordinates
coords_df = pd.read_csv(coords_file)
print("Coords dataframe loaded.")
print("Original coords_df shape:", coords_df.shape)

coords_df['Station_ID'] = coords_df['gauge_id'].apply(lambda x: x.split('_')[1])
coords_df['Station_ID'] = coords_df['Station_ID'].astype(str).str.zfill(8)

print("Coords_df unique Station_ID count BEFORE drop_duplicates:", coords_df['Station_ID'].nunique())

# FIX: Drop duplicates
coords_df = coords_df.drop_duplicates(subset=['Station_ID'])

print("Coords_df shape AFTER drop_duplicates:", coords_df.shape)
print("Coords_df unique Station_ID count AFTER drop_duplicates:", coords_df['Station_ID'].nunique())
print("--------------------------------------------------------")

# Station ID to highlight
selected_station_id = ''

merged_data_list = []

for fh in forecast_horizons:
    file_path = csv_files.get(fh)

    if not os.path.isfile(file_path):
        print(f"CSV file for {fh}-day forecast not found at {file_path}. Skipping.")
        continue

    # Read forecast performance data
    performance_df = pd.read_csv(file_path)
    print(f"\nPerformance dataframe loaded for {fh}-day forecast.")
    print("Original performance_df shape:", performance_df.shape)

    performance_df['Station_ID'] = performance_df['Station_ID'].astype(str).str.zfill(8)

    print("Performance_df unique Station_ID count BEFORE drop_duplicates:", performance_df['Station_ID'].nunique())

    # FIX: Drop duplicates
    performance_df = performance_df.drop_duplicates(subset=['Station_ID'])

    print("Performance_df shape AFTER drop_duplicates:", performance_df.shape)
    print("Performance_df unique Station_ID count AFTER drop_duplicates:", performance_df['Station_ID'].nunique())

    # Merge
    merged_df = pd.merge(performance_df, coords_df, on='Station_ID', how='inner')

    print("Merged_df shape AFTER merge:", merged_df.shape)
    print("Merged_df unique Station_ID count:", merged_df['Station_ID'].nunique())

    # Drop missing data
    merged_df = merged_df.dropna(subset=['gauge_lat', 'gauge_lon', 'NSE_BMA'])

    print("Merged_df shape AFTER dropna:", merged_df.shape)

    # Add forecast horizon column
    merged_df['Forecast_Horizon'] = f"{fh}-Day"

    merged_data_list.append(merged_df)

print("\nMerging and cleaning complete.")
# Combine all merged data
all_merged_df = pd.concat(merged_data_list, ignore_index=True)

print("All merged dataframe shape:", all_merged_df.shape)
print("Unique Station_ID count in all_merged_df:", all_merged_df['Station_ID'].nunique())
print("Total records per Forecast Horizon:")
print(all_merged_df['Forecast_Horizon'].value_counts())
print("--------------------------------------------------------")

# Load shapefile
shapefile_path = '/scratch/kdahal3/camels_losses/huc2.shp'
shape_gdf = gpd.read_file(shapefile_path)

# Print HUC2 IDs
print("HUC2 IDs:")
print(shape_gdf['huc2'].unique())

# Create plots
fig, axes = plt.subplots(
    len(forecast_horizons), 1,
    figsize=(5, 7.5),
    subplot_kw={'projection': ccrs.PlateCarree()},
    constrained_layout=True
)

if len(forecast_horizons) == 1:
    axes = [axes]

for ax, fh in zip(axes, forecast_horizons):
    fh_df = all_merged_df[all_merged_df['Forecast_Horizon'] == f"{fh}-Day"]

    # Map features
    # ax.add_feature(cfeature.COASTLINE, linewidth=1)
    # ax.add_feature(cfeature.BORDERS, linestyle=':', linewidth=0.5)
    # ax.add_feature(cfeature.STATES, linestyle=':', linewidth=0.5)
    # ax.add_feature(cfeature.RIVERS, linewidth=0.5)

    ax.set_extent([-125, -66.5, 24, 49.5], crs=ccrs.PlateCarree())

    # Plot shapefile
    shape_gdf.plot(ax=ax, facecolor='none', edgecolor='black')

    # Annotate HUC2 IDs
    for idx, row in shape_gdf.iterrows():
        centroid = row.geometry.centroid
        ax.text(centroid.x, centroid.y, row['huc2'], fontsize=8, ha='center', va='center', color='red')

    scatter = ax.scatter(
        fh_df['gauge_lon'],
        fh_df['gauge_lat'],
        c=fh_df['NSE_BMA'],
        cmap=cmap,
        marker='o',
        s=20,
        edgecolor='k',
        alpha=0.7,
        vmin=0,
        vmax=1
    )

    cbar = fig.colorbar(scatter, ax=ax, orientation='vertical', pad=0.02, shrink=0.6)
    cbar.set_label('NSE (BMA)', fontsize=10)

    if selected_lat is not None and selected_lon is not None:
        ax.scatter(
            selected_lon,
            selected_lat,
            facecolors='none',
            edgecolors='red',
            s=100,
            linewidths=2,
            marker='o',
            label='Selected Station'
        )
        if fh == forecast_horizons[0]:
            ax.legend(loc='upper right', fontsize=10)

    ax.set_title(f"{fh}-Day NSE Forecast", fontsize=12)

    gl = ax.gridlines(draw_labels=True, linestyle='--', alpha=0.5)
    gl.top_labels = False
    gl.right_labels = False
    gl.left_labels = True
    gl.bottom_labels = True
    gl.xlabel_style = {'size': 10}
    gl.ylabel_style = {'size': 10}

output_plot_path = os.path.join(base_dir, 'bma_performance_maps_1_7_30day_with_huc2.png')
plt.savefig(output_plot_path, dpi=300)
print(f"Performance maps saved to {output_plot_path}")

plt.show()






# Calculate mean NSE (BMA) for each HUC2 basin

# First, make sure that each station in all_merged_df is assigned a HUC2
# We will spatially join the station locations with the shape_gdf polygons to get HUC2 assignments.

# Create a GeoDataFrame from station lat/lon
stations_gdf = gpd.GeoDataFrame(
    all_merged_df,
    geometry=gpd.points_from_xy(all_merged_df['gauge_lon'], all_merged_df['gauge_lat']),
    crs="EPSG:4326"
)

# Spatial join to assign HUC2 to each station
stations_with_huc = gpd.sjoin(stations_gdf, shape_gdf[['huc2', 'geometry']], how="left", predicate='within')

# Now calculate mean NSE (BMA) by HUC2
mean_nse_by_huc2 = stations_with_huc.groupby('huc2')['NSE_BMA'].mean()

# Display results
print("\nMean NSE (BMA) for each HUC2 basin:")
print(mean_nse_by_huc2)


# Calculate statistics for each HUC2 basin AND Forecast Horizon

summary_stats = stations_with_huc.groupby(['huc2', 'Forecast_Horizon'])['NSE_BMA'].agg(['mean', 'min', 'max', 'count'])

# Calculate range
summary_stats['range'] = summary_stats['max'] - summary_stats['min']

# Rename columns
summary_stats = summary_stats.rename(columns={
    'mean': 'mean_nse',
    'min': 'min_nse',
    'max': 'max_nse',
    'count': 'station_count',
    'range': 'range_nse'
})

# Display result
print("\nHUC2 Basin NSE Statistics by Forecast Horizon:")
print(summary_stats)



================================================
File: huc2_spatial_nse_uncertainty.py
================================================
import os
import pandas as pd
import glob
import numpy as np
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cfeature
from functools import reduce
import re # Import the regular expression module
import geopandas as gpd # Import geopandas

# --- CONFIGURATION ---
base_dir = '/scratch/kdahal3/camels_losses'
log_file_path = os.path.join(base_dir, 'merge_log.txt')
forecast_horizons = [1, 7, 30]
loss_functions = [
    "nse_loss", "kge_loss", "huber_loss", "quantile_low",
    "quantile_high", "expectile_low", "expectile_high"
]
coords_file = '/scratch/kdahal3/Caravan/attributes/camels/attributes_other_camels.csv'
shapefile_path = '/scratch/kdahal3/camels_losses/huc2.shp'

def clean_and_zfill(station_id_series):
    return station_id_series.astype(str).str.strip().str.zfill(8)

# Clear old log file
if os.path.exists(log_file_path):
    os.remove(log_file_path)

def log_message(message):
    print(message)
    with open(log_file_path, 'a') as f:
        f.write(message + '\n')

log_message("="*60)
log_message("Starting spatial uncertainty analysis...")
log_message("="*60)

# --- 1. LOAD AND PREPARE GEOSPATIAL DATA ---
log_message("\n--- STEP 1: Loading and Cleaning Geospatial Data ---")
try:
    coords_df = pd.read_csv(coords_file)
    coords_df['Station_ID'] = coords_df['gauge_id'].apply(lambda x: str(x).split('_')[1])
    coords_df['Station_ID'] = clean_and_zfill(coords_df['Station_ID'])
    coords_df = coords_df[['Station_ID', 'gauge_lat', 'gauge_lon']].drop_duplicates(subset=['Station_ID'])
    log_message(f"  Cleaned and prepared coordinates for {len(coords_df)} unique stations.")
    
    shape_gdf = gpd.read_file(shapefile_path)
    log_message(f"  Successfully loaded HUC2 shapefile with {len(shape_gdf)} basins.")
    log_message("-" * 50)
except Exception as e:
    log_message(f"[FATAL ERROR] Could not load coordinate or shapefile: {e}")
    exit()

# --- 2. PROCESS DATA FOR EACH HORIZON (Condensed for brevity) ---
all_horizon_data = []
for fh in forecast_horizons:
    log_message(f"\n{'='*25} Processing {fh}-Day Forecast {'='*25}")
    horizon_key = f'{fh}day'
    model_dfs = []
    for loss_func in loss_functions:
        folder_path = os.path.join(base_dir, f"camels_LSTM_{loss_func}_{horizon_key}/evaluation_metrics/")
        if not os.path.exists(folder_path): continue
        csv_files = glob.glob(os.path.join(folder_path, '*.csv'))
        if not csv_files: continue
        station_ids, nse_values = [], []
        for file_path in csv_files:
            try:
                station_id_from_filename = os.path.basename(file_path).split('.')[0]
                temp_df = pd.read_csv(file_path, index_col=0).reset_index()
                temp_df.rename(columns={'index': 'Day'}, inplace=True)
                day_specific_series = temp_df[temp_df['Day'] == fh]
                if day_specific_series.empty or 'NSE' not in day_specific_series.columns: continue
                nse_value = day_specific_series['NSE'].iloc[0]
                station_ids.append(station_id_from_filename)
                nse_values.append(nse_value)
            except Exception: continue
        if not station_ids: continue
        model_df = pd.DataFrame({'Station_ID': station_ids, f'NSE_{loss_func}': nse_values})
        model_df['Station_ID'] = clean_and_zfill(model_df['Station_ID'])
        model_dfs.append(model_df)
    if not model_dfs: continue
    merged_individual_df = reduce(lambda left, right: pd.merge(left, right, on='Station_ID', how='outer'), model_dfs)
    id_pattern = re.compile(r'(\d{8})')
    def extract_clean_id(messy_id):
        match = id_pattern.search(str(messy_id)); return match.group(1) if match else None
    merged_individual_df['Clean_Station_ID'] = merged_individual_df['Station_ID'].apply(extract_clean_id)
    merged_individual_df = merged_individual_df.drop(columns=['Station_ID']).rename(columns={'Clean_Station_ID': 'Station_ID'})
    merged_individual_df['Station_ID'] = clean_and_zfill(merged_individual_df['Station_ID'])
    nse_cols = [col for col in merged_individual_df.columns if 'NSE_' in col]
    merged_individual_df['NSE_Range'] = merged_individual_df[nse_cols].max(axis=1) - merged_individual_df[nse_cols].min(axis=1)
    bma_file = os.path.join(base_dir, f'bma_results_{fh}day_with_metrics.csv')
    if not os.path.isfile(bma_file): bma_df = pd.DataFrame(columns=['Station_ID', 'NSE_BMA'])
    else:
        bma_df = pd.read_csv(bma_file)
        if 'station_id' in bma_df.columns: bma_df.rename(columns={'station_id': 'Station_ID'}, inplace=True)
        bma_df['Station_ID'] = clean_and_zfill(bma_df['Station_ID'])
        bma_df = bma_df[['Station_ID', 'NSE_BMA']].drop_duplicates(subset=['Station_ID'])
    merged1 = pd.merge(merged_individual_df, bma_df, on='Station_ID', how='inner')
    final_df = pd.merge(merged1, coords_df, on='Station_ID', how='inner')
    if final_df.empty: continue
    final_df['Forecast_Horizon'] = f"{fh}-Day"
    all_horizon_data.append(final_df)
    log_message(f"  SUCCESS! Final data for {fh}-day prepared. Shape: {final_df.shape}")

# --- 5. CREATE THE FINAL PLOT ---
log_message(f"\n{'='*25} Generating Final Plot {'='*25}")
if not all_horizon_data:
    log_message("[FATAL] No data processed. Cannot generate plot.")
else:
    master_df = pd.concat(all_horizon_data, ignore_index=True)
    master_df.dropna(subset=['NSE_BMA', 'NSE_Range', 'gauge_lat', 'gauge_lon'], inplace=True)
    log_message(f"  Master DataFrame created for plotting. Total records: {len(master_df)}")
    
    fig, axes = plt.subplots(
        len(forecast_horizons), 2,
        figsize=(12, 10), # Increased figure size slightly for better label spacing
        subplot_kw={'projection': ccrs.PlateCarree()},
        constrained_layout=True
    )
    
    for i, fh_str in enumerate([f"{fh}-Day" for fh in forecast_horizons]):
        fh_df = master_df[master_df['Forecast_Horizon'] == fh_str]
        if fh_df.empty:
            axes[i, 0].set_visible(False); axes[i, 1].set_visible(False)
            continue
        
        # Plot on both axes for this horizon
        for ax in [axes[i, 0], axes[i, 1]]:
            ax.set_extent([-125, -66.5, 24, 49.5], crs=ccrs.PlateCarree())
            ax.add_feature(cfeature.COASTLINE, zorder=2)
            ax.add_feature(cfeature.STATES, linestyle=':', linewidth=0.5, zorder=2)
            shape_gdf.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=0.8, zorder=3)
            
            # ######################################
            # ### ANNOTATE HUC2 BASIN NUMBERS  ###
            # ######################################
            for idx, row in shape_gdf.iterrows():
                # Use a representative point to place the label
                centroid = row.geometry.centroid
                ax.text(centroid.x, centroid.y, row['huc2'],
                        fontsize=8, ha='center', va='center',
                        color='dimgray', weight='bold', zorder=5)

        # --- Left Plot: BMA NSE Performance ---
        ax1 = axes[i, 0]
        scatter1 = ax1.scatter(
            fh_df['gauge_lon'], fh_df['gauge_lat'], c=fh_df['NSE_BMA'],
            cmap='viridis', vmin=-0.5, vmax=1, s=25, 
            # ########################
            # ### REMOVE BORDERS ###
            # ########################
            edgecolor='none',
            alpha=0.8, zorder=4
        )
        cbar1 = fig.colorbar(scatter1, ax=ax1, orientation='vertical', pad=0.02, shrink=0.6)
        cbar1.set_label('NSE (BMA)'); ax1.set_title(f'{fh_str} NSE Forecast')

        # --- Right Plot: Uncertainty (NSE Range) ---
        ax2 = axes[i, 1]
        vmax_val = fh_df['NSE_Range'].quantile(0.95) 
        scatter2 = ax2.scatter(
            fh_df['gauge_lon'], fh_df['gauge_lat'], c=fh_df['NSE_Range'],
            cmap='plasma', vmin=0, vmax=vmax_val, s=25,
            # ########################
            # ### REMOVE BORDERS ###
            # ########################
            edgecolor='none',
            alpha=0.8, zorder=4
        )
        cbar2 = fig.colorbar(scatter2, ax=ax2, orientation='vertical', pad=0.02, shrink=0.6)
        cbar2.set_label('Uncertainty (NSE Range)'); ax2.set_title(f'{fh_str} Forecast Uncertainty')

    # Add gridlines
    for i in range(len(forecast_horizons)):
        for j in range(2):
            if i >= axes.shape[0] or not axes[i,j].get_visible(): continue
            ax = axes[i, j]
            gl = ax.gridlines(draw_labels=True, linestyle='--', alpha=0.5, zorder=1)
            gl.top_labels = False; gl.right_labels = False
            gl.left_labels = (j == 0); gl.bottom_labels = (i == len(forecast_horizons) - 1)
    
    output_plot_path = os.path.join(base_dir, 'spatial_performance_vs_uncertainty_final.png')
    plt.savefig(output_plot_path, dpi=300, bbox_inches='tight')
    log_message(f"\nSUCCESS! Final plot saved to {output_plot_path}")
    
    plt.show()


================================================
File: kill_gpu.sh
================================================
#!/bin/bash

# List all processes using the GPU and extract their process IDs (PIDs)
# This command will filter for "python" processes, but it can be adjusted to any process type.
processes=$(nvidia-smi --query-compute-apps=pid --format=csv,noheader,nounits | awk '{print $1}')

# Check if any processes were found
if [ -z "$processes" ]; then
    echo "No GPU processes to kill."
else
    echo "Found GPU processes. Killing them..."

    # Iterate through each process ID and kill it
    for pid in $processes; do
        echo "Killing process with PID: $pid"
        kill -9 "$pid" 2>/dev/null

        # Check if the process was killed successfully
        if [ $? -eq 0 ]; then
            echo "Successfully killed process $pid"
        else
            echo "Failed to kill process $pid"
        fi
    done
fi




================================================
File: losses.py
================================================
# losses.py

import tensorflow as tf
from keras import backend as K
from keras.losses import Loss

class KgeLoss(Loss):
    def __init__(self, name='kge_loss'):
        super().__init__(name=name)

    def call(self, y_true, y_pred):
        mean_true = tf.reduce_mean(y_true)
        mean_pred = tf.reduce_mean(y_pred)
        std_true = tf.math.reduce_std(y_true)
        std_pred = tf.math.reduce_std(y_pred)
        covariance = tf.reduce_mean((y_true - mean_true) * (y_pred - mean_pred))
        correlation = covariance / (std_true * std_pred + K.epsilon())
        std_ratio = std_pred / (std_true + K.epsilon())
        bias_ratio = mean_pred / (mean_true + K.epsilon())
        kge = 1 - tf.sqrt(
            tf.square(correlation - 1) + tf.square(std_ratio - 1) + tf.square(bias_ratio - 1)
        )
        return 1 - kge

    def get_config(self):
        base_config = super().get_config()
        return {**base_config}

class NseLoss(Loss):
    def __init__(self, name='nse_loss'):
        super().__init__(name=name)

    def call(self, y_true, y_pred):
        mean_true = tf.reduce_mean(y_true)
        numerator = tf.reduce_sum(tf.square(y_true - y_pred))
        denominator = tf.reduce_sum(tf.square(y_true - mean_true))
        nse = 1 - numerator / (denominator + K.epsilon())
        return 1 - nse

    def get_config(self):
        base_config = super().get_config()
        return {**base_config}

class ExpectileLoss(Loss):
    def __init__(self, expectile=0.5, name='expectile_loss'):
        super().__init__(name=name)
        self.expectile = expectile

    def call(self, y_true, y_pred):
        e = y_true - y_pred
        loss = tf.reduce_mean(
            tf.where(
                e >= 0,
                self.expectile * tf.square(e),
                (1 - self.expectile) * tf.square(e)
            )
        )
        return loss

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "expectile": self.expectile}

    @classmethod
    def from_config(cls, config):
        return cls(**config)

class QuantileLoss(Loss):
    def __init__(self, quantile=0.5, name='quantile_loss'):
        super().__init__(name=name)
        self.quantile = quantile

    def call(self, y_true, y_pred):
        e = y_true - y_pred
        loss = tf.reduce_mean(
            tf.maximum(self.quantile * e, (self.quantile - 1) * e)
        )
        return loss

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "quantile": self.quantile}

    @classmethod
    def from_config(cls, config):
        return cls(**config)

class HuberLoss(Loss):
    def __init__(self, delta=1.0, name='huber_loss'):
        super().__init__(name=name)
        self.delta = delta

    def call(self, y_true, y_pred):
        return tf.reduce_mean(tf.keras.losses.huber(y_true, y_pred, delta=self.delta))

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "delta": self.delta}

    @classmethod
    def from_config(cls, config):
        return cls(**config)

class WeightedMSELoss(Loss):
    def __init__(self, weights=None, name='weighted_mse_loss'):
        super().__init__(name=name)
        self.weights = weights

    def call(self, y_true, y_pred):
        if self.weights is None:
            weights = tf.ones_like(y_true)
        else:
            weights = tf.cast(self.weights, dtype=tf.float32)
        return tf.reduce_mean(weights * tf.square(y_true - y_pred))

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "weights": self.weights}

    @classmethod
    def from_config(cls, config):
        return cls(**config)

class MaeLoss(Loss):
    def __init__(self, name='mae_loss'):
        super().__init__(name=name)

    def call(self, y_true, y_pred):
        return tf.reduce_mean(tf.abs(y_true - y_pred))

    def get_config(self):
        base_config = super().get_config()
        return {**base_config}

class LogNseLoss(Loss):
    def __init__(self, epsilon=1e-6, name='lognse_loss'):
        super().__init__(name=name)
        self.epsilon = epsilon  # Small constant to avoid log(0)

    def call(self, y_true, y_pred):
        # Ensure y_true and y_pred are non-negative
        y_true = tf.maximum(y_true, 0.0)
        y_pred = tf.maximum(y_pred, 0.0)

        # Add epsilon to avoid log(0)
        y_true_log = tf.math.log(y_true + self.epsilon)
        y_pred_log = tf.math.log(y_pred + self.epsilon)

        # Handle cases where y_true_log has zero variance
        mean_true_log = tf.reduce_mean(y_true_log)
        denominator = tf.reduce_sum(tf.square(y_true_log - mean_true_log))
        # Replace zero denominators with a small constant
        denominator = tf.where(tf.equal(denominator, 0.0), K.epsilon(), denominator)

        numerator = tf.reduce_sum(tf.square(y_true_log - y_pred_log))

        nse = 1 - numerator / denominator
        # Ensure nse is within a valid range [-1, 1]
        nse = tf.clip_by_value(nse, -1.0, 1.0)

        # Return the loss
        loss = 1 - nse  # Subtract from 1 to convert to a loss
        return loss

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "epsilon": self.epsilon}


def kge_metric(y_true, y_pred):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    
    mean_true = tf.reduce_mean(y_true)
    mean_pred = tf.reduce_mean(y_pred)
    std_true = tf.math.reduce_std(y_true)
    std_pred = tf.math.reduce_std(y_pred)
    covariance = tf.reduce_mean((y_true - mean_true) * (y_pred - mean_pred))
    correlation = covariance / (std_true * std_pred + K.epsilon())
    std_ratio = std_pred / (std_true + K.epsilon())
    bias_ratio = mean_pred / (mean_true + K.epsilon())
    kge = 1 - tf.sqrt(
        tf.square(correlation - 1) + tf.square(std_ratio - 1) + tf.square(bias_ratio - 1)
    )
    return kge

def nse_metric(y_true, y_pred):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    
    mean_true = tf.reduce_mean(y_true)
    numerator = tf.reduce_sum(tf.square(y_true - y_pred))
    denominator = tf.reduce_sum(tf.square(y_true - mean_true))
    nse = 1 - numerator / (denominator + K.epsilon())
    return nse


def get_loss_function(loss_name, loss_params=None):
    available_loss_functions = {
        'nse_loss': NseLoss,
        'kge_loss': KgeLoss,
        'expectile_loss': ExpectileLoss,
        'quantile_loss': QuantileLoss,
        'huber_loss': HuberLoss,
        'weighted_mse_loss': WeightedMSELoss,
        'mae_loss': MaeLoss,
        'log_nse':LogNseLoss
    }

    if loss_name not in available_loss_functions:
        raise ValueError(f"Invalid loss function name: {loss_name}")

    loss_cls = available_loss_functions[loss_name]

    if loss_params:
        return loss_cls(**loss_params)
    else:
        return loss_cls()

def get_metric_function(metric_name):
    available_metrics = {
        'nse_metric': nse_metric,
        'kge_metric': kge_metric,
        'mae': tf.keras.metrics.MeanAbsoluteError(),
        'mse': tf.keras.metrics.MeanSquaredError(),
        # Add more metrics if needed
    }

    if metric_name not in available_metrics:
        raise ValueError(f"Invalid metric name: {metric_name}")

    return available_metrics[metric_name]



================================================
File: metrics.py
================================================
import logging
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from scipy import stats, signal
from xarray.core.dataarray import DataArray

import utils
from errors import AllNaNError

LOGGER = logging.getLogger(__name__)


def _check_all_nan(obs, sim):
    # Use np.isnan to check for missing values in NumPy arrays
    if np.all(np.isnan(obs)):
        raise ValueError("Observed data contains all NaN values.")
    if np.all(np.isnan(sim)):
        raise ValueError("Simulated data contains all NaN values.")


def get_available_metrics() -> List[str]:
    """Get list of available metrics.

    Returns
    -------
    List[str]
        List of implemented metric names.
    """
    metrics = [
        "NSE", "MSE", "RMSE", "KGE", "Alpha-NSE", "Pearson-r", "Beta-KGE", "Beta-NSE", "FHV", "FMS", "FLV",
        "Peak-Timing", "Missed-Peaks", "Peak-MAPE"
    ]
    return metrics


def _validate_inputs(obs: DataArray, sim: DataArray):
    if obs.shape != sim.shape:
        raise RuntimeError("Shapes of observations and simulations must match")

    if (len(obs.shape) > 1) and (obs.shape[1] > 1):
        raise RuntimeError("Metrics only defined for time series (1d or 2d with second dimension 1)")


def _mask_valid(obs: DataArray, sim: DataArray) -> Tuple[DataArray, DataArray]:
    # mask of invalid entries. NaNs in simulations can happen during validation/testing
    idx = (~sim.isnull()) & (~obs.isnull())

    obs = obs[idx]
    sim = sim[idx]

    return obs, sim


def _get_fdc(da: DataArray) -> np.ndarray:
    return da.sortby(da, ascending=False).values


def nse(obs: DataArray, sim: DataArray) -> float:
    r"""Calculate Nash-Sutcliffe Efficiency [#]_
    
    Nash-Sutcliffe Efficiency is the R-square between observed and simulated discharge.
    
    .. math:: \text{NSE} = 1 - \frac{\sum_{t=1}^{T}(Q_m^t - Q_o^t)^2}{\sum_{t=1}^T(Q_o^t - \overline{Q}_o)^2},
    
    where :math:`Q_m` are the simulations (here, `sim`) and :math:`Q_o` are observations (here, `obs`).
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.

    Returns
    -------
    float
        Nash-Sutcliffe Efficiency 
        
    References
    ----------
    .. [#] Nash, J. E.; Sutcliffe, J. V. (1970). "River flow forecasting through conceptual models part I - A 
        discussion of principles". Journal of Hydrology. 10 (3): 282-290. doi:10.1016/0022-1694(70)90255-6.

    """

    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    denominator = ((obs - obs.mean())**2).sum()
    numerator = ((sim - obs)**2).sum()

    value = 1 - numerator / denominator

    return float(value)


def mse(obs: DataArray, sim: DataArray) -> float:
    r"""Calculate mean squared error.
    
    .. math:: \text{MSE} = \frac{1}{T}\sum_{t=1}^T (\widehat{y}_t - y_t)^2,
    
    where :math:`\widehat{y}` are the simulations (here, `sim`) and :math:`y` are observations 
    (here, `obs`).
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.

    Returns
    -------
    float
        Mean squared error. 

    """

    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    return float(((sim - obs)**2).mean())


def rmse(obs: DataArray, sim: DataArray) -> float:
    r"""Calculate root mean squared error.
    
    .. math:: \text{RMSE} = \sqrt{\frac{1}{T}\sum_{t=1}^T (\widehat{y}_t - y_t)^2},
    
    where :math:`\widehat{y}` are the simulations (here, `sim`) and :math:`y` are observations 
    (here, `obs`).
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.

    Returns
    -------
    float
        Root mean sqaured error.

    """

    return np.sqrt(mse(obs, sim))


def alpha_nse(obs: DataArray, sim: DataArray) -> float:
    r"""Calculate the alpha NSE decomposition [#]_
    
    The alpha NSE decomposition is the fraction of the standard deviations of simulations and observations.
    
    .. math:: \alpha = \frac{\sigma_s}{\sigma_o},
    
    where :math:`\sigma_s` is the standard deviation of the simulations (here, `sim`) and :math:`\sigma_o` is the 
    standard deviation of the observations (here, `obs`).
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.

    Returns
    -------
    float
        Alpha NSE decomposition.
        
    References
    ----------
    .. [#] Gupta, H. V., Kling, H., Yilmaz, K. K., & Martinez, G. F. (2009). Decomposition of the mean squared error 
        and NSE performance criteria: Implications for improving hydrological modelling. Journal of hydrology, 377(1-2),
        80-91.

    """

    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    return float(sim.std() / obs.std())


def beta_nse(obs: DataArray, sim: DataArray) -> float:
    r"""Calculate the beta NSE decomposition [#]_

    The beta NSE decomposition is the difference of the mean simulation and mean observation divided by the standard 
    deviation of the observations.

    .. math:: \beta = \frac{\mu_s - \mu_o}{\sigma_o},
    
    where :math:`\mu_s` is the mean of the simulations (here, `sim`), :math:`\mu_o` is the mean of the observations 
    (here, `obs`) and :math:`\sigma_o` the standard deviation of the observations.

    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.

    Returns
    -------
    float
        Beta NSE decomposition.

    References
    ----------
    .. [#] Gupta, H. V., Kling, H., Yilmaz, K. K., & Martinez, G. F. (2009). Decomposition of the mean squared error 
        and NSE performance criteria: Implications for improving hydrological modelling. Journal of hydrology, 377(1-2),
        80-91.

    """
    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    return float((sim.mean() - obs.mean()) / obs.std())


def beta_kge(obs: DataArray, sim: DataArray) -> float:
    r"""Calculate the beta KGE term [#]_
    
    The beta term of the Kling-Gupta Efficiency is defined as the fraction of the means.
    
    .. math:: \beta_{\text{KGE}} = \frac{\mu_s}{\mu_o},
    
    where :math:`\mu_s` is the mean of the simulations (here, `sim`) and :math:`\mu_o` is the mean of the observations 
    (here, `obs`).
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.

    Returns
    -------
    float
        Beta NSE decomposition.

    References
    ----------
    .. [#] Gupta, H. V., Kling, H., Yilmaz, K. K., & Martinez, G. F. (2009). Decomposition of the mean squared error 
        and NSE performance criteria: Implications for improving hydrological modelling. Journal of hydrology, 377(1-2),
        80-91.

    """
    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    return float(sim.mean() / obs.mean())


def kge(obs: DataArray, sim: DataArray, weights: List[float] = [1., 1., 1.]) -> float:
    r"""Calculate the Kling-Gupta Efficieny [#]_
    
    .. math:: 
        \text{KGE} = 1 - \sqrt{[ s_r (r - 1)]^2 + [s_\alpha ( \alpha - 1)]^2 + 
            [s_\beta(\beta_{\text{KGE}} - 1)]^2},
            
    where :math:`r` is the correlation coefficient, :math:`\alpha` the :math:`\alpha`-NSE decomposition, 
    :math:`\beta_{\text{KGE}}` the fraction of the means and :math:`s_r, s_\alpha, s_\beta` the corresponding weights
    (here the three float values in the `weights` parameter).
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.
    weights : List[float]
        Weighting factors of the 3 KGE parts, by default each part has a weight of 1.

    Returns
    -------
    float
        Kling-Gupta Efficiency
    
    References
    ----------
    .. [#] Gupta, H. V., Kling, H., Yilmaz, K. K., & Martinez, G. F. (2009). Decomposition of the mean squared error 
        and NSE performance criteria: Implications for improving hydrological modelling. Journal of hydrology, 377(1-2),
        80-91.

    """
    if len(weights) != 3:
        raise ValueError("Weights of the KGE must be a list of three values")

    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    if len(obs) < 2:
        return np.nan

    r, _ = stats.pearsonr(obs.values, sim.values)

    alpha = sim.std() / obs.std()
    beta = sim.mean() / obs.mean()

    value = (weights[0] * (r - 1)**2 + weights[1] * (alpha - 1)**2 + weights[2] * (beta - 1)**2)

    return 1 - np.sqrt(float(value))


def pearsonr(obs: DataArray, sim: DataArray) -> float:
    """Calculate pearson correlation coefficient (using scipy.stats.pearsonr)

    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.

    Returns
    -------
    float
        Pearson correlation coefficient

    """

    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    if len(obs) < 2:
        return np.nan

    r, _ = stats.pearsonr(obs.values, sim.values)

    return float(r)


def fdc_fms(obs: DataArray, sim: DataArray, lower: float = 0.2, upper: float = 0.7) -> float:
    r"""Calculate the slope of the middle section of the flow duration curve [#]_
    
    .. math:: 
        \%\text{BiasFMS} = \frac{\left | \log(Q_{s,\text{lower}}) - \log(Q_{s,\text{upper}}) \right | - 
            \left | \log(Q_{o,\text{lower}}) - \log(Q_{o,\text{upper}}) \right |}{\left | 
            \log(Q_{s,\text{lower}}) - \log(Q_{s,\text{upper}}) \right |} \times 100,
            
    where :math:`Q_{s,\text{lower/upper}}` corresponds to the FDC of the simulations (here, `sim`) at the `lower` and
    `upper` bound of the middle section and :math:`Q_{o,\text{lower/upper}}` similarly for the observations (here, 
    `obs`).
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.
    lower : float, optional
        Lower bound of the middle section in range ]0,1[, by default 0.2
    upper : float, optional
        Upper bound of the middle section in range ]0,1[, by default 0.7
        
    Returns
    -------
    float
        Slope of the middle section of the flow duration curve.
    
    References
    ----------
    .. [#] Yilmaz, K. K., Gupta, H. V., and Wagener, T. ( 2008), A process-based diagnostic approach to model 
        evaluation: Application to the NWS distributed hydrologic model, Water Resour. Res., 44, W09417, 
        doi:10.1029/2007WR006716. 
    """
    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    if len(obs) < 1:
        return np.nan

    if any([(x <= 0) or (x >= 1) for x in [upper, lower]]):
        raise ValueError("upper and lower have to be in range ]0,1[")

    if lower >= upper:
        raise ValueError("The lower threshold has to be smaller than the upper.")

    # get arrays of sorted (descending) discharges
    obs = _get_fdc(obs)
    sim = _get_fdc(sim)

    # for numerical reasons change 0s to 1e-6. Simulations can still contain negatives, so also reset those.
    sim[sim <= 0] = 1e-6
    obs[obs == 0] = 1e-6

    # calculate fms part by part
    qsm_lower = np.log(sim[np.round(lower * len(sim)).astype(int)])
    qsm_upper = np.log(sim[np.round(upper * len(sim)).astype(int)])
    qom_lower = np.log(obs[np.round(lower * len(obs)).astype(int)])
    qom_upper = np.log(obs[np.round(upper * len(obs)).astype(int)])

    fms = ((qsm_lower - qsm_upper) - (qom_lower - qom_upper)) / (qom_lower - qom_upper + 1e-6)

    return fms * 100


def fdc_fhv(obs: DataArray, sim: DataArray, h: float = 0.02) -> float:
    r"""Calculate the peak flow bias of the flow duration curve [#]_
    
    .. math:: \%\text{BiasFHV} = \frac{\sum_{h=1}^{H}(Q_{s,h} - Q_{o,h})}{\sum_{h=1}^{H}Q_{o,h}} \times 100,
    
    where :math:`Q_s` are the simulations (here, `sim`), :math:`Q_o` the observations (here, `obs`) and `H` is the upper
    fraction of flows of the FDC (here, `h`). 
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.
    h : float, optional
        Fraction of upper flows to consider as peak flows of range ]0,1[, be default 0.02.
        
    Returns
    -------
    float
        Peak flow bias.
    
    References
    ----------
    .. [#] Yilmaz, K. K., Gupta, H. V., and Wagener, T. ( 2008), A process-based diagnostic approach to model 
        evaluation: Application to the NWS distributed hydrologic model, Water Resour. Res., 44, W09417, 
        doi:10.1029/2007WR006716. 
    """
    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    if len(obs) < 1:
        return np.nan

    if (h <= 0) or (h >= 1):
        raise ValueError("h has to be in range ]0,1[. Consider small values, e.g. 0.02 for 2% peak flows")

    # get arrays of sorted (descending) discharges
    obs = _get_fdc(obs)
    sim = _get_fdc(sim)

    # subset data to only top h flow values
    obs = obs[:np.round(h * len(obs)).astype(int)]
    sim = sim[:np.round(h * len(sim)).astype(int)]

    fhv = np.sum(sim - obs) / np.sum(obs)

    return fhv * 100


def fdc_flv(obs: DataArray, sim: DataArray, l: float = 0.3) -> float:
    r"""Calculate the low flow bias of the flow duration curve [#]_
    
    .. math:: 
        \%\text{BiasFMS} = -1 \frac{\sum_{l=1}^{L}[\log(Q_{s,l}) - \log(Q_{s,L})] - \sum_{l=1}^{L}[\log(Q_{o,l})
            - \log(Q_{o,L})]}{\sum_{l=1}^{L}[\log(Q_{o,l}) - \log(Q_{o,L})]} \times 100,
    
    where :math:`Q_s` are the simulations (here, `sim`), :math:`Q_o` the observations (here, `obs`) and `L` is the lower
    fraction of flows of the FDC (here, `l`). 
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.
    l : float, optional
        Fraction of lower flows to consider as low flows of range ]0,1[, be default 0.3.
        
    Returns
    -------
    float
        Low flow bias.
    
    References
    ----------
    .. [#] Yilmaz, K. K., Gupta, H. V., and Wagener, T. ( 2008), A process-based diagnostic approach to model 
        evaluation: Application to the NWS distributed hydrologic model, Water Resour. Res., 44, W09417, 
        doi:10.1029/2007WR006716. 
    """
    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    if len(obs) < 1:
        return np.nan

    if (l <= 0) or (l >= 1):
        raise ValueError("l has to be in range ]0,1[. Consider small values, e.g. 0.3 for 30% low flows")

    # get arrays of sorted (descending) discharges
    obs = _get_fdc(obs)
    sim = _get_fdc(sim)

    # for numerical reasons change 0s to 1e-6. Simulations can still contain negatives, so also reset those.
    sim[sim <= 0] = 1e-6
    obs[obs == 0] = 1e-6

    obs = obs[-np.round(l * len(obs)).astype(int):]
    sim = sim[-np.round(l * len(sim)).astype(int):]

    # transform values to log scale
    obs = np.log(obs)
    sim = np.log(sim)

    # calculate flv part by part
    qsl = np.sum(sim - sim.min())
    qol = np.sum(obs - obs.min())

    flv = -1 * (qsl - qol) / (qol + 1e-6)

    return flv * 100


def mean_peak_timing(obs: DataArray,
                     sim: DataArray,
                     window: int = None,
                     resolution: str = '1D',
                     datetime_coord: str = None) -> float:
    """Mean difference in peak flow timing.
    
    Uses scipy.find_peaks to find peaks in the observed time series. Starting with all observed peaks, those with a
    prominence of less than the standard deviation of the observed time series are discarded. Next, the lowest peaks
    are subsequently discarded until all remaining peaks have a distance of at least 100 steps. Finally, the
    corresponding peaks in the simulated time series are searched in a window of size `window` on either side of the
    observed peaks and the absolute time differences between observed and simulated peaks is calculated.
    The final metric is the mean absolute time difference across all peaks. For more details, see Appendix of [#]_
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.
    window : int, optional
        Size of window to consider on each side of the observed peak for finding the simulated peak. That is, the total
        window length to find the peak in the simulations is :math:`2 * \\text{window} + 1` centered at the observed
        peak. The default depends on the temporal resolution, e.g. for a resolution of '1D', a window of 3 is used and 
        for a resolution of '1H' the the window size is 12.
    resolution : str, optional
        Temporal resolution of the time series in pandas format, e.g. '1D' for daily and '1H' for hourly.
    datetime_coord : str, optional
        Name of datetime coordinate. Tried to infer automatically if not specified.
        

    Returns
    -------
    float
        Mean peak time difference.

    References
    ----------
    .. [#] Kratzert, F., Klotz, D., Hochreiter, S., and Nearing, G. S.: A note on leveraging synergy in multiple 
        meteorological datasets with deep learning for rainfall-runoff modeling, Hydrol. Earth Syst. Sci. Discuss., 
        https://doi.org/10.5194/hess-2020-221, in review, 2020. 
    """
    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations (scipy's find_peaks doesn't guarantee correctness with NaNs)
    obs, sim = _mask_valid(obs, sim)

    # heuristic to get indices of peaks and their corresponding height.
    peaks, _ = signal.find_peaks(obs.values, distance=100, prominence=np.std(obs.values))

    # infer name of datetime index
    if datetime_coord is None:
        datetime_coord = utils.infer_datetime_coord(obs)

    if window is None:
        # infer a reasonable window size
        window = max(int(utils.get_frequency_factor('12H', resolution)), 3)

    # evaluate timing
    timing_errors = []
    for idx in peaks:
        # skip peaks at the start and end of the sequence and peaks around missing observations
        # (NaNs that were removed in obs & sim would result in windows that span too much time).
        if (idx - window < 0) or (idx + window >= len(obs)) or (pd.date_range(obs[idx - window][datetime_coord].values,
                                                                              obs[idx + window][datetime_coord].values,
                                                                              freq=resolution).size != 2 * window + 1):
            continue

        # check if the value at idx is a peak (both neighbors must be smaller)
        if (sim[idx] > sim[idx - 1]) and (sim[idx] > sim[idx + 1]):
            peak_sim = sim[idx]
        else:
            # define peak around idx as the max value inside of the window
            values = sim[idx - window:idx + window + 1]
            peak_sim = values[values.argmax()]

        # get xarray object of qobs peak, for getting the date and calculating the datetime offset
        peak_obs = obs[idx]

        # calculate the time difference between the peaks
        delta = peak_obs.coords[datetime_coord] - peak_sim.coords[datetime_coord]

        timing_error = np.abs(delta.values / pd.to_timedelta(resolution))

        timing_errors.append(timing_error)

    return np.mean(timing_errors) if len(timing_errors) > 0 else np.nan


def missed_peaks(obs: DataArray,
                 sim: DataArray,
                 window: int = None,
                 resolution: str = '1D',
                 percentile: float = 80,
                 datetime_coord: str = None) -> float:
    """Fraction of missed peaks.
    
    Uses scipy.find_peaks to find peaks in the observed and simulated time series above a certain percentile. Counts
    the number of peaks in obs that do not exist in sim within the specified window.

    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.
    window : int, optional
        Size of window to consider on each side of the observed peak for finding the simulated peak. That is, the total
        window length to find the peak in the simulations is :math:`2 * \\text{window} + 1` centered at the observed
        peak. The default depends on the temporal resolution, e.g. for a resolution of '1D', a window of 1 is used and 
        for a resolution of '1H' the the window size is 12. Note that this is a different default window size than is
        used in the peak-timing metric for '1D'.
    resolution : str, optional
        Temporal resolution of the time series in pandas format, e.g. '1D' for daily and '1H' for hourly.
    percentile: float, optional
        Only consider peaks above this flow percentile (0, 100).
    datetime_coord : str, optional
        Name of datetime coordinate. Tried to infer automatically if not specified.

    Returns
    -------
    float
        Fraction of missed peaks.   
    """
    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations (scipy's find_peaks doesn't guarantee correctness with NaNs)
    obs, sim = _mask_valid(obs, sim)

    # minimum height of a peak, as defined by percentile, which can be passed
    min_obs_height = np.percentile(obs.values, percentile)
    min_sim_height = np.percentile(sim.values, percentile)

    # get time indices of peaks in obs and sim.
    peaks_obs_times, _ = signal.find_peaks(obs, distance=30, height=min_obs_height)
    peaks_sim_times, _ = signal.find_peaks(sim, distance=30, height=min_sim_height)

    if len(peaks_obs_times) == 0:
        return 0.

    # infer name of datetime index
    if datetime_coord is None:
        datetime_coord = utils.infer_datetime_coord(obs)

    # infer a reasonable window size
    if window is None:
        window = max(int(utils.get_frequency_factor('12H', resolution)), 1)

    # count missed peaks
    missed_events = 0

    for idx in peaks_obs_times:

        # skip peaks at the start and end of the sequence and peaks around missing observations
        # (NaNs that were removed in obs & sim would result in windows that span too much time).
        if (idx - window < 0) or (idx + window >= len(obs)) or (pd.date_range(obs[idx - window][datetime_coord].values,
                                                                              obs[idx + window][datetime_coord].values,
                                                                              freq=resolution).size != 2 * window + 1):
            continue

        nearby_peak_sim_index = np.where(np.abs(peaks_sim_times - idx) <= window)[0]
        if len(nearby_peak_sim_index) == 0:
            missed_events += 1

    return missed_events / len(peaks_obs_times)


def mean_absolute_percentage_peak_error(obs: DataArray, sim: DataArray) -> float:
    r"""Calculate the mean absolute percentage error (MAPE) for peaks

    .. math:: \text{MAPE}_\text{peak} = \frac{1}{P}\sum_{p=1}^{P} \left |\frac{Q_{s,p} - Q_{o,p}}{Q_{o,p}} \right | \times 100,

    where :math:`Q_{s,p}` are the simulated peaks (here, `sim`), :math:`Q_{o,p}` the observed peaks (here, `obs`) and
    `P` is the number of peaks.

    Uses scipy.find_peaks to find peaks in the observed time series. The observed peaks indices are used to subset
    observed and simulated flows. Finally, the MAPE metric is calculated as the mean absolute percentage error
    of observed peak flows and corresponding simulated flows.

    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.

    Returns
    -------
    float
        Mean absolute percentage error (MAPE) for peaks.
    """
    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    # return np.nan if there are no valid observed or simulated values
    if obs.size == 0 or sim.size == 0:
        return np.nan

    # heuristic to get indices of peaks and their corresponding height.
    peaks, _ = signal.find_peaks(obs.values, distance=100, prominence=np.std(obs.values))

    # check if any peaks exist, otherwise return np.nan
    if peaks.size == 0:
        return np.nan

    # subset data to only peak values
    obs = obs[peaks].values
    sim = sim[peaks].values

    # calculate the mean absolute percentage peak error
    peak_mape = np.sum(np.abs((sim - obs) / obs)) / peaks.size * 100

    return peak_mape


def calculate_all_metrics(obs: DataArray,
                          sim: DataArray,
                          resolution: str = "1D",
                          datetime_coord: str = None) -> Dict[str, float]:
    """Calculate all metrics with default values.
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.
    resolution : str, optional
        Temporal resolution of the time series in pandas format, e.g. '1D' for daily and '1H' for hourly.
    datetime_coord : str, optional
        Datetime coordinate in the passed DataArray. Tried to infer automatically if not specified.
        
    Returns
    -------
    Dict[str, float]
        Dictionary with keys corresponding to metric name and values corresponding to metric values.

    Raises
    ------
    AllNaNError
        If all observations or all simulations are NaN.
    """
    _check_all_nan(obs, sim)

    results = {
        "NSE": nse(obs, sim),
        "MSE": mse(obs, sim),
        "RMSE": rmse(obs, sim),
        "KGE": kge(obs, sim),
        "Alpha-NSE": alpha_nse(obs, sim),
        "Beta-KGE": beta_kge(obs, sim),
        "Beta-NSE": beta_nse(obs, sim),
        "Pearson-r": pearsonr(obs, sim),
        "FHV": fdc_fhv(obs, sim),
        "FMS": fdc_fms(obs, sim),
        "FLV": fdc_flv(obs, sim),
        "Peak-Timing": mean_peak_timing(obs, sim, resolution=resolution, datetime_coord=datetime_coord),
        "Peak-MAPE": mean_absolute_percentage_peak_error(obs, sim)
    }

    return results


def calculate_metrics(obs: DataArray,
                      sim: DataArray,
                      metrics: List[str],
                      resolution: str = "1D",
                      datetime_coord: str = None) -> Dict[str, float]:
    """Calculate specific metrics with default values.
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.
    metrics : List[str]
        List of metric names.
    resolution : str, optional
        Temporal resolution of the time series in pandas format, e.g. '1D' for daily and '1H' for hourly.
    datetime_coord : str, optional
        Datetime coordinate in the passed DataArray. Tried to infer automatically if not specified.

    Returns
    -------
    Dict[str, float]
        Dictionary with keys corresponding to metric name and values corresponding to metric values.

    Raises
    ------
    AllNaNError
        If all observations or all simulations are NaN.
    """
    if 'all' in metrics:
        return calculate_all_metrics(obs, sim, resolution=resolution)

    _check_all_nan(obs, sim)

    values = {}
    for metric in metrics:
        if metric.lower() == "nse":
            values["NSE"] = nse(obs, sim)
        elif metric.lower() == "mse":
            values["MSE"] = mse(obs, sim)
        elif metric.lower() == "rmse":
            values["RMSE"] = rmse(obs, sim)
        elif metric.lower() == "kge":
            values["KGE"] = kge(obs, sim)
        elif metric.lower() == "alpha-nse":
            values["Alpha-NSE"] = alpha_nse(obs, sim)
        elif metric.lower() == "beta-kge":
            values["Beta-KGE"] = beta_kge(obs, sim)
        elif metric.lower() == "beta-nse":
            values["Beta-NSE"] = beta_nse(obs, sim)
        elif metric.lower() == "pearson-r":
            values["Pearson-r"] = pearsonr(obs, sim)
        elif metric.lower() == "fhv":
            values["FHV"] = fdc_fhv(obs, sim)
        elif metric.lower() == "fms":
            values["FMS"] = fdc_fms(obs, sim)
        elif metric.lower() == "flv":
            values["FLV"] = fdc_flv(obs, sim)
        elif metric.lower() == "peak-timing":
            values["Peak-Timing"] = mean_peak_timing(obs, sim, resolution=resolution, datetime_coord=datetime_coord)
        elif metric.lower() == "missed-peaks":
            values["Missed-Peaks"] = missed_peaks(obs, sim, resolution=resolution, datetime_coord=datetime_coord)
        elif metric.lower() == "peak-mape":
            values["Peak-MAPE"] = mean_absolute_percentage_peak_error(obs, sim)
        else:
            raise RuntimeError(f"Unknown metric {metric}")

    return values


def _check_all_nan(obs: DataArray, sim: DataArray):
    """Check if all observations or simulations are NaN and raise an exception if this is the case.

    Raises
    ------
    AllNaNError
        If all observations or all simulations are NaN.
    """
    if all(obs.isnull()):
        raise AllNaNError("All observed values are NaN, thus metrics will be NaN, too.")
    if all(sim.isnull()):
        raise AllNaNError("All simulated values are NaN, thus metrics will be NaN, too.")



================================================
File: models.py
================================================
# models.py

import tensorflow as tf
from keras.models import Sequential, Model
from keras.layers import (
    Dense, LSTM, GRU, Dropout, Input, Conv1D, MaxPooling1D,
    Flatten, TimeDistributed, Bidirectional, LayerNormalization,
    SimpleRNN, Attention, BatchNormalization, Activation,
    RepeatVector, Reshape, Embedding, MultiHeadAttention, Add, Concatenate, GlobalAveragePooling1D
)
import keras
import numpy as np
import losses  # To access loss functions and metrics if needed
from keras import backend as K

def get_model(config, number_of_features):
    model_type = config.get('model_type', 'LSTM').upper()
    model_params = config.get('model_params', {})
    n_steps_in = config['n_steps_in']
    n_steps_out = config['n_steps_out']

    if model_type == 'LSTM':
        model = build_lstm_model(n_steps_in, n_steps_out, number_of_features, model_params)
    elif model_type == 'GRU':
        model = build_gru_model(n_steps_in, n_steps_out, number_of_features, model_params)
    elif model_type == 'TRANSFORMER':
        # Retrieve num_time_features from model_params
        num_time_features = model_params.get('num_time_features')
        if num_time_features is None:
            raise ValueError("num_time_features must be specified in model_params for TRANSFORMER model.")
        model = build_transformer_model_with_temporal_embeddings(
            n_steps_in, n_steps_out, number_of_features, num_time_features, model_params
        )
    elif model_type == 'TCN':
        model = build_tcn_model(n_steps_in, n_steps_out, number_of_features, model_params)
    elif model_type == 'CNN_RNN':
        model = build_cnn_rnn_model(n_steps_in, n_steps_out, number_of_features, model_params)
    elif model_type == 'SEQ2SEQ':
        model = build_seq2seq_model(n_steps_in, n_steps_out, number_of_features, model_params)
    else:
        raise ValueError(f"Invalid model type: {model_type}")

    return model

def build_lstm_model(n_steps_in, n_steps_out, number_of_features, model_params):
    units = model_params.get('units', 256)
    dropout = model_params.get('dropout', 0.4)
    activation = model_params.get('activation', 'tanh')

    model = Sequential()
    model.add(Input(shape=(n_steps_in, number_of_features)))
    model.add(LSTM(units, activation=activation))
    model.add(Dropout(dropout))
    model.add(Dense(n_steps_out, dtype='float32'))
    return model

def build_gru_model(n_steps_in, n_steps_out, number_of_features, model_params):
    units = model_params.get('units', 256)
    dropout = model_params.get('dropout', 0.4)
    activation = model_params.get('activation', 'tanh')

    model = Sequential()
    model.add(Input(shape=(n_steps_in, number_of_features)))
    model.add(GRU(units, activation=activation))
    model.add(Dropout(dropout))
    model.add(Dense(n_steps_out, dtype='float32'))
    return model

def positional_encoding(seq_len, d_model):
    """Generate a positional encoding matrix with sine and cosine functions."""
    angle_rads = np.arange(seq_len)[:, np.newaxis] / np.power(
        10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model)
    )

    # Apply sin to even indices (2i) and cos to odd indices (2i+1)
    pos_encoding = np.zeros((seq_len, d_model))
    pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])
    pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])
    return tf.constant(pos_encoding, dtype=tf.float32)  # Shape: (seq_len, d_model)

def temporal_encoding(time_features, time_feature_sizes):
    """
    Compute temporal embeddings from time features.

    Args:
        time_features: tensor of shape (batch_size, seq_len, num_time_features), containing the time features ti(k)
        time_feature_sizes: list or tensor of integers Ni, the total number of values for each time feature i

    Returns:
        temporal_embeddings: tensor of shape (batch_size, seq_len, num_time_features), containing the di(k)
    """
    time_feature_sizes = tf.constant(time_feature_sizes, dtype=tf.float32)
    time_feature_sizes = tf.reshape(time_feature_sizes, (1, 1, -1))  # Shape: (1, 1, num_time_features)
    di = (time_features / time_feature_sizes) - 0.5
    return di

def transformer_encoder(inputs, head_size, num_heads, ff_dim, hidden_dim, dropout=0):
    # Attention and Normalization
    x = MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads, dropout=dropout
    )(inputs, inputs)
    x = Dropout(dropout)(x)
    x = Add()([x, inputs])
    res = LayerNormalization(epsilon=1e-6)(x)  # Add and Norm

    # Feed Forward Part
    x = Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(res)
    x = Dropout(dropout)(x)
    x = Conv1D(filters=hidden_dim, kernel_size=1)(x)
    x = Add()([x, res])
    x = LayerNormalization(epsilon=1e-6)(x)  # Add and Norm
    return x

def build_transformer_model_with_temporal_embeddings(
    n_steps_in, n_steps_out, number_of_features, num_time_features, model_params
):
    num_heads = model_params.get('num_heads', 8)  # Tunable
    head_size = model_params.get('head_size', 16)  # Tunable
    ff_dim = model_params.get('ff_dim', 128)
    dropout_rate = model_params.get('dropout_rate', 0.3)
    num_transformer_blocks = model_params.get('num_transformer_blocks', 6)  # Tunable
    mlp_units = model_params.get('mlp_units', [128])  # Tunable
    mlp_dropout_rate = model_params.get('mlp_dropout_rate', 0.3)
    hidden_dim = model_params.get('hidden_dim', 128)  # Dimension of model hidden layer

    # Inputs
    inputs = keras.Input(shape=(n_steps_in, number_of_features))
    time_inputs = keras.Input(shape=(n_steps_in, num_time_features))

    # Generate positional encoding
    pos_encoding = positional_encoding(n_steps_in, hidden_dim)  # Shape: (n_steps_in, hidden_dim)
    pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)

    # Compute temporal embeddings
    time_feature_sizes = model_params.get('time_feature_sizes', [2022, 12, 31, 7, 366])
    temporal_embeddings = temporal_encoding(time_inputs, time_feature_sizes)  # Shape: (batch_size, n_steps_in, num_time_features)

    # Project inputs and temporal embeddings to hidden_dim
    EX = Dense(hidden_dim)(inputs)  # Shape: (batch_size, n_steps_in, hidden_dim)
    ET = Dense(hidden_dim)(temporal_embeddings)  # Shape: (batch_size, n_steps_in, hidden_dim)

    # Sum them with positional encoding (broadcasting will handle the addition)
    x = EX + ET + pos_encoding  # Broadcasting adds pos_encoding to each example in the batch

    # Transformer Blocks
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, hidden_dim, dropout_rate)

    # Pooling and Output Layers
    x = GlobalAveragePooling1D(data_format="channels_last")(x)
    for dim in mlp_units:
        x = Dense(dim, activation="relu")(x)
        x = Dropout(mlp_dropout_rate)(x)
    outputs = Dense(n_steps_out)(x)

    # Model with two inputs: features and time features
    return keras.Model(inputs=[inputs, time_inputs], outputs=outputs)

def build_tcn_model(n_steps_in, n_steps_out, number_of_features, model_params):
    # You need to install keras-tcn: pip install keras-tcn
    from tcn import TCN

    units = model_params.get('units', 64)
    dropout = model_params.get('dropout', 0.2)
    kernel_size = model_params.get('kernel_size', 3)
    dilations = model_params.get('dilations', [1, 2, 4, 8])
    nb_stacks = model_params.get('nb_stacks', 1)
    use_skip_connections = model_params.get('use_skip_connections', True)

    inputs = Input(shape=(n_steps_in, number_of_features))
    x = TCN(nb_filters=units,
            kernel_size=kernel_size,
            dilations=dilations,
            nb_stacks=nb_stacks,
            use_skip_connections=use_skip_connections,
            dropout_rate=dropout)(inputs)
    outputs = Dense(n_steps_out, dtype='float32')(x)
    model = Model(inputs=inputs, outputs=outputs)
    return model

def build_cnn_rnn_model(n_steps_in, n_steps_out, number_of_features, model_params):
    conv_filters = model_params.get('conv_filters', 64)
    kernel_size = model_params.get('kernel_size', 3)
    pool_size = model_params.get('pool_size', 2)
    rnn_units = model_params.get('rnn_units', 128)
    dropout = model_params.get('dropout', 0.2)

    inputs = Input(shape=(n_steps_in, number_of_features))
    x = Conv1D(filters=conv_filters, kernel_size=kernel_size, activation='relu')(inputs)
    x = MaxPooling1D(pool_size=pool_size)(x)
    x = LSTM(rnn_units)(x)
    x = Dropout(dropout)(x)
    outputs = Dense(n_steps_out, dtype='float32')(x)
    model = Model(inputs=inputs, outputs=outputs)
    return model

def build_seq2seq_model(n_steps_in, n_steps_out, number_of_features, model_params):
    encoder_units = model_params.get('encoder_units', 128)
    decoder_units = model_params.get('decoder_units', 128)
    dropout = model_params.get('dropout', 0.2)
    activation = model_params.get('activation', 'tanh')

    # Encoder
    encoder_inputs = Input(shape=(n_steps_in, number_of_features))
    encoder = LSTM(encoder_units, activation=activation, return_state=True)
    encoder_outputs, state_h, state_c = encoder(encoder_inputs)
    encoder_states = [state_h, state_c]

    # Decoder
    decoder_inputs = RepeatVector(n_steps_out)(encoder_outputs)
    decoder_lstm = LSTM(decoder_units, activation=activation, return_sequences=True)
    decoder_outputs = decoder_lstm(decoder_inputs, initial_state=encoder_states)
    decoder_outputs = TimeDistributed(Dense(1))(decoder_outputs)
    decoder_outputs = tf.keras.layers.Flatten()(decoder_outputs)
    outputs = Dense(n_steps_out, dtype='float32')(decoder_outputs)

    model = Model(inputs=encoder_inputs, outputs=outputs)
    return model


================================================
File: notes.txt
================================================
apptainer shell --nv --bind /scratch/:/scratch/ /scratch/kdahal3/streamflow-forecasting/streamflow.sif


================================================
File: run_all_models.sh
================================================
#!/bin/bash

# Define the different values for n_steps_out and corresponding output_prefix
n_steps_out_values=(7)
output_prefix_suffix=("7day")

# Define the Model types
model_types=("LSTM")  # All supported models

# Define the loss functions and parameters
declare -A loss_functions
loss_functions=(
    # ["nse_loss"]=""
    # ["kge_loss"]=""
    # ["huber_loss"]=""
    ["quantile_low"]="quantile: 0.15"
    ["quantile_high"]="quantile: 0.85"
    # ["expectile_low"]="expectile: 0.15"
    # ["expectile_high"]="expectile: 0.85"
)

# Map loss function keys to actual loss function names
declare -A loss_function_names
loss_function_names=(
    # ["nse_loss"]="nse_loss"
    # ["kge_loss"]="kge_loss"
    # ["huber_loss"]="huber_loss"
    ["quantile_low"]="quantile_loss"
    ["quantile_high"]="quantile_loss"
    # ["expectile_low"]="expectile_loss"
    # ["expectile_high"]="expectile_loss"
)

# Create a folder for the configuration files if it doesn't exist
config_folder="config_files"
mkdir -p $config_folder

# Path to the base config file
base_config="config.yml"

# Loop over the Model types, n_steps_out values, and loss functions
for model_type in "${model_types[@]}"; do
    for i in "${!n_steps_out_values[@]}"; do
        # Set the n_steps_out and output_prefix for this run
        n_steps_out=${n_steps_out_values[$i]}
        output_suffix=${output_prefix_suffix[$i]}
        
        for loss_key in "${!loss_functions[@]}"; do
            loss_function=${loss_function_names[$loss_key]}
            loss_params=${loss_functions[$loss_key]}
            
            # Construct the output prefix using Model type, loss function, and day suffix
            output_prefix="camels_${model_type}_${loss_key}_${output_suffix}"
            
            # Construct the filename for the config file
            config_file="${config_folder}/config_${output_prefix}_nsteps_${n_steps_out}.yml"
            
            # Create a new config file for this run by copying the base config
            cp "$base_config" "$config_file"
            
            # Modify the copied config file using | as the delimiter to avoid conflicts
            sed -i "s|^n_steps_out:.*|n_steps_out: $n_steps_out|" "$config_file"
            sed -i "s|^output_prefix:.*|output_prefix: \"$output_prefix\"|" "$config_file"
            sed -i "s|^model_type:.*|model_type: \"$model_type\"|" "$config_file"
            sed -i "s|^loss_function:.*|loss_function: \"$loss_function\"|" "$config_file"
            
            # Remove any existing loss_params section
            sed -i "/^loss_params:/,/^[^[:space:]]/{/^loss_params:/!{/^[^[:space:]]/!d}}" "$config_file"
            
            # If loss_params is not empty, add it
            if [ -n "$loss_params" ]; then
                echo -e "\nloss_params:\n  $loss_params" >> "$config_file"
            fi
            
            # Adjust epochs if model_type is TRANSFORMER
            if [ "$model_type" == "TRANSFORMER" ]; then
                sed -i "s|^epochs:.*|epochs: 150|" "$config_file"
            else
                # Assuming the base config has epochs set to default value
                sed -i "s|^epochs:.*|epochs: 50|" "$config_file"
            fi
            
            # Submit the job and pass the specific config file as an argument
            sbatch submit_jobs.sh "$config_file"
            
            echo "Submitted job with model_type: $model_type, n_steps_out: $n_steps_out, loss_function: $loss_function, and output_prefix: $output_prefix using $config_file"
        done
    done
done



================================================
File: run_evaluation.sh
================================================
#!/bin/bash

# This script automates the evaluation of all pre-trained models
# on the validation dataset. It reconstructs the output directory names
# from the training script and calls the evaluation python script for each model.

# --- Configuration: Match this to your training script and environment ---

# Base directory containing all your model run folders
BASE_MODEL_DIR="/scratch/kdahal3/camels_losses"

# Define the different values for n_steps_out and corresponding output_prefix
n_steps_out_values=(1 7 30)
output_prefix_suffix=("1day" "7day" "30day")

# Define the Model types
model_types=("LSTM") # Add other models like "TRANSFORMER" if you trained them

# Define the loss functions and parameters
declare -A loss_functions
loss_functions=(
    # ["nse_loss"]=""
    # ["kge_loss"]=""
    # ["huber_loss"]=""
    # ["quantile_low"]="quantile: 0.15"
    # ["quantile_high"]="quantile: 0.85"
    ["expectile_low"]="expectile: 0.15"
    ["expectile_high"]="expectile: 0.85"
)

# --- Paths (relative to BASE_MODEL_DIR or absolute) ---
CONFIG_FOLDER="${BASE_MODEL_DIR}/config_files"
VAL_DATA_FOLDER="/scratch/kdahal3/FF/val_data"

# !!! THIS IS THE CORRECTED LINE !!!
# Point to the actual location of your evaluation script
PYTHON_SCRIPT_PATH="/scratch/kdahal3/FF/evaluate_validation.py"

# --- Evaluation Loop ---

echo "Starting evaluation of all trained models..."
echo "Base model directory: ${BASE_MODEL_DIR}"

# Loop over the Model types, n_steps_out values, and loss functions
for model_type in "${model_types[@]}"; do
    for i in "${!n_steps_out_values[@]}"; do
        n_steps_out=${n_steps_out_values[$i]}
        output_suffix=${output_prefix_suffix[$i]}
        
        for loss_key in "${!loss_functions[@]}"; do
            # Reconstruct the output prefix to find the model directory
            output_prefix="camels_${model_type}_${loss_key}_${output_suffix}"
            
            echo "-----------------------------------------------------------------"
            echo "Looking for model: $output_prefix"
            
            # Define paths for this specific model run using the BASE_MODEL_DIR
            MODEL_DIR="${BASE_MODEL_DIR}/${output_prefix}"
            CONFIG_FILE="${CONFIG_FOLDER}/config_${output_prefix}_nsteps_${n_steps_out}.yml"
            MODEL_PATH="${MODEL_DIR}/combined_model.keras"
            MIN_MAX_PATH="${MODEL_DIR}/min_max.csv"
            OUTPUT_DIR="${MODEL_DIR}/validation_results" # Save results in a subfolder

            # Check if the model file actually exists before trying to evaluate
            if [ -f "$MODEL_PATH" ]; then
                echo "Found model. Starting evaluation for $output_prefix"
                
                # Call the Python evaluation script with all the necessary arguments
                python "$PYTHON_SCRIPT_PATH" \
                  --config "$CONFIG_FILE" \
                  --model-path "$MODEL_PATH" \
                  --min-max-path "$MIN_MAX_PATH" \
                  --val-data-folder "$VAL_DATA_FOLDER" \
                  --output-dir "$OUTPUT_DIR"
                  
                echo "Evaluation finished for $output_prefix"
            else
                echo "Warning: Model file not found at $MODEL_PATH. Skipping evaluation."
            fi
        done
    done
done

echo "-----------------------------------------------------------------"
echo "All evaluations complete."


================================================
File: spatial_uncertainty_analysis.py
================================================
import os
import pandas as pd
import glob
import numpy as np
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cfeature
from functools import reduce
import re # Import the regular expression module

# --- CONFIGURATION ---
base_dir = '/scratch/kdahal3/camels_losses'
log_file_path = os.path.join(base_dir, 'merge_log.txt')
forecast_horizons = [1, 7, 30]
loss_functions = [
    "nse_loss", "kge_loss", "huber_loss", "quantile_low",
    "quantile_high", "expectile_low", "expectile_high"
]
coords_file = '/scratch/kdahal3/Caravan/attributes/camels/attributes_other_camels.csv'

def clean_and_zfill(station_id_series):
    return station_id_series.astype(str).str.strip().str.zfill(8)

# Clear old log file
if os.path.exists(log_file_path):
    os.remove(log_file_path)

def log_message(message):
    print(message)
    with open(log_file_path, 'a') as f:
        f.write(message + '\n')

log_message("="*60)
log_message("Starting spatial uncertainty analysis...")
log_message("="*60)

# --- 1. LOAD AND PREPARE COORDINATES DATAFRAME ---
log_message("\n--- STEP 1: Loading and Cleaning Coordinate Data ---")
try:
    coords_df = pd.read_csv(coords_file)
    coords_df['Station_ID'] = coords_df['gauge_id'].apply(lambda x: str(x).split('_')[1])
    coords_df['Station_ID'] = clean_and_zfill(coords_df['Station_ID'])
    coords_df = coords_df[['Station_ID', 'gauge_lat', 'gauge_lon']].drop_duplicates(subset=['Station_ID'])
    log_message(f"  Cleaned and prepared coordinates for {len(coords_df)} unique stations.")
    log_message(f"  Example coord Station_IDs:\n{coords_df.head().to_string()}")
    log_message("-" * 50)
except Exception as e:
    log_message(f"[FATAL ERROR] Could not load or process coordinate file: {e}")
    exit()

# --- 2. PROCESS DATA FOR EACH HORIZON ---
all_horizon_data = []

for fh in forecast_horizons:
    log_message(f"\n{'='*25} Processing {fh}-Day Forecast {'='*25}")
    horizon_key = f'{fh}day'
    
    model_dfs = []
    
    for loss_func in loss_functions:
        # ... (This part is working correctly, so keeping it concise for brevity) ...
        folder_path = os.path.join(base_dir, f"camels_LSTM_{loss_func}_{horizon_key}/evaluation_metrics/")
        if not os.path.exists(folder_path): continue
        csv_files = glob.glob(os.path.join(folder_path, '*.csv'))
        if not csv_files: continue
        
        station_ids = []
        nse_values = []
        
        for file_path in csv_files:
            try:
                station_id_from_filename = os.path.basename(file_path).split('.')[0]
                temp_df = pd.read_csv(file_path, index_col=0).reset_index()
                temp_df.rename(columns={'index': 'Day'}, inplace=True)
                day_specific_series = temp_df[temp_df['Day'] == fh]
                if day_specific_series.empty or 'NSE' not in day_specific_series.columns: continue
                nse_value = day_specific_series['NSE'].iloc[0]
                station_ids.append(station_id_from_filename)
                nse_values.append(nse_value)
            except Exception:
                continue

        if not station_ids: continue
        model_df = pd.DataFrame({'Station_ID': station_ids, f'NSE_{loss_func}': nse_values})
        model_df['Station_ID'] = clean_and_zfill(model_df['Station_ID'])
        model_dfs.append(model_df)

    if not model_dfs:
        log_message(f"[CRITICAL] No individual model data loaded for {fh}-day. Skipping.")
        continue
    
    log_message(f"\n--- STEP 2: Merging {len(model_dfs)} individual models for {fh}-day ---")
    merged_individual_df = reduce(lambda left, right: pd.merge(left, right, on='Station_ID', how='outer'), model_dfs)
    
    log_message(f"  Merged individual models DataFrame shape: {merged_individual_df.shape}")
    log_message(f"  Example IDs in 'Station_ID' column RIGHT AFTER MERGE:\n{merged_individual_df.head().to_string()}")

    # #######################################################################
    # # --- THE BRUTE FORCE FIX ---
    # # If the 'Station_ID' column is messy, we will rebuild it.
    # #######################################################################
    
    # Define a regex to find an 8-digit number
    id_pattern = re.compile(r'(\d{8})')

    # Function to apply the regex
    def extract_clean_id(messy_id):
        match = id_pattern.search(str(messy_id))
        return match.group(1) if match else None

    log_message("  Forcibly cleaning the 'Station_ID' column using regex...")
    # Apply the function to create a new, clean column
    merged_individual_df['Clean_Station_ID'] = merged_individual_df['Station_ID'].apply(extract_clean_id)
    
    # Drop the old, messy column and rename the new one
    merged_individual_df = merged_individual_df.drop(columns=['Station_ID'])
    merged_individual_df = merged_individual_df.rename(columns={'Clean_Station_ID': 'Station_ID'})

    # Zfill just to be safe
    merged_individual_df['Station_ID'] = clean_and_zfill(merged_individual_df['Station_ID'])

    log_message(f"  Example IDs in 'Station_ID' column AFTER BRUTE FORCE FIX:\n{merged_individual_df.head().to_string()}")

    nse_cols = [col for col in merged_individual_df.columns if 'NSE_' in col]
    merged_individual_df['NSE_Range'] = merged_individual_df[nse_cols].max(axis=1) - merged_individual_df[nse_cols].min(axis=1)
    log_message("  Calculated NSE_Range (uncertainty metric).")
    log_message("-" * 50)

    # --- (The rest of the script is largely the same, but uses log_message) ---
    log_message(f"\n--- STEP 3: Loading BMA results for {fh}-day ---")
    bma_file = os.path.join(base_dir, f'bma_results_{fh}day_with_metrics.csv')
    if not os.path.isfile(bma_file):
        log_message(f"  [WARNING] BMA results file not found: {bma_file}")
        bma_df = pd.DataFrame(columns=['Station_ID', 'NSE_BMA'])
    else:
        log_message(f"  Loading BMA results from: {bma_file}")
        bma_df = pd.read_csv(bma_file)
        if 'station_id' in bma_df.columns:
             bma_df.rename(columns={'station_id': 'Station_ID'}, inplace=True)
        bma_df['Station_ID'] = clean_and_zfill(bma_df['Station_ID'])
        bma_df = bma_df[['Station_ID', 'NSE_BMA']].drop_duplicates(subset=['Station_ID'])
        log_message(f"  Cleaned BMA DataFrame. Shape: {bma_df.shape}")
        log_message(f"  Example BMA Station_IDs:\n{bma_df.head().to_string()}")
    log_message("-" * 50)

    log_message(f"\n--- STEP 4: Performing Final Merges for {fh}-day ---")
    
    # Log the head of each dataframe to the log file before merging
    log_message("\n--- PRE-MERGE DATAFRAME HEADS ---\n")
    log_message("Individual Models DF (to be merged):")
    log_message(merged_individual_df.head().to_string())
    log_message("\nBMA DF (to be merged):")
    log_message(bma_df.head().to_string())
    log_message("\nCoords DF (to be merged):")
    log_message(coords_df.head().to_string())
    log_message("\n----------------------------------\n")

    log_message("    Merging Individual models with BMA results...")
    merged1 = pd.merge(merged_individual_df, bma_df, on='Station_ID', how='inner')
    log_message(f"    ...Shape after merging with BMA: {merged1.shape}")

    log_message("\n    Merging result with Coordinates...")
    final_df = pd.merge(merged1, coords_df, on='Station_ID', how='inner')
    log_message(f"    ...Shape after merging with Coords: {final_df.shape}")

    if final_df.empty:
        log_message("\n  [FATAL ERROR] FINAL MERGE RESULTED IN AN EMPTY DATAFRAME!")
        # ... (error diagnostics) ...
        continue

    final_df['Forecast_Horizon'] = f"{fh}-Day"
    all_horizon_data.append(final_df)
    log_message(f"  SUCCESS! Final data for {fh}-day prepared.")
    log_message("-" * 50)


# --- 5. CREATE THE COMPARISON PLOT (with improved colormaps) ---
log_message(f"\n{'='*25} Generating Final Plot {'='*25}")
if not all_horizon_data:
    log_message("[FATAL] No data was successfully processed for ANY horizon. Cannot generate plot.")
else:
    master_df = pd.concat(all_horizon_data, ignore_index=True)
    master_df.dropna(subset=['NSE_BMA', 'NSE_Range', 'gauge_lat', 'gauge_lon'], inplace=True)
    log_message(f"  Successfully created master DataFrame for plotting. Total records: {len(master_df)}")
    
    fig, axes = plt.subplots(
        len(forecast_horizons), 2,
        figsize=(10, 11),
        subplot_kw={'projection': ccrs.PlateCarree()},
        constrained_layout=True
    )
    # fig.suptitle('Spatial Analysis of BMA Performance vs. Model Uncertainty', fontsize=16, y=1.03)

    for i, fh_str in enumerate([f"{fh}-Day" for fh in forecast_horizons]):
        fh_df = master_df[master_df['Forecast_Horizon'] == fh_str]
        if fh_df.empty:
            axes[i, 0].set_visible(False); axes[i, 1].set_visible(False)
            continue
        
        # --- Left Plot: BMA NSE Performance (using 'viridis') ---
        ax1 = axes[i, 0]
        ax1.set_extent([-125, -66.5, 24, 49.5], crs=ccrs.PlateCarree())
        ax1.add_feature(cfeature.COASTLINE); ax1.add_feature(cfeature.STATES, linestyle=':', linewidth=0.5)
        scatter1 = ax1.scatter(
            fh_df['gauge_lon'], fh_df['gauge_lat'], c=fh_df['NSE_BMA'],
            # ######################
            # ### CHANGE IS HERE ###
            # ######################
            cmap='viridis', 
            vmin=-0.5, vmax=1, s=20, edgecolor='k', alpha=0.9
        )
        cbar1 = fig.colorbar(scatter1, ax=ax1, orientation='vertical', pad=0.02, shrink=0.6)
        cbar1.set_label('NSE (BMA)'); ax1.set_title(f'{fh_str} NSE Forecast')

        # --- Right Plot: Uncertainty (NSE Range) (using 'plasma') ---
        ax2 = axes[i, 1]
        ax2.set_extent([-125, -66.5, 24, 49.5], crs=ccrs.PlateCarree())
        ax2.add_feature(cfeature.COASTLINE); ax2.add_feature(cfeature.STATES, linestyle=':', linewidth=0.5)
        
        vmax_val = fh_df['NSE_Range'].quantile(0.95) 
        scatter2 = ax2.scatter(
            fh_df['gauge_lon'], fh_df['gauge_lat'], c=fh_df['NSE_Range'],
            # ######################
            # ### CHANGE IS HERE ###
            # ######################
            cmap='plasma', 
            vmin=0, vmax=vmax_val, s=20, edgecolor='k', alpha=0.9
        )
        cbar2 = fig.colorbar(scatter2, ax=ax2, orientation='vertical', pad=0.02, shrink=0.6)
        cbar2.set_label('Uncertainty (NSE Range)'); ax2.set_title(f'{fh_str} Forecast Uncertainty')

    # Add gridlines (unchanged)
    for i in range(len(forecast_horizons)):
        for j in range(2):
            if i >= axes.shape[0] or not axes[i,j].get_visible(): continue
            ax = axes[i, j]
            gl = ax.gridlines(draw_labels=True, linestyle='--', alpha=0.5)
            gl.top_labels = False; gl.right_labels = False
            gl.left_labels = (j == 0); gl.bottom_labels = (i == len(forecast_horizons) - 1)
    
    # Save with a new name to avoid overwriting
    output_plot_path = os.path.join(base_dir, 'spatial_performance_vs_uncertainty_viridis.png')
    plt.savefig(output_plot_path, dpi=300, bbox_inches='tight')
    log_message(f"\nSUCCESS! Comparison plot with improved colormaps saved to {output_plot_path}")
    
    plt.show()


================================================
File: streamflow.def
================================================
Bootstrap: docker
From: pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime

%help
    This is a minimal container with the Python environment for the TADA project.
    It includes PyTorch, CUDA, TensorFlow, and other required packages.
    
    The project code itself is NOT included in this image. You must mount it
    at runtime using the '--bind' flag.

    Example:
    apptainer exec --nv --bind /path/to/your/code:/work <sif_file>.sif python /work/main.py

%post
    # Update package lists
    apt-get update

    # Install core Python dependencies
    pip install --no-cache-dir \
        numpy>=1.21.0 \
        pandas>=1.3.0 \
        scipy>=1.7.0 \
        matplotlib>=3.4.0 \
        pyyaml>=5.4.0 \
        xarray>=0.19.0 \
        scikit-learn>=1.0.0
        

    # Install deep learning frameworks
    pip install --no-cache-dir \
        'tensorflow[and-cuda]' \
        keras>=2.8.0

    # Additional utilities
    pip install --no-cache-dir \
        tqdm \
        thop \
        ipython \
        importlib_metadata \
        dask

    # Clean up apt cache to reduce image size
    apt-get clean
    rm -rf /var/lib/apt/lists/*

%runscript
    echo "This is a minimal container with TADA dependencies."
    echo "Bind your project directory and use 'apptainer exec' to run your code."




================================================
File: submit_jobs.sh
================================================
#!/bin/bash
#SBATCH -p htc
#SBATCH -N 1            # number of nodes
#SBATCH -c 16           # number of cores 
#SBATCH --gres=gpu:a100:1    # request 1 GPU
#SBATCH -t 0-04:00:00   # time in d-hh:mm:ss
#SBATCH --mem=128G      # memory for all cores in GB
#SBATCH -q public       # QOS
#SBATCH -o slurm.%j.out # file to save job's STDOUT (%j = JobId)
#SBATCH -e slurm.%j.err # file to save job's STDERR (%j = JobId)

# Load required modules for job's environment
source activate tf-gpu

# Run the Python script with the specific config file passed as an argument
python /scratch/kdahal3/camels_losses/MAIN2.py


================================================
File: test_sig.py
================================================
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from scipy.stats import wilcoxon
from statsmodels.stats.multitest import multipletests

# --- Config ---
base_dir = "/scratch/kdahal3/camels_losses"
forecast_horizons = [1, 7, 30]
metric = "NSE"
alpha = 0.05

# --- Create folder for Q-Q plots ---
plot_dir = os.path.join(base_dir, "qqplots")
os.makedirs(plot_dir, exist_ok=True)

all_results = []

for fh in forecast_horizons:
    file_path = os.path.join(base_dir, f"bma_results_{fh}day_with_metrics.csv")
    print(f"\n=== Checking for file: {file_path}")

    if not os.path.exists(file_path):
        print(f"File not found. Skipping...")
        continue

    df = pd.read_csv(file_path)
    print(f"Loaded data for {fh}-Day | Stations: {len(df)}")

    metric_cols = [col for col in df.columns if col.startswith(f"{metric}_")]
    bma_col = f"{metric}_BMA"

    if bma_col not in metric_cols:
        print("BMA column not found, skipping this horizon.")
        continue

    for col in metric_cols:
        if col == bma_col or "Mean_Forecast" in col:
            continue  # Skip BMA and Mean Forecast

        model_name_raw = col.replace(f"{metric}_", "")
        pretty_model_name = model_name_raw.replace("camels_LSTM_", "").replace("_loss", " Loss").replace("_", " ").title()

        print(f"\n--- Comparing Model {pretty_model_name} with BMA ({fh}-Day) ---")

        bma_scores = df[bma_col]
        model_scores = df[col]

        valid = ~(bma_scores.isna() | model_scores.isna())
        bma_scores = bma_scores[valid]
        model_scores = model_scores[valid]

        if len(bma_scores) < 5:
            print("Not enough valid samples, skipping.")
            continue

        # Calculate differences
        diff = model_scores - bma_scores

        # --- Q-Q plot ---
        plt.figure(figsize=(7, 7))
        stats.probplot(diff, dist="norm", plot=plt)
        plt.title(f"Q-Q Plot of NSE Difference: {pretty_model_name} vs BMA ({fh}-Day Forecast)", fontsize=14)
        plt.xlabel("Theoretical Quantiles")
        plt.ylabel("Observed Differences")
        plt.grid(True)

        horizon_plot_dir = os.path.join(plot_dir, f"{fh}Day")
        os.makedirs(horizon_plot_dir, exist_ok=True)

        plot_file = os.path.join(horizon_plot_dir, f"qqplot_{pretty_model_name.replace(' ', '_')}.png")
        plt.savefig(plot_file, dpi=300)
        plt.close()
        print(f"Saved Q-Q plot to {plot_file}")

        # Shapiro-Wilk test
        stat, p_shapiro = stats.shapiro(diff)
        normal_flag = p_shapiro > 0.05
        print(f"Shapiro-Wilk: stat={stat:.3f}, p={p_shapiro:.3f} -> {'Normal' if normal_flag else 'Non-normal'}")

        # Wilcoxon test
        try:
            w_stat, p_w = wilcoxon(model_scores, bma_scores, zero_method='wilcox')
        except ValueError:
            w_stat, p_w = np.nan, np.nan

        mean_diff = diff.mean()

        all_results.append({
            "Forecast Horizon": f"{fh}-Day",
            "Model": pretty_model_name,
            "Number of Stations": len(bma_scores),
            "Mean Difference (Model - BMA)": mean_diff,
            "Shapiro-Wilk p-value": p_shapiro,
            "Normality": "Yes" if normal_flag else "No",
            "Wilcoxon Statistic": w_stat,
            "Wilcoxon p-value": p_w
        })

# ---- Create DataFrame ----
results_df = pd.DataFrame(all_results)

# Holm correction for Wilcoxon
results_df["Adjusted p-value"] = multipletests(results_df["Wilcoxon p-value"], method="holm")[1]
results_df["Significant"] = results_df["Adjusted p-value"] < alpha

# --- Save final clean csv ---
output_csv = os.path.join(base_dir, "wilcoxon_summary_for_paper.csv")
results_df.to_csv(output_csv, index=False)

print(f"\n=== Exported clean summary to {output_csv} ===")
print(results_df)

# --- Explain why Wilcoxon is safer ---
print("\n=== Why Wilcoxon test was safer choice ===")
print("""
Normality tests (Shapiro-Wilk) and Q-Q plots consistently indicated non-normality in the differences (Model - BMA),
which violates paired t-test assumptions. 

The Wilcoxon Signed-Rank test does not assume normality and only requires symmetric distributions.
Thus, it was selected as a more robust method for significance testing in this study.
""")



================================================
File: utils.py
================================================
import functools
import pickle
import re
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Union

import numpy as np
import pandas as pd
import xarray
from pandas.tseries.frequencies import to_offset
from ruamel.yaml import YAML
from xarray.core.dataarray import DataArray
from xarray.core.dataset import Dataset

# Pandas switched from "Y" to "YE" and similar identifiers in 2.2.0. This snippet checks which one is correct for the
# current pandas installation.
_YE_FREQ = 'YE'
_ME_FREQ = 'ME'
_QE_FREQ = 'QE'
try:
    to_offset(_YE_FREQ)
except ValueError:
    _YE_FREQ = 'Y'
    _ME_FREQ = 'M'
    _QE_FREQ = 'Q'


def load_scaler(run_dir: Path) -> Dict[str, Union[pd.Series, xarray.Dataset]]:
    """Load feature scaler from run directory.

    Checks run directory for scaler file in yaml format (new) or pickle format (old).

    Parameters
    ----------
    run_dir: Path
        Run directory. Has to contain a folder 'train_data' that contains the 'train_data_scaler' file.

    Returns
    -------
    Dictionary, containing the feature scaler for static and dynamic features.
    
    Raises
    ------
    FileNotFoundError
        If neither a 'train_data_scaler.yml' or 'train_data_scaler.p' file is found in the 'train_data' folder of the 
        run directory.
    """
    scaler_file = run_dir / "train_data" / "train_data_scaler.yml"

    if scaler_file.is_file():
        # read scaler from disk
        with scaler_file.open("r") as fp:
            yaml = YAML(typ="safe")
            scaler_dump = yaml.load(fp)

        # transform scaler into the format expected by NeuralHydrology
        scaler = {}
        for key, value in scaler_dump.items():
            if key in ["attribute_means", "attribute_stds", "camels_attr_means", "camels_attr_stds"]:
                scaler[key] = pd.Series(value)
            elif key in ["xarray_feature_scale", "xarray_feature_center"]:
                scaler[key] = xarray.Dataset.from_dict(value).astype(np.float32)

        return scaler

    else:
        scaler_file = run_dir / "train_data" / "train_data_scaler.p"

        if scaler_file.is_file():
            with scaler_file.open('rb') as fp:
                scaler = pickle.load(fp)
            return scaler
        else:
            raise FileNotFoundError(f"No scaler file found in {scaler_file.parent}. "
                                    "Looked for (new) yaml file or (old) pickle file")


def load_hydroatlas_attributes(data_dir: Path, basins: List[str] = []) -> pd.DataFrame:
    """Load HydroATLAS attributes into a pandas DataFrame

    Parameters
    ----------
    data_dir : Path
        Path to the root directory of the dataset. Must contain a folder called 'hydroatlas_attributes' with a file
        called `attributes.csv`. The attributes file is expected to have one column called `basin_id`.
    basins : List[str], optional
        If passed, return only attributes for the basins specified in this list. Otherwise, the attributes of all basins
        are returned.

    Returns
    -------
    pd.DataFrame
        Basin-indexed DataFrame containing the HydroATLAS attributes.
    """
    attribute_file = data_dir / "hydroatlas_attributes" / "attributes.csv"
    if not attribute_file.is_file():
        raise FileNotFoundError(attribute_file)

    df = pd.read_csv(attribute_file, dtype={'basin_id': str})
    df = df.set_index('basin_id')

    if basins:
        drop_basins = [b for b in df.index if b not in basins]
        df = df.drop(drop_basins, axis=0)

    return df


def load_basin_file(basin_file: Path) -> List[str]:
    """Load list of basins from text file.
    
    Note: Basins names are not allowed to end with '_period*'
    
    Parameters
    ----------
    basin_file : Path
        Path to a basin txt file. File has to contain one basin id per row, while empty rows are ignored.

    Returns
    -------
    List[str]
        List of basin ids as strings.
        
    Raises
    ------
    ValueError
        In case of invalid basin names that would cause problems internally.
    """
    with basin_file.open('r') as fp:
        basins = sorted(basin.strip() for basin in fp if basin.strip())

    # sanity check basin names
    problematic_basins = [basin for basin in basins if basin.split('_')[-1].startswith('period')]
    if problematic_basins:
        msg = [
            f"The following basin names are invalid {problematic_basins}. Check documentation of the ",
            "'load_basin_file()' functions for details."
        ]
        raise ValueError(" ".join(msg))

    return basins


def attributes_sanity_check(df: pd.DataFrame):
    """Utility function to check the suitability of the attributes for model training.
    
    This utility function can be used to check if any attribute has a standard deviation of zero. This would lead to 
    NaN's when normalizing the features and thus would lead to NaN's when training the model. It also checks if any
    attribute for any basin contains a NaN, which would also cause NaNs during model training.
    
    Parameters
    ----------
    df : pd.DataFrame
        DataFrame of catchment attributes as columns.

    Raises
    ------
    RuntimeError
        If one or more attributes have a standard deviation of zero or any attribute for any basin is NaN.
    """
    # Check for NaNs in standard deviation of attributes.
    attributes = []
    if any(df.std() == 0.0) or any(df.std().isnull()):
        for k, v in df.std().items():
            if (v == 0) or (np.isnan(v)):
                attributes.append(k)
    if attributes:
        msg = [
            "The following attributes have a std of zero or NaN, which results in NaN's ",
            "when normalizing the features. Remove the attributes from the attribute feature list ",
            "and restart the run. \n", f"Attributes: {attributes}"
        ]
        raise RuntimeError("".join(msg))

    # Check for NaNs in any attribute of any basin
    nan_df = df[df.isnull().any(axis=1)]
    if len(nan_df) > 0:
        failure_cases = defaultdict(list)
        for basin, row in nan_df.iterrows():
            for feature, value in row.items():
                if np.isnan(value):
                    failure_cases[basin].append(feature)
        # create verbose error message
        msg = ["The following basins/attributes are NaN, which can't be used as input:"]
        for basin, features in failure_cases.items():
            msg.append(f"{basin}: {features}")
        raise RuntimeError("\n".join(msg))


def sort_frequencies(frequencies: List[str]) -> List[str]:
    """Sort the passed frequencies from low to high frequencies.

    Use `pandas frequency strings
    <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timeseries-offset-aliases>`_
    to define frequencies. Note: The strings need to include values, e.g., '1D' instead of 'D'.

    Parameters
    ----------
    frequencies : List[str]
        List of pandas frequency identifiers to be sorted.

    Returns
    -------
    List[str]
        Sorted list of pandas frequency identifiers.

    Raises
    ------
    ValueError
        If a pair of frequencies in `frequencies` is not comparable via `compare_frequencies`.
    """
    return sorted(frequencies, key=functools.cmp_to_key(compare_frequencies))


def infer_frequency(index: Union[pd.DatetimeIndex, np.ndarray]) -> str:
    """Infer the frequency of an index of a pandas DataFrame/Series or xarray DataArray.

    Parameters
    ----------
    index : Union[pd.DatetimeIndex, np.ndarray]
        DatetimeIndex of a DataFrame/Series or array of datetime values.

    Returns
    -------
    str
        Frequency of the index as a `pandas frequency string
        <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timeseries-offset-aliases>`_

    Raises
    ------
    ValueError
        If the frequency cannot be inferred from the index or is zero.
    """
    native_frequency = pd.infer_freq(index)
    if native_frequency is None:
        raise ValueError(f'Cannot infer a legal frequency from dataset: {native_frequency}.')
    if native_frequency[0] not in '0123456789':  # add a value to the unit so to_timedelta works
        native_frequency = f'1{native_frequency}'

    # pd.Timedelta doesn't understand weekly (W) frequencies, so we convert them to the equivalent multiple of 7D.
    weekly_freq = re.match('(\d+)W(-(MON|TUE|WED|THU|FRI|SAT|SUN))?$', native_frequency)
    if weekly_freq is not None:
        n = int(weekly_freq[1]) * 7
        native_frequency = f'{n}D'

    # Assert that the frequency corresponds to a positive time delta. We first add one offset to the base datetime
    # to make sure it's aligned with the frequency. Otherwise, adding an offset of e.g. 0Y would round up to the
    # nearest year-end, so we'd incorrectly miss a frequency of zero.
    base_datetime = pd.to_datetime('2001-01-01 00:00:00') + to_offset(native_frequency)
    if base_datetime >= base_datetime + to_offset(native_frequency):
        raise ValueError('Inferred dataset frequency is zero or negative.')
    return native_frequency


def infer_datetime_coord(xr: Union[DataArray, Dataset]) -> str:
    """Checks for coordinate with 'date' in its name and returns the name.
    
    Parameters
    ----------
    xr : Union[DataArray, Dataset]
        Array to infer coordinate name of.
        
    Returns
    -------
    str
        Name of datetime coordinate name.
        
    Raises
    ------
    RuntimeError
        If none or multiple coordinates with 'date' in its name are found.
    """
    candidates = [c for c in list(xr.coords) if "date" in c]
    if len(candidates) > 1:
        raise RuntimeError("Found multiple coordinates with 'date' in its name.")
    if not candidates:
        raise RuntimeError("Did not find any coordinate with 'date' in its name")

    return candidates[0]


def compare_frequencies(freq_one: str, freq_two: str) -> int:
    """Compare two frequencies.

    Note that only frequencies that work with `get_frequency_factor` can be compared.

    Parameters
    ----------
    freq_one : str
        First frequency.
    freq_two : str
        Second frequency.

    Returns
    -------
    int
        -1 if `freq_one` is lower than `freq_two`, +1 if it is larger, 0 if they are equal.

    Raises
    ------
    ValueError
        If the two frequencies are not comparable via `get_frequency_factor`.
    """
    freq_factor = get_frequency_factor(freq_one, freq_two)
    if freq_factor < 1:
        return 1
    if freq_factor > 1:
        return -1
    return 0


def get_frequency_factor(freq_one: str, freq_two: str) -> float:
    """Get relative factor between the two frequencies.

    Parameters
    ----------
    freq_one : str
        String representation of the first frequency.
    freq_two : str
        String representation of the second frequency.

    Returns
    -------
    float
        Ratio of `freq_one` to `freq_two`.

    Raises
    ------
    ValueError
        If the frequency factor cannot be determined. This can be the case if the frequencies do not represent a fixed
        time delta and are not directly comparable (e.g., because they have the same unit)
        E.g., a month does not represent a fixed time delta. Thus, 1D and 1M are not comparable. However, 1M and 2M are
        comparable since they have the same unit.
    """
    if freq_one == freq_two:
        return 1

    offset_one = to_offset(freq_one)
    offset_two = to_offset(freq_two)
    if offset_one.n < 0 or offset_two.n < 0:
        # Would be possible to implement, but we should never need negative frequencies, so it seems reasonable to
        # fail gracefully rather than to open ourselves to potential unexpected corner cases.
        raise NotImplementedError('Cannot compare negative frequencies.')
    # avoid division by zero errors
    if offset_one.n == offset_two.n == 0:
        return 1
    if offset_two.n == 0:
        return np.inf
    if offset_one.name == offset_two.name:
        return offset_one.n / offset_two.n

    # some simple hard-coded cases
    factor = None
    regex_month_or_day = '-(JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC|MON|TUE|WED|THU|FRI|SAT|SUN)$'
    for i, (one, two) in enumerate([(offset_one, offset_two), (offset_two, offset_one)]):
        # the offset anchor is irrelevant for the ratio between the frequencies, so we remove it from the string
        name_one = re.sub(regex_month_or_day, '', one.name)
        name_two = re.sub(regex_month_or_day, '', two.name)
        if (name_one in ['A', _YE_FREQ] and name_two == _ME_FREQ) or (name_one in ['AS', 'YS'] and name_two == 'MS'):
            factor = 12 * one.n / two.n
        if (name_one in ['A', _YE_FREQ] and name_two == _QE_FREQ) or (name_one in ['AS', 'YS'] and name_two == 'QS'):
            factor = 4 * one.n / two.n
        if (name_one == _QE_FREQ and name_two == _ME_FREQ) or (name_one == 'QS' and name_two == 'MS'):
            factor = 3 * one.n / two.n
        if name_one == 'W' and name_two == 'D':
            factor = 7 * one.n / two.n

        if factor is not None:
            if i == 1:
                return 1 / factor  # `one` was `offset_two`, `two` was `offset_one`
            return factor

    # If all other checks didn't match, we try to convert the frequencies to timedeltas. However, we first need to avoid
    # two cases: (1) pd.to_timedelta currently interprets 'M' as minutes, while it means months in to_offset.
    # (2) Using 'M', 'Y', and 'y' in pd.to_timedelta is deprecated and won't work in the future, so we don't allow it.
    if any(
            re.sub(regex_month_or_day, '', offset.name) in ['M', 'Y', 'A', 'y', 'ME', 'YE']
            for offset in [offset_one, offset_two]):
        raise ValueError(f'Frequencies {freq_one} and/or {freq_two} are not comparable.')
    try:
        factor = pd.to_timedelta(freq_one) / pd.to_timedelta(freq_two)
    except ValueError as err:
        raise ValueError(f'Frequencies {freq_one} and/or {freq_two} are not comparable.') from err
    return factor




================================================
File: .ipynb_checkpoints/BMA-checkpoint.py
================================================
import os
import glob
import pandas as pd
import numpy as np
import xarray as xr
from metrics import calculate_metrics, get_available_metrics  # Import metrics functions

# Base directory containing the model output directories
base_dir = '/scratch/kdahal3/camels_losses'

# Define the forecast horizons you want to process
forecast_horizons = [1, 7, 30]

# Define the metrics list, excluding Peak-Timing and Missed-Peaks
metrics_list = get_available_metrics()
metrics_list = [m for m in metrics_list if m not in ["Peak-Timing", "Missed-Peaks"]]

# Function to process data for a given forecast horizon
def process_forecast_horizon(forecast_horizon):
    print(f"\nProcessing Forecast Horizon: {forecast_horizon} day(s)")

    # Find all model directories ending with f'_{forecast_horizon}day'
    model_dirs = glob.glob(os.path.join(base_dir, f'*_{forecast_horizon}day'))

    if not model_dirs:
        print(f"No model directories found for forecast horizon {forecast_horizon} day(s).")
        return

    # Get a list of all station IDs from the CSV filenames in the first model directory
    first_model_dir = model_dirs[0]
    csv_pattern = 'camels_*_combined_forecast_vs_observed.csv'
    csv_files = glob.glob(os.path.join(first_model_dir, csv_pattern))

    if not csv_files:
        print(f"No CSV files found in the first model directory: {first_model_dir}")
        return

    station_ids = [os.path.basename(f).split('_')[1] for f in csv_files]

    # Prepare a DataFrame to store results
    bma_results = []

    # Keep track of all possible model names for consistent columns
    all_model_names = set()

    # Loop over each station
    for station_id in station_ids:
        print(f"\nProcessing Station ID: {station_id}")
        station_data = {}
        station_data['Station_ID'] = station_id
        model_forecasts = {}
        observed_data = None
        models_with_data = []

        # Loop over each model directory to collect forecasts for the station
        for model_dir in model_dirs:
            model_name = os.path.basename(model_dir)
            csv_filename = f'camels_{station_id}_combined_forecast_vs_observed.csv'
            csv_file_path = os.path.join(model_dir, csv_filename)

            if not os.path.isfile(csv_file_path):
                # Skip if the station data is not available in this model
                print(f"Model {model_name}: No data for station {station_id}")
                continue

            print(f"Reading data for model {model_name}, station {station_id}")
            # Read the CSV file
            df = pd.read_csv(csv_file_path)

            # For forecast horizons >1, select only the desired Forecast_Horizon
            if forecast_horizon > 1:
                df = df[df['Forecast_Horizon'] == forecast_horizon]
                if df.empty:
                    print(f"Warning: No data for Forecast_Horizon={forecast_horizon} in station {station_id}, model {model_name}")
                    continue

            # Convert 'Date' column to datetime
            df['Date'] = pd.to_datetime(df['Date'])
            df = df.sort_values('Date').reset_index(drop=True)

            # Store the observed data (assuming it's the same across models)
            if observed_data is None:
                observed_data = df[['Date', 'Observed']].copy()
            else:
                # Check if observed data matches
                if not observed_data['Observed'].equals(df['Observed']):
                    print(f"Warning: Observed data mismatch for station {station_id} in model {model_name}")

            # Store the model's forecast
            model_forecasts[model_name] = df['Forecasted'].values
            models_with_data.append(model_name)
            all_model_names.add(model_name)  # Add to the set of all model names

        if not models_with_data:
            # No models have data for this station
            print(f"No models have data for station {station_id} at Forecast_Horizon={forecast_horizon}")
            continue

        # Create a DataFrame with observed and forecasts
        combined_df = observed_data.copy()
        for model_name in models_with_data:
            combined_df[model_name] = model_forecasts[model_name]

        # Remove any rows with NaN values (if any models are missing forecasts for certain dates)
        combined_df = combined_df.dropna()

        # Extract observed and forecasts
        observed = combined_df['Observed'].values
        forecasts = combined_df[models_with_data].values  # Shape: (n_samples, n_models)

        print(f"Number of samples: {len(observed)}")
        print(f"Models with data: {models_with_data}")

        n_models = forecasts.shape[1]
        n_samples = forecasts.shape[0]

        # Initialize a list to store NSE values for weighting
        nse_values = []
        model_metrics = {}

        # Calculate metrics for individual models
        for idx, model_name in enumerate(models_with_data):
            model_forecast = forecasts[:, idx]

            # Convert to xarray DataArrays for metric calculations
            obs_da = xr.DataArray(observed)
            sim_da = xr.DataArray(model_forecast)

            # Calculate metrics
            print(f"Calculating metrics for model {model_name}")
            metrics = calculate_metrics(obs_da, sim_da, metrics=metrics_list)

            # Store metrics
            for metric_name, metric_value in metrics.items():
                station_data[f'{metric_name}_{model_name}'] = metric_value

            # Store NSE value for weighting
            nse_model = metrics['NSE']
            nse_values.append(nse_model)
            model_metrics[model_name] = metrics

        # Convert NSE values to a numpy array for calculations
        nse_values = np.array(nse_values)

        # Implement proper Bayesian Model Averaging

        # Calculate prior probabilities (uniform priors)
        prior_probabilities = np.full(n_models, 1 / n_models)

        # Compute log-likelihoods for each model
        log_likelihoods = []
        for idx in range(n_models):
            residuals = observed - forecasts[:, idx]
            sigma_k_squared = np.var(residuals, ddof=1)
            n = len(observed)
            # Handle case where sigma_k_squared is zero
            if sigma_k_squared == 0:
                # Assign a very small variance
                sigma_k_squared = 1e-6
            log_likelihood = -0.5 * n * np.log(2 * np.pi * sigma_k_squared) - (np.sum(residuals ** 2) / (2 * sigma_k_squared))
            log_likelihoods.append(log_likelihood)

        log_likelihoods = np.array(log_likelihoods)

        # Compute log posterior probabilities (since priors are equal, they cancel out in log space)
        log_posteriors = log_likelihoods

        # To prevent numerical underflow, subtract the maximum log posterior
        max_log_posterior = np.max(log_posteriors)
        log_posteriors -= max_log_posterior

        # Convert to posterior probabilities
        posteriors = np.exp(log_posteriors)
        posterior_probabilities = posteriors / np.sum(posteriors)

        print(f"Posterior probabilities for models: {dict(zip(models_with_data, posterior_probabilities))}")

        # Compute the BMA forecast
        bma_forecast = np.dot(forecasts, posterior_probabilities)

        # Convert to xarray DataArrays for metric calculations
        bma_forecast_da = xr.DataArray(bma_forecast)
        obs_da = xr.DataArray(observed)

        # Calculate metrics for BMA forecast
        print("Calculating metrics for BMA forecast")
        metrics_bma = calculate_metrics(obs_da, bma_forecast_da, metrics=metrics_list)
        for metric_name, metric_value in metrics_bma.items():
            station_data[f'{metric_name}_BMA'] = metric_value

        # Store the posterior probabilities as weights
        for idx, model_name in enumerate(models_with_data):
            station_data[f'Weight_{model_name}'] = posterior_probabilities[idx]

        # Ensure that weights for all possible models are included, even if zero
        for model_name in all_model_names:
            if f'Weight_{model_name}' not in station_data:
                station_data[f'Weight_{model_name}'] = 0.0

        # Calculate Mean Forecast
        mean_forecast = forecasts.mean(axis=1)

        # Convert to xarray DataArrays for metric calculations
        mean_forecast_da = xr.DataArray(mean_forecast)
        obs_da = xr.DataArray(observed)

        # Calculate metrics for Mean Forecast
        print("Calculating metrics for Mean Forecast")
        metrics_mean = calculate_metrics(obs_da, mean_forecast_da, metrics=metrics_list)
        for metric_name, metric_value in metrics_mean.items():
            station_data[f'{metric_name}_Mean_Forecast'] = metric_value

        # Calculate actual coverage based on observed data and forecast range (min-max bounds from all models)
        lower_bound_bma = forecasts.min(axis=1)
        upper_bound_bma = forecasts.max(axis=1)
        actual_coverage = np.sum((observed >= lower_bound_bma) & (observed <= upper_bound_bma)) / n_samples
        station_data['Actual_Coverage_BMA'] = actual_coverage

        # Append the results
        bma_results.append(station_data)

    if not bma_results:
        print(f"No results to display for Forecast_Horizon={forecast_horizon} day(s).")
        return

    # Create a DataFrame from the results, ensuring all columns are included
    bma_results_df = pd.DataFrame(bma_results)

    # Display the results
    print(bma_results_df)

    # Optionally, save the results to a CSV file
    output_filename = f'bma_results_{forecast_horizon}day_with_metrics.csv'
    bma_results_df.to_csv(output_filename, index=False)
    print(f"Results saved to {output_filename}")

# Process each forecast horizon
for fh in forecast_horizons:
    process_forecast_horizon(fh)



================================================
File: .ipynb_checkpoints/BMA_val-checkpoint.py
================================================



================================================
File: .ipynb_checkpoints/MAIN2-checkpoint.py
================================================
#MAIN2.py

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress INFO and WARNING logs from TensorFlow

import warnings
warnings.filterwarnings('ignore')  # Suppress all Python warnings

import logging
logging.getLogger('tensorflow').setLevel(logging.ERROR)  # Suppress INFO and WARNING logs from TensorFlow

import yaml
import argparse
import glob
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from keras.callbacks import EarlyStopping, TensorBoard, LearningRateScheduler
import datetime
import gc
from keras import backend as K
from keras import mixed_precision
import xarray as xr

import losses
import models
from metrics import calculate_metrics, get_available_metrics

# Enable mixed precision for memory efficiency
mixed_precision.set_global_policy('mixed_float16')

# Enable GPU memory growth to avoid pre-allocating all GPU memory at once
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# Learning rate schedule
def lr_schedule(epoch, lr):
    if epoch < 20:
        return 1e-3
    elif 20 <= epoch < 25:
        return 5e-4
    else:
        return 1e-4

# Create sequences from the data
def create_sequences(data, n_steps_in, n_steps_out):
    X, y = [], []
    for i in range(len(data)):
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out
        if out_end_ix > len(data):
            break
        seq_x, seq_y = data[i:end_ix], data[end_ix:out_end_ix, -1]  # Assuming the last column is the target
        X.append(seq_x)
        y.append(seq_y)
    return np.array(X), np.array(y)

def create_time_sequences(data, n_steps_in, n_steps_out):
    X = []
    for i in range(len(data)):
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out
        if out_end_ix > len(data):
            break
        seq_x = data[i:end_ix]
        X.append(seq_x)
    return np.array(X)

# Parse command-line arguments
parser = argparse.ArgumentParser(description='Run forecast with a specific config file.')
parser.add_argument('--config', type=str, default='config.yml', help='Path to the config file.')
args = parser.parse_args()

# Load the specified configuration file
with open(args.config, 'r') as f:
    config = yaml.safe_load(f)

# Create output directory based on the output_prefix
output_dir = config['output_prefix']
os.makedirs(output_dir, exist_ok=True)

# Prepare directory for saving evaluation metrics
evaluation_folder = os.path.join(output_dir, config.get('evaluation_folder', 'evaluation_metrics'))
os.makedirs(evaluation_folder, exist_ok=True)

# Helper function to load and process datasets
def load_and_process_data(file_paths, for_transfer_learning=False):
    data_list = []
    for path in file_paths:
        if os.path.isdir(path):
            csv_files = glob.glob(os.path.join(path, "*.csv"))
        else:
            csv_files = [path]

        for csv_file in csv_files:
            dataset = pd.read_csv(csv_file, header=0, index_col=0)
            dataset.index = pd.to_datetime(dataset.index)
            dataset.replace('Min', np.nan, inplace=True)
            dataset = dataset.asfreq('D')
            dataset = dataset.apply(pd.to_numeric, errors='coerce')
            dataset = dataset.loc[dataset['streamflow'].first_valid_index():]
            dataset = dataset.loc[:dataset['streamflow'].last_valid_index()]
            dataset = dataset.interpolate(method='linear').ffill().bfill()
            if for_transfer_learning:
                data_list.append(dataset)
            else:
                data_list.append((csv_file, dataset))
    return data_list

# Load main datasets
print("Loading main datasets...")
main_datasets = load_and_process_data(config['data_paths'])

# Load transfer learning datasets (if applicable)
if 'transfer_learning_paths' in config:
    print("Loading transfer learning datasets...")
    transfer_datasets = load_and_process_data(config['transfer_learning_paths'], for_transfer_learning=True)
else:
    transfer_datasets = []

# Identify all feature names (excluding 'streamflow') across all main datasets
print("Identifying all feature names...")
all_features = set()
for _, dataset in main_datasets:
    all_features.update([col for col in dataset.columns if col != 'streamflow'])
all_features = sorted(all_features)  # Ensure consistent ordering
number_of_features = len(all_features) + 1  # +1 for 'streamflow' column

# Compute global min and max for each column across all main and transfer learning datasets
print("Computing global min and max for each column...")
min_max_dict = {}
# Include both main and transfer datasets for scaling
for dataset in main_datasets:
    _, ds = dataset
    for col in ds.columns:
        col_min = ds[col].min()
        col_max = ds[col].max()
        if col not in min_max_dict:
            min_max_dict[col] = {'min': col_min, 'max': col_max}
        else:
            if col_min < min_max_dict[col]['min']:
                min_max_dict[col]['min'] = col_min
            if col_max > min_max_dict[col]['max']:
                min_max_dict[col]['max'] = col_max

for ds in transfer_datasets:
    for col in ds.columns:
        col_min = ds[col].min()
        col_max = ds[col].max()
        if col not in min_max_dict:
            min_max_dict[col] = {'min': col_min, 'max': col_max}
        else:
            if col_min < min_max_dict[col]['min']:
                min_max_dict[col]['min'] = col_min
            if col_max > min_max_dict[col]['max']:
                min_max_dict[col]['max'] = col_max

# Convert min_max_dict to a DataFrame and save as CSV
min_max_df = pd.DataFrame(min_max_dict).T  # Transpose for easier CSV format
min_max_csv_path = os.path.join(output_dir, 'min_max.csv')
min_max_df.to_csv(min_max_csv_path)
print(f"Global min and max saved to {min_max_csv_path}")

# Function to scale data using global min and max
def scale_data(df, min_max_dict):
    scaled_df = pd.DataFrame(index=df.index)
    for col in df.columns:
        min_val = min_max_dict[col]['min']
        max_val = min_max_dict[col]['max']
        if max_val > min_val:
            scaled_df[col] = (df[col] - min_val) / (max_val - min_val)
        else:
            scaled_df[col] = 0.0  # Handle case where min == max to avoid division by zero
    return scaled_df

# Data generator for tf.data.Dataset
def data_generator(datasets, n_steps_in, n_steps_out, min_max_dict, features, split='train'):
    for dataset_info in datasets:
        csv_file, dataset = dataset_info
        station_name = os.path.splitext(os.path.basename(csv_file))[0] if csv_file else "transfer_learning"
        print(f"Processing station: {station_name} for split: {split}")

        # Ensure the dataset has all required features
        missing_features = set(features) - set(dataset.columns)
        if missing_features:
            print(f"Warning: Missing features {missing_features} in station {station_name}. Filling with NaN.")
            for mf in missing_features:
                dataset[mf] = np.nan

        # Reorder the dataset columns to match all_features + 'streamflow'
        dataset = dataset[features + ['streamflow']]

        # Extract time features
        dataset['year'] = dataset.index.year
        dataset['month'] = dataset.index.month
        dataset['day'] = dataset.index.day
        dataset['day_of_week'] = dataset.index.dayofweek + 1  # Monday=1, Sunday=7
        dataset['day_of_year'] = dataset.index.dayofyear

        time_features = ['year', 'month', 'day', 'day_of_week', 'day_of_year']

        # Scale all features using global min and max
        scaled_features = scale_data(dataset[features], min_max_dict)
        # Scale the target variable (streamflow) using global min and max
        scaled_target = scale_data(dataset[['streamflow']], min_max_dict)

        # Combine scaled features and target
        scaled = np.hstack((scaled_features.values, scaled_target.values))  # 'streamflow' is the last column

        # Prepare time features (without scaling, as temporal encoding will handle it)
        time_data = dataset[time_features].values  # Shape: (num_samples, num_time_features)

        # Create sequences
        X, y = create_sequences(scaled, n_steps_in, n_steps_out)
        time_sequences = create_time_sequences(time_data, n_steps_in, n_steps_out)  # Updated call

        # Determine split indices
        total_sequences = len(X)
        train_end = int(total_sequences * 0.7)
        val_end = int(total_sequences * 0.8)

        if split == 'train':
            sequences_X = X[:train_end]
            sequences_y = y[:train_end]
            sequences_time = time_sequences[:train_end]
        elif split == 'val':
            sequences_X = X[train_end:val_end]
            sequences_y = y[train_end:val_end]
            sequences_time = time_sequences[train_end:val_end]
        else:
            continue  # Skip processing for 'test' split

        # Yield data for the specified split
        for i in range(len(sequences_X)):
            yield (
                (sequences_X[i].astype(np.float32), sequences_time[i].astype(np.float32)),
                sequences_y[i].astype(np.float32)
            )  # Ensure float32 dtype for TensorFlow

# Create separate generators for training and validation
train_generator = lambda: data_generator(
    [(None, ds) for ds in transfer_datasets] + main_datasets,  # Include transfer datasets in train split
    config['n_steps_in'],
    config['n_steps_out'],
    min_max_dict,
    all_features,
    split='train'
)

val_generator = lambda: data_generator(
    [(None, ds) for ds in transfer_datasets] + main_datasets,  # Include transfer datasets in val split
    config['n_steps_in'],
    config['n_steps_out'],
    min_max_dict,
    all_features,
    split='val'
)

# Create dataset from generator for training
train_dataset = tf.data.Dataset.from_generator(
    train_generator,
    output_signature=(
        (
            tf.TensorSpec(shape=(config['n_steps_in'], number_of_features), dtype=tf.float32),  # Input features
            tf.TensorSpec(shape=(config['n_steps_in'], 5), dtype=tf.float32)  # Time features
        ),
        tf.TensorSpec(shape=(config['n_steps_out'],), dtype=tf.float32)  # Target
    )
)

# Create dataset from generator for validation
val_dataset = tf.data.Dataset.from_generator(
    val_generator,
    output_signature=(
        (
            tf.TensorSpec(shape=(config['n_steps_in'], number_of_features), dtype=tf.float32),  # Input features
            tf.TensorSpec(shape=(config['n_steps_in'], 5), dtype=tf.float32)  # Time features
        ),
        tf.TensorSpec(shape=(config['n_steps_out'],), dtype=tf.float32)  # Target
    )
)

# Shuffle, batch, and prefetch the training dataset
train_dataset = train_dataset.shuffle(buffer_size=10000) \
                             .batch(config['batch_size']) \
                             .prefetch(tf.data.AUTOTUNE)

# Batch and prefetch the validation dataset
val_dataset = val_dataset.batch(config['batch_size']) \
                         .prefetch(tf.data.AUTOTUNE)

# Build the model using models.py
output_prefix = config['output_prefix']  # e.g., "run_1"
log_dir = os.path.join(
    "logs",
    "fit",
    f"{output_prefix}_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}"
)

# Callbacks
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)
early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
lr_scheduler = LearningRateScheduler(lr_schedule)  # Uncomment if using learning rate scheduler

# Build the model using models.py
model = models.get_model(config, number_of_features)

# Get loss function and metrics
loss_function = losses.get_loss_function(
    config['loss_function'], config.get('loss_params', {})
)
metrics = [
    losses.get_metric_function(metric_name)
    for metric_name in config.get('metrics', [])
]

# Compile the model
learning_rate = config.get('learning_rate', 0.001)
optimizer_name = config.get('optimizer', 'Adam').lower()
clipvalue = config.get('clipvalue', 1.0)

if optimizer_name == 'adam':
    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipvalue=clipvalue)
elif optimizer_name == 'sgd':
    opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)
else:
    raise ValueError(f"Unsupported optimizer: {optimizer_name}")

model.compile(optimizer=opt, loss=loss_function, metrics=metrics)

model.summary()

# Train the model using the separate training and validation datasets
history = model.fit(
    train_dataset,
    epochs=config['epochs'],
    validation_data=val_dataset,
    verbose=1,
    callbacks=[early_stopping, tensorboard_callback]#, lr_scheduler]  # Add lr_scheduler if needed
)

# Save the model if required
if config.get('export_model', False):
    model_path = os.path.join(output_dir, "combined_model.keras")
    model.save(model_path)
    print(f"Model saved at {model_path}")

# Clear GPU memory
gc.collect()
K.clear_session()

# ========================= EVALUATION ================================
# Load test data
test_data_folder = config.get('test_data_folder', 'test_data')
test_csv_files = glob.glob(os.path.join(test_data_folder, "*_test_data.csv"))

for test_csv_file in test_csv_files:
    station_name = os.path.splitext(os.path.basename(test_csv_file))[0].replace('_test_data', '')
    print(f"Testing on station: {station_name}")

    test_dataset = pd.read_csv(test_csv_file, header=0, index_col=0)
    test_dataset.index = pd.to_datetime(test_dataset.index)

    # Ensure the dataset has all required features
    missing_features = set(all_features + ['streamflow']) - set(test_dataset.columns)
    if missing_features:
        print(f"Warning: Missing features {missing_features} in station {station_name}. Filling with NaN.")
        for mf in missing_features:
            test_dataset[mf] = np.nan

    # Reorder the dataset columns to match all_features + 'streamflow'
    test_dataset = test_dataset[all_features + ['streamflow']]

    # Extract time features
    test_dataset['year'] = test_dataset.index.year
    test_dataset['month'] = test_dataset.index.month
    test_dataset['day'] = test_dataset.index.day
    test_dataset['day_of_week'] = test_dataset.index.dayofweek + 1
    test_dataset['day_of_year'] = test_dataset.index.dayofyear
    time_features = ['year', 'month', 'day', 'day_of_week', 'day_of_year']
    time_data = test_dataset[time_features].values

    # Scale all features using global min and max
    scaled_features = scale_data(test_dataset[all_features], min_max_dict)
    # Scale the target variable (streamflow) using global min and max
    scaled_target = scale_data(test_dataset[['streamflow']], min_max_dict)

    # Combine scaled features and target
    scaled = np.hstack((scaled_features.values, scaled_target.values))  # 'streamflow' is the last column

    # Create sequences for LSTM
    test_X, test_y = create_sequences(scaled, config['n_steps_in'], config['n_steps_out'])
    test_time_sequences = create_time_sequences(time_data, config['n_steps_in'], config['n_steps_out'])

    # Create a tf.data.Dataset for testing
    test_dataset_tf = tf.data.Dataset.from_tensor_slices(
        ((test_X.astype(np.float32), test_time_sequences.astype(np.float32)), test_y.astype(np.float32))
    )
    test_dataset_tf = test_dataset_tf.batch(config['batch_size']).prefetch(tf.data.AUTOTUNE)

    # Predict on the test dataset
    y_pred = model.predict(test_dataset_tf)

    # Inverse transform the predictions and actual values using global min and max
    # Assuming 'streamflow' is the last column in min_max_dict
    streamflow_min = min_max_dict['streamflow']['min']
    streamflow_max = min_max_dict['streamflow']['max']
    y_pred_inv = y_pred * (streamflow_max - streamflow_min) + streamflow_min
    test_y_inv = test_y * (streamflow_max - streamflow_min) + streamflow_min

    # Ensure that the lengths of y_pred_inv and test_y_inv match before constructing the DataFrame
    min_length = min(len(y_pred_inv), len(test_y_inv))
    y_pred_inv = y_pred_inv[:min_length]  # Truncate to the minimum length
    test_y_inv = test_y_inv[:min_length]  # Truncate to the minimum length

    # Create the date range for the test data
    test_start_date = test_dataset.index[config['n_steps_in'] + config['n_steps_out'] - 1]
    test_dates = pd.date_range(start=test_start_date, periods=min_length)

    # Repeat each date for each forecast horizon and shift by the horizon
    forecast_horizons = np.arange(1, config['n_steps_out'] + 1)  # e.g., [1, 2, 3] for n_steps_out=3
    repeated_dates = np.repeat(test_dates, config['n_steps_out'])
    shifted_dates = repeated_dates + pd.to_timedelta(np.tile(forecast_horizons, min_length), unit='D')

    # Flatten the observed and forecasted values
    forecasted_values = y_pred_inv.flatten()
    observed_values = test_y_inv.flatten()

    # Construct DataFrame with correctly aligned dates
    df_result = pd.DataFrame({
        'Date': shifted_dates,
        'Forecast_Horizon': np.tile(forecast_horizons, min_length),
        'Observed': observed_values,
        'Forecasted': forecasted_values
    })

    # Save the forecast vs observed results to a CSV file
    result_csv_path = os.path.join(output_dir, f"{station_name}_forecast_vs_observed.csv")
    df_result.to_csv(result_csv_path, index=False)
    print(f"Results saved for {station_name} at {result_csv_path}")

    # ======================= METRICS CALCULATION ===========================
    print(f"Calculating metrics for station: {station_name}")

    # Convert to xarray DataArrays for metric calculations
    y_pred_da = xr.DataArray(y_pred_inv)
    test_y_da = xr.DataArray(test_y_inv)

    # Define the metrics list, excluding Peak-Timing and Missed-Peaks
    metrics_list = get_available_metrics()
    metrics_list = [m for m in metrics_list if m not in ["Peak-Timing", "Missed-Peaks"]]

    # Dictionary to store metrics per forecast day
    metrics_per_day = {}

    # Calculate metrics for each forecast day
    for day in range(config['n_steps_out']):
        # Extract the observations and simulations for the current forecast horizon
        obs = test_y_da[:, day] if test_y_da.ndim > 1 else test_y_da
        sim = y_pred_da[:, day] if y_pred_da.ndim > 1 else y_pred_da

        # Calculate metrics
        day_metrics = calculate_metrics(obs, sim, metrics=metrics_list)
        metrics_per_day[day + 1] = day_metrics  # Store metrics for each forecast day

    # Convert metrics_per_day into a DataFrame and save as CSV
    metrics_df = pd.DataFrame(metrics_per_day).T  # Transpose for better readability
    metrics_csv_path = os.path.join(evaluation_folder, f"{station_name}_daywise_metrics.csv")
    metrics_df.to_csv(metrics_csv_path, index=True)
    print(f"Metrics saved for {station_name} at {metrics_csv_path}")



================================================
File: .ipynb_checkpoints/config-checkpoint.yml
================================================
# config.yml

data_paths:
  - "/scratch/kdahal3/Caravan-stat-dyna/camels"

output_prefix: "test-run"

n_steps_in: 365  # Number of input steps
n_steps_out: 1   # Number of output steps

epochs: 150        # Number of training epochs
batch_size: 256   # Batch size for training
learning_rate: 0.0001  # Initial learning rate

# # Uncomment and add paths as needed
# transfer_learning_paths:
#   - "/scratch/kdahal3/Caravan-stat-dyna/camels"

export_sequences: false
export_model: true
save_plots: true
evaluation_folder: "evaluation_metrics"  # Folder to store evaluation metrics for each CSV

optimizer: 'Adam'

model_type: "LSTM"  # Options: "GRU", "LSTM", "TRANSFORMER", "CNN_RNN", "SEQ2SEQ" etc.

loss_function: "nse_loss"  # Options: "nse_loss", "kge_loss", "weighted_mse_loss", "huber_loss", "quantile_loss", "expectile_loss", "mae_loss"

# # Parameters for loss functions
# loss_params:
#   expectile: 0.15  # Used for expectile_loss

metrics:
  - 'kge_metric'
  - 'nse_metric'

test_data_folder: "test_data"
export_test_data: True  # Set to False if you don't want to export test data

model_params:
  num_time_features: 5 #useful for transformers only
  num_heads: 4
  head_size: 32
  ff_dim: 128
  num_transformer_blocks: 4
  dropout_rate: 0.4
  mlp_units: [128]
  mlp_dropout_rate: 0.4
  hidden_dim: 128
  time_feature_sizes: [2020, 12, 31, 7, 366]  # Adjust these values according to your data



================================================
File: .ipynb_checkpoints/evaluate_validation-checkpoint.py
================================================
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import warnings
warnings.filterwarnings('ignore')

import logging
logging.getLogger('tensorflow').setLevel(logging.ERROR)

import yaml
import argparse
import glob
import numpy as np
import pandas as pd
import tensorflow as tf
from keras import backend as K
from keras import mixed_precision
import xarray as xr

# Import all custom objects the models were trained with.
from losses import (QuantileLoss, KgeLoss, NseLoss, ExpectileLoss, 
                    HuberLoss, WeightedMSELoss, MaeLoss, LogNseLoss,
                    kge_metric, nse_metric)

from metrics import calculate_metrics, get_available_metrics

mixed_precision.set_global_policy('mixed_float16')

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# --- Helper Functions (No changes) ---
def create_sequences(data, n_steps_in, n_steps_out):
    X, y = [], []
    for i in range(len(data)):
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out
        if out_end_ix > len(data):
            break
        seq_x, seq_y = data[i:end_ix], data[end_ix:out_end_ix, -1]
        X.append(seq_x)
        y.append(seq_y)
    return np.array(X), np.array(y)

def create_time_sequences(data, n_steps_in, n_steps_out):
    X = []
    for i in range(len(data)):
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out
        if out_end_ix > len(data):
            break
        seq_x = data[i:end_ix]
        X.append(seq_x)
    return np.array(X)

def scale_data(df, min_max_dict):
    scaled_df = pd.DataFrame(index=df.index)
    for col in df.columns:
        if col in min_max_dict:
            min_val = min_max_dict[col]['min']
            max_val = min_max_dict[col]['max']
            if max_val > min_val:
                scaled_df[col] = (df[col] - min_val) / (max_val - min_val)
            else:
                scaled_df[col] = 0.0
        else:
            print(f"Warning: Column '{col}' not found in min_max dictionary. Skipping scaling.")
            scaled_df[col] = df[col]
    return scaled_df

# --- Main Evaluation Script ---
def main():
    parser = argparse.ArgumentParser(description='Evaluate a pre-trained model on validation data.')
    parser.add_argument('--config', type=str, required=True, help='Path to the config file used for training.')
    parser.add_argument('--model-path', type=str, required=True, help='Path to the saved .keras model file.')
    parser.add_argument('--min-max-path', type=str, required=True, help='Path to the min_max.csv file from training.')
    parser.add_argument('--val-data-folder', type=str, required=True, help='Path to the folder containing validation CSV files.')
    parser.add_argument('--output-dir', type=str, required=True, help='Directory to save evaluation results and metrics.')
    args = parser.parse_args()

    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)

    os.makedirs(args.output_dir, exist_ok=True)
    evaluation_folder = os.path.join(args.output_dir, 'evaluation_metrics')
    os.makedirs(evaluation_folder, exist_ok=True)

    print(f"Loading min/max scaling values from {args.min_max_path}...")
    min_max_df = pd.read_csv(args.min_max_path, index_col=0)
    min_max_dict = min_max_df.to_dict('index')

    all_features = [col for col in min_max_df.index if col != 'streamflow']
    print(f"Using {len(all_features)} features for evaluation.")

    print(f"Loading model from {args.model_path}...")
    
    # ==================================================================
    # === FIX STARTS HERE: Corrected Monkey Patching Logic           ===
    # ==================================================================
    custom_objects = {
        "QuantileLoss": QuantileLoss, "ExpectileLoss": ExpectileLoss,
        "HuberLoss": HuberLoss, "NseLoss": NseLoss, "KgeLoss": KgeLoss,
        "WeightedMSELoss": WeightedMSELoss, "MaeLoss": MaeLoss, "LogNseLoss": LogNseLoss,
        "kge_metric": kge_metric, "nse_metric": nse_metric
    }

    # --- Monkey Patching Section ---
    original_from_configs = {}
    classes_to_patch_from_config = [QuantileLoss, ExpectileLoss, HuberLoss]
    
    for loss_class in classes_to_patch_from_config:
        if hasattr(loss_class, 'from_config'):
            original_from_configs[loss_class.__name__] = loss_class.from_config
            
            @classmethod
            def patched_from_config(cls, config, original_method=original_from_configs[loss_class.__name__]):
                config.pop('reduction', None)
                return original_method(config)
            
            loss_class.from_config = patched_from_config

    # Corrected patching for classes without from_config (like NseLoss, KgeLoss)
    original_inits = {}
    classes_to_patch_init = [NseLoss, KgeLoss] 
    
    def create_patched_init(original_init):
        def patched_init(self, *args, **kwargs):
            kwargs.pop('reduction', None)
            original_init(self, *args, **kwargs)
        return patched_init

    for loss_class in classes_to_patch_init:
        original_inits[loss_class.__name__] = loss_class.__init__
        loss_class.__init__ = create_patched_init(original_inits[loss_class.__name__])

    # Load the model with the patched classes
    model = tf.keras.models.load_model(args.model_path, custom_objects=custom_objects)
    
    # Restore original methods
    for loss_class in classes_to_patch_from_config:
        if loss_class.__name__ in original_from_configs:
            loss_class.from_config = original_from_configs[loss_class.__name__]
    for loss_class in classes_to_patch_init:
        if loss_class.__name__ in original_inits:
            loss_class.__init__ = original_inits[loss_class.__name__]
    # ==================================================================
    # === FIX ENDS HERE ================================================
    
    model.summary()

    val_csv_files = glob.glob(os.path.join(args.val_data_folder, "*.csv"))
    if not val_csv_files:
        print(f"Error: No CSV files found in {args.val_data_folder}")
        return

    for val_csv_file in val_csv_files:
        station_name = os.path.splitext(os.path.basename(val_csv_file))[0].replace('_val_data', '')
        print(f"\n--- Evaluating on station: {station_name} ---")

        # ... (rest of the file is unchanged) ...
        val_dataset = pd.read_csv(val_csv_file, header=0, index_col=0)
        val_dataset.index = pd.to_datetime(val_dataset.index)
        val_dataset.replace('Min', np.nan, inplace=True)
        val_dataset = val_dataset.asfreq('D')
        val_dataset = val_dataset.apply(pd.to_numeric, errors='coerce')
        val_dataset = val_dataset.interpolate(method='linear').ffill().bfill()

        required_cols = all_features + ['streamflow']
        missing_features = set(required_cols) - set(val_dataset.columns)
        if missing_features:
            print(f"Warning: Missing features {missing_features} in {station_name}. Filling with NaN and interpolating.")
            for mf in missing_features:
                val_dataset[mf] = np.nan
            val_dataset = val_dataset.interpolate(method='linear').ffill().bfill()

        val_dataset = val_dataset[required_cols]

        val_dataset['year'] = val_dataset.index.year
        val_dataset['month'] = val_dataset.index.month
        val_dataset['day'] = val_dataset.index.day
        val_dataset['day_of_week'] = val_dataset.index.dayofweek + 1
        val_dataset['day_of_year'] = val_dataset.index.dayofyear
        time_features = ['year', 'month', 'day', 'day_of_week', 'day_of_year']
        time_data = val_dataset[time_features].values

        scaled_features = scale_data(val_dataset[all_features], min_max_dict)
        scaled_target = scale_data(val_dataset[['streamflow']], min_max_dict)
        scaled = np.hstack((scaled_features.values, scaled_target.values))

        val_X, val_y = create_sequences(scaled, config['n_steps_in'], config['n_steps_out'])
        val_time_sequences = create_time_sequences(time_data, config['n_steps_in'], config['n_steps_out'])

        if len(val_X) == 0:
            print(f"Not enough data in {station_name} to create sequences. Skipping.")
            continue

        val_dataset_tf = tf.data.Dataset.from_tensor_slices(
            ((val_X.astype(np.float32), val_time_sequences.astype(np.float32)),)
        ).batch(config['batch_size']).prefetch(tf.data.AUTOTUNE)

        print("Running predictions...")
        y_pred_scaled = model.predict(val_dataset_tf)

        streamflow_min = min_max_dict['streamflow']['min']
        streamflow_max = min_max_dict['streamflow']['max']
        y_pred_inv = y_pred_scaled * (streamflow_max - streamflow_min) + streamflow_min
        val_y_inv = val_y * (streamflow_max - streamflow_min) + streamflow_min

        min_length = min(len(y_pred_inv), len(val_y_inv))
        y_pred_inv = y_pred_inv[:min_length]
        val_y_inv = val_y_inv[:min_length]

        val_start_date = val_dataset.index[config['n_steps_in']]
        val_dates = pd.date_range(start=val_start_date, periods=min_length)

        results_list = []
        for i in range(min_length):
            for j in range(config['n_steps_out']):
                forecast_day = j + 1
                date = val_dates[i] + pd.Timedelta(days=j)
                observed = val_y_inv[i, j]
                forecasted = y_pred_inv[i, j]
                results_list.append([date, forecast_day, observed, forecasted])

        df_result = pd.DataFrame(results_list, columns=['Date', 'Forecast_Horizon', 'Observed', 'Forecasted'])
        result_csv_path = os.path.join(args.output_dir, f"{station_name}_validation_forecast_vs_observed.csv")
        df_result.to_csv(result_csv_path, index=False)
        print(f"Validation results saved for {station_name} at {result_csv_path}")

        print(f"Calculating metrics for station: {station_name}")
        y_pred_da = xr.DataArray(y_pred_inv, dims=('time', 'horizon'))
        val_y_da = xr.DataArray(val_y_inv, dims=('time', 'horizon'))

        metrics_list = get_available_metrics()
        metrics_list = [m for m in metrics_list if m not in ["Peak-Timing", "Missed-Peaks"]]
        metrics_per_day = {}

        for day in range(config['n_steps_out']):
            obs = val_y_da.isel(horizon=day)
            sim = y_pred_da.isel(horizon=day)
            day_metrics = calculate_metrics(obs, sim, metrics=metrics_list)
            metrics_per_day[day + 1] = day_metrics

        metrics_df = pd.DataFrame(metrics_per_day).T
        metrics_df.index.name = 'Forecast_Day'
        metrics_csv_path = os.path.join(evaluation_folder, f"{station_name}_validation_daywise_metrics.csv")
        metrics_df.to_csv(metrics_csv_path)
        print(f"Validation metrics saved for {station_name} at {metrics_csv_path}")

    K.clear_session()
    print("\nEvaluation complete for this model run.")

if __name__ == '__main__':
    main()


================================================
File: .ipynb_checkpoints/export_test_data-checkpoint.py
================================================
import os
import yaml
import argparse
import numpy as np
import pandas as pd
import glob

# Parse command-line arguments
parser = argparse.ArgumentParser(description='Export test data for evaluation.')
parser.add_argument('--config', type=str, default='config.yml', help='Path to the config file.')
args = parser.parse_args()

# Load the specified configuration file
with open(args.config, 'r') as f:
    config = yaml.safe_load(f)

# Create output directory for test data
test_data_folder = config.get('test_data_folder', 'test_data')
os.makedirs(test_data_folder, exist_ok=True)

# Helper function to load and process datasets
def load_and_process_data(file_paths):
    data_list = []
    for path in file_paths:
        if os.path.isdir(path):
            csv_files = glob.glob(os.path.join(path, "*.csv"))
        else:
            csv_files = [path]

        for csv_file in csv_files:
            dataset = pd.read_csv(csv_file, header=0, index_col=0)
            dataset.index = pd.to_datetime(dataset.index)
            dataset.replace('Min', np.nan, inplace=True)
            dataset = dataset.asfreq('D')
            dataset = dataset.apply(pd.to_numeric, errors='coerce')
            dataset = dataset.loc[dataset['streamflow'].first_valid_index():]
            dataset = dataset.loc[:dataset['streamflow'].last_valid_index()]
            dataset = dataset.interpolate(method='linear').ffill().bfill()
            data_list.append((csv_file, dataset))
    return data_list

# Load main datasets
print("Loading main datasets...")
main_datasets = load_and_process_data(config['data_paths'])

# Export test data
for csv_file, dataset in main_datasets:
    station_name = os.path.splitext(os.path.basename(csv_file))[0]
    # Split the dataset into train, validation, and test
    total_records = len(dataset)
    train_end = int(total_records * 0.7)
    val_end = int(total_records * 0.8)
    test_data = dataset.iloc[val_end:]
    # Save the test data to CSV
    test_data_path = os.path.join(test_data_folder, f"{station_name}_test_data.csv")
    test_data.to_csv(test_data_path)
    print(f"Test data saved for {station_name} at {test_data_path}")



================================================
File: .ipynb_checkpoints/export_val_data-checkpoint.py
================================================
import os
import yaml
import argparse
import numpy as np
import pandas as pd
import glob

# Parse command-line arguments
parser = argparse.ArgumentParser(description='Export validation data for evaluation.')
parser.add_argument('--config', type=str, default='config.yml', help='Path to the config file.')
args = parser.parse_args()

# Load the specified configuration file
with open(args.config, 'r') as f:
    config = yaml.safe_load(f)

# Create output directory for validation data
val_data_folder = config.get('val_data_folder', 'val_data')
os.makedirs(val_data_folder, exist_ok=True)

# Helper function to load and process datasets
def load_and_process_data(file_paths):
    data_list = []
    for path in file_paths:
        if os.path.isdir(path):
            csv_files = glob.glob(os.path.join(path, "*.csv"))
        else:
            csv_files = [path]

        for csv_file in csv_files:
            dataset = pd.read_csv(csv_file, header=0, index_col=0)
            dataset.index = pd.to_datetime(dataset.index)
            dataset.replace('Min', np.nan, inplace=True)
            dataset = dataset.asfreq('D')
            dataset = dataset.apply(pd.to_numeric, errors='coerce')
            dataset = dataset.loc[dataset['streamflow'].first_valid_index():]
            dataset = dataset.loc[:dataset['streamflow'].last_valid_index()]
            dataset = dataset.interpolate(method='linear').ffill().bfill()
            data_list.append((csv_file, dataset))
    return data_list

# Load main datasets
print("Loading main datasets...")
main_datasets = load_and_process_data(config['data_paths'])

# Export validation data
for csv_file, dataset in main_datasets:
    station_name = os.path.splitext(os.path.basename(csv_file))[0]
    # Split the dataset into train, validation, and test
    total_records = len(dataset)
    train_end = int(total_records * 0.7)
    val_end = int(total_records * 0.8)
    # Select the validation data slice
    val_data = dataset.iloc[train_end:val_end]
    # Save the validation data to CSV
    val_data_path = os.path.join(val_data_folder, f"{station_name}_val_data.csv")
    val_data.to_csv(val_data_path)
    print(f"Validation data saved for {station_name} at {val_data_path}")


================================================
File: .ipynb_checkpoints/hp-checkpoint.py
================================================
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress INFO and WARNING logs from TensorFlow

import warnings
warnings.filterwarnings('ignore')  # Suppress all Python warnings

import logging
logging.getLogger('tensorflow').setLevel(logging.ERROR)  # Suppress INFO and WARNING logs from TensorFlow


import keras_tuner as kt
import tensorflow as tf
from models import get_model
import yaml
import numpy as np
import os
import argparse
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import glob

# from tensorflow.keras import mixed_precision
# mixed_precision.set_global_policy('mixed_float16')

# tf.config.optimizer.set_jit(False)  # Disables XLA compilation


# Function to load the configuration file
def load_config(config_path):
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

# Load and process data (similar to main.py)
def load_and_process_data(file_paths):
    data_list = []
    for path in file_paths:
        csv_files = [path] if os.path.isfile(path) else glob.glob(os.path.join(path, "*.csv"))
        for csv_file in csv_files:
            dataset = pd.read_csv(csv_file, header=0, index_col=0)
            dataset.index = pd.to_datetime(dataset.index)
            dataset.replace('Min', np.nan, inplace=True)
            dataset = dataset.asfreq('D')
            dataset = dataset.apply(pd.to_numeric, errors='coerce')
            dataset = dataset.interpolate(method='linear').ffill().bfill()
            data_list.append(dataset)
    return data_list

# Define hypermodel for tuning
def build_hyper_model(hp):
    model_params = {
        'num_heads': hp.Int('num_heads', min_value=2, max_value=8, step=2),
        'head_size': hp.Int('head_size', min_value=8, max_value=64, step=8),
        'ff_dim': hp.Int('ff_dim', min_value=64, max_value=512, step=64),
        'num_transformer_blocks': hp.Int('num_transformer_blocks', min_value=2, max_value=6, step=1),
        'dropout_rate': hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1),
        'mlp_units': [hp.Int('mlp_units1', min_value=64, max_value=256, step=64),
                      hp.Int('mlp_units2', min_value=64, max_value=256, step=64)],
        'mlp_dropout_rate': hp.Float('mlp_dropout_rate', min_value=0.1, max_value=0.5, step=0.1),
        'hidden_dim': hp.Int('hidden_dim', min_value=64, max_value=256, step=64),
        'num_time_features': 5,  # Ensure this is always included
        'time_feature_sizes': [2020, 12, 31, 7, 366]
    }

    # Build and compile model with dynamically selected parameters
    model = get_model({
        'model_type': 'TRANSFORMER',
        'n_steps_in': config['n_steps_in'],
        'n_steps_out': config['n_steps_out'],
        'model_params': model_params
    }, number_of_features)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=config.get('learning_rate', 0.001)),
        loss='mse',
        metrics=['mae']
    )
    return model

# Function to create sequences for main features
def create_sequences(data, n_steps_in, n_steps_out):
    X, y = [], []
    for i in range(len(data)):
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out
        if out_end_ix > len(data):
            break
        seq_x, seq_y = data[i:end_ix], data[end_ix:out_end_ix, -1]  # Assuming last column is target
        X.append(seq_x)
        y.append(seq_y)
    return np.array(X), np.array(y)

# Function to create sequences for time features
def create_time_sequences(data, n_steps_in, n_steps_out):
    X = []
    for i in range(len(data)):
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out
        if out_end_ix > len(data):
            break
        seq_x = data[i:end_ix]
        X.append(seq_x)
    return np.array(X)

def run_hyperparameter_tuning():
    # Load and process the dataset
    main_datasets = load_and_process_data(config['data_paths'])
    scaler = MinMaxScaler()

    # Prepare data for tuning
    X_train, y_train, X_time_train = [], [], []
    for dataset in main_datasets:
        # Scale and prepare the main features
        scaled_data = scaler.fit_transform(dataset)
        X, y = create_sequences(scaled_data, config['n_steps_in'], config['n_steps_out'])
        X_train.append(X)
        y_train.append(y)

        # Prepare time features from the dataset index
        index = dataset.index  # Directly use the index
        time_features = pd.DataFrame({
            'year': index.year,
            'month': index.month,
            'day': index.day,
            'day_of_week': index.dayofweek + 1,  # Monday=1, Sunday=7
            'day_of_year': index.dayofyear
        }).values

        # Create time sequences
        X_time = create_time_sequences(time_features, config['n_steps_in'], config['n_steps_out'])
        X_time_train.append(X_time)

    # Concatenate all training data
    X_train = np.concatenate(X_train)
    y_train = np.concatenate(y_train)
    X_time_train = np.concatenate(X_time_train)

    # Verify shapes before calling tuner.search
    print(f"Final X_train shape (main features): {X_train.shape}")       # Expecting (56968, 365, 49)
    print(f"Final X_time_train shape (time features): {X_time_train.shape}") # Expecting (56968, 365, 5)
    print(f"Final y_train shape (target): {y_train.shape}")             # Expecting (56968, 3)

    # Define tuner
    tuner = kt.Hyperband(
        build_hyper_model,
        objective='val_mae',
        max_epochs=10,
        factor=3,
        directory=config['output_prefix'],
        project_name='hyperparameter_tuning'
    )

    # Run search with both main and time inputs
    tuner.search((X_train, X_time_train), y_train, validation_split=0.2, epochs=5, batch_size=config['batch_size'])

    # Get best hyperparameters and save them
    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
    print("Best hyperparameters found:")
    for param in best_hps.values:
        print(f"{param}: {best_hps.get(param)}")

    # Save best hyperparameters to file
    best_params = {param: best_hps.get(param) for param in best_hps.values}
    with open(os.path.join(config['output_prefix'], 'best_params.yml'), 'w') as file:
        yaml.dump(best_params, file)

# Load config and run tuning
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Run hyperparameter tuning with a specific config file.')
    parser.add_argument('--config', type=str, default='config.yml', help='Path to the config file.')
    args = parser.parse_args()
    
    config = load_config(args.config)
    number_of_features = 49
    run_hyperparameter_tuning()



================================================
File: .ipynb_checkpoints/huc2_spatial_nse_uncertainty-checkpoint.py
================================================
import os
import pandas as pd
import glob
import numpy as np
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cfeature
from functools import reduce
import re # Import the regular expression module
import geopandas as gpd # Import geopandas

# --- CONFIGURATION ---
base_dir = '/scratch/kdahal3/camels_losses'
log_file_path = os.path.join(base_dir, 'merge_log.txt')
forecast_horizons = [1, 7, 30]
loss_functions = [
    "nse_loss", "kge_loss", "huber_loss", "quantile_low",
    "quantile_high", "expectile_low", "expectile_high"
]
coords_file = '/scratch/kdahal3/Caravan/attributes/camels/attributes_other_camels.csv'
shapefile_path = '/scratch/kdahal3/camels_losses/huc2.shp'

def clean_and_zfill(station_id_series):
    return station_id_series.astype(str).str.strip().str.zfill(8)

# Clear old log file
if os.path.exists(log_file_path):
    os.remove(log_file_path)

def log_message(message):
    print(message)
    with open(log_file_path, 'a') as f:
        f.write(message + '\n')

log_message("="*60)
log_message("Starting spatial uncertainty analysis...")
log_message("="*60)

# --- 1. LOAD AND PREPARE GEOSPATIAL DATA ---
log_message("\n--- STEP 1: Loading and Cleaning Geospatial Data ---")
try:
    coords_df = pd.read_csv(coords_file)
    coords_df['Station_ID'] = coords_df['gauge_id'].apply(lambda x: str(x).split('_')[1])
    coords_df['Station_ID'] = clean_and_zfill(coords_df['Station_ID'])
    coords_df = coords_df[['Station_ID', 'gauge_lat', 'gauge_lon']].drop_duplicates(subset=['Station_ID'])
    log_message(f"  Cleaned and prepared coordinates for {len(coords_df)} unique stations.")
    
    shape_gdf = gpd.read_file(shapefile_path)
    log_message(f"  Successfully loaded HUC2 shapefile with {len(shape_gdf)} basins.")
    log_message("-" * 50)
except Exception as e:
    log_message(f"[FATAL ERROR] Could not load coordinate or shapefile: {e}")
    exit()

# --- 2. PROCESS DATA FOR EACH HORIZON (Condensed for brevity) ---
all_horizon_data = []
for fh in forecast_horizons:
    log_message(f"\n{'='*25} Processing {fh}-Day Forecast {'='*25}")
    horizon_key = f'{fh}day'
    model_dfs = []
    for loss_func in loss_functions:
        folder_path = os.path.join(base_dir, f"camels_LSTM_{loss_func}_{horizon_key}/evaluation_metrics/")
        if not os.path.exists(folder_path): continue
        csv_files = glob.glob(os.path.join(folder_path, '*.csv'))
        if not csv_files: continue
        station_ids, nse_values = [], []
        for file_path in csv_files:
            try:
                station_id_from_filename = os.path.basename(file_path).split('.')[0]
                temp_df = pd.read_csv(file_path, index_col=0).reset_index()
                temp_df.rename(columns={'index': 'Day'}, inplace=True)
                day_specific_series = temp_df[temp_df['Day'] == fh]
                if day_specific_series.empty or 'NSE' not in day_specific_series.columns: continue
                nse_value = day_specific_series['NSE'].iloc[0]
                station_ids.append(station_id_from_filename)
                nse_values.append(nse_value)
            except Exception: continue
        if not station_ids: continue
        model_df = pd.DataFrame({'Station_ID': station_ids, f'NSE_{loss_func}': nse_values})
        model_df['Station_ID'] = clean_and_zfill(model_df['Station_ID'])
        model_dfs.append(model_df)
    if not model_dfs: continue
    merged_individual_df = reduce(lambda left, right: pd.merge(left, right, on='Station_ID', how='outer'), model_dfs)
    id_pattern = re.compile(r'(\d{8})')
    def extract_clean_id(messy_id):
        match = id_pattern.search(str(messy_id)); return match.group(1) if match else None
    merged_individual_df['Clean_Station_ID'] = merged_individual_df['Station_ID'].apply(extract_clean_id)
    merged_individual_df = merged_individual_df.drop(columns=['Station_ID']).rename(columns={'Clean_Station_ID': 'Station_ID'})
    merged_individual_df['Station_ID'] = clean_and_zfill(merged_individual_df['Station_ID'])
    nse_cols = [col for col in merged_individual_df.columns if 'NSE_' in col]
    merged_individual_df['NSE_Range'] = merged_individual_df[nse_cols].max(axis=1) - merged_individual_df[nse_cols].min(axis=1)
    bma_file = os.path.join(base_dir, f'bma_results_{fh}day_with_metrics.csv')
    if not os.path.isfile(bma_file): bma_df = pd.DataFrame(columns=['Station_ID', 'NSE_BMA'])
    else:
        bma_df = pd.read_csv(bma_file)
        if 'station_id' in bma_df.columns: bma_df.rename(columns={'station_id': 'Station_ID'}, inplace=True)
        bma_df['Station_ID'] = clean_and_zfill(bma_df['Station_ID'])
        bma_df = bma_df[['Station_ID', 'NSE_BMA']].drop_duplicates(subset=['Station_ID'])
    merged1 = pd.merge(merged_individual_df, bma_df, on='Station_ID', how='inner')
    final_df = pd.merge(merged1, coords_df, on='Station_ID', how='inner')
    if final_df.empty: continue
    final_df['Forecast_Horizon'] = f"{fh}-Day"
    all_horizon_data.append(final_df)
    log_message(f"  SUCCESS! Final data for {fh}-day prepared. Shape: {final_df.shape}")

# --- 5. CREATE THE FINAL PLOT ---
log_message(f"\n{'='*25} Generating Final Plot {'='*25}")
if not all_horizon_data:
    log_message("[FATAL] No data processed. Cannot generate plot.")
else:
    master_df = pd.concat(all_horizon_data, ignore_index=True)
    master_df.dropna(subset=['NSE_BMA', 'NSE_Range', 'gauge_lat', 'gauge_lon'], inplace=True)
    log_message(f"  Master DataFrame created for plotting. Total records: {len(master_df)}")
    
    fig, axes = plt.subplots(
        len(forecast_horizons), 2,
        figsize=(12, 10), # Increased figure size slightly for better label spacing
        subplot_kw={'projection': ccrs.PlateCarree()},
        constrained_layout=True
    )
    
    for i, fh_str in enumerate([f"{fh}-Day" for fh in forecast_horizons]):
        fh_df = master_df[master_df['Forecast_Horizon'] == fh_str]
        if fh_df.empty:
            axes[i, 0].set_visible(False); axes[i, 1].set_visible(False)
            continue
        
        # Plot on both axes for this horizon
        for ax in [axes[i, 0], axes[i, 1]]:
            ax.set_extent([-125, -66.5, 24, 49.5], crs=ccrs.PlateCarree())
            ax.add_feature(cfeature.COASTLINE, zorder=2)
            ax.add_feature(cfeature.STATES, linestyle=':', linewidth=0.5, zorder=2)
            shape_gdf.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=0.8, zorder=3)
            
            # ######################################
            # ### ANNOTATE HUC2 BASIN NUMBERS  ###
            # ######################################
            for idx, row in shape_gdf.iterrows():
                # Use a representative point to place the label
                centroid = row.geometry.centroid
                ax.text(centroid.x, centroid.y, row['huc2'],
                        fontsize=8, ha='center', va='center',
                        color='dimgray', weight='bold', zorder=5)

        # --- Left Plot: BMA NSE Performance ---
        ax1 = axes[i, 0]
        scatter1 = ax1.scatter(
            fh_df['gauge_lon'], fh_df['gauge_lat'], c=fh_df['NSE_BMA'],
            cmap='viridis', vmin=-0.5, vmax=1, s=25, 
            # ########################
            # ### REMOVE BORDERS ###
            # ########################
            edgecolor='none',
            alpha=0.8, zorder=4
        )
        cbar1 = fig.colorbar(scatter1, ax=ax1, orientation='vertical', pad=0.02, shrink=0.6)
        cbar1.set_label('NSE (BMA)'); ax1.set_title(f'{fh_str} NSE Forecast')

        # --- Right Plot: Uncertainty (NSE Range) ---
        ax2 = axes[i, 1]
        vmax_val = fh_df['NSE_Range'].quantile(0.95) 
        scatter2 = ax2.scatter(
            fh_df['gauge_lon'], fh_df['gauge_lat'], c=fh_df['NSE_Range'],
            cmap='plasma', vmin=0, vmax=vmax_val, s=25,
            # ########################
            # ### REMOVE BORDERS ###
            # ########################
            edgecolor='none',
            alpha=0.8, zorder=4
        )
        cbar2 = fig.colorbar(scatter2, ax=ax2, orientation='vertical', pad=0.02, shrink=0.6)
        cbar2.set_label('Uncertainty (NSE Range)'); ax2.set_title(f'{fh_str} Forecast Uncertainty')

    # Add gridlines
    for i in range(len(forecast_horizons)):
        for j in range(2):
            if i >= axes.shape[0] or not axes[i,j].get_visible(): continue
            ax = axes[i, j]
            gl = ax.gridlines(draw_labels=True, linestyle='--', alpha=0.5, zorder=1)
            gl.top_labels = False; gl.right_labels = False
            gl.left_labels = (j == 0); gl.bottom_labels = (i == len(forecast_horizons) - 1)
    
    output_plot_path = os.path.join(base_dir, 'spatial_performance_vs_uncertainty_final.png')
    plt.savefig(output_plot_path, dpi=300, bbox_inches='tight')
    log_message(f"\nSUCCESS! Final plot saved to {output_plot_path}")
    
    plt.show()


================================================
File: .ipynb_checkpoints/losses-checkpoint.py
================================================
# losses.py

import tensorflow as tf
from keras import backend as K
from keras.losses import Loss

class KgeLoss(Loss):
    def __init__(self, name='kge_loss'):
        super().__init__(name=name)

    def call(self, y_true, y_pred):
        mean_true = tf.reduce_mean(y_true)
        mean_pred = tf.reduce_mean(y_pred)
        std_true = tf.math.reduce_std(y_true)
        std_pred = tf.math.reduce_std(y_pred)
        covariance = tf.reduce_mean((y_true - mean_true) * (y_pred - mean_pred))
        correlation = covariance / (std_true * std_pred + K.epsilon())
        std_ratio = std_pred / (std_true + K.epsilon())
        bias_ratio = mean_pred / (mean_true + K.epsilon())
        kge = 1 - tf.sqrt(
            tf.square(correlation - 1) + tf.square(std_ratio - 1) + tf.square(bias_ratio - 1)
        )
        return 1 - kge

    def get_config(self):
        base_config = super().get_config()
        return {**base_config}

class NseLoss(Loss):
    def __init__(self, name='nse_loss'):
        super().__init__(name=name)

    def call(self, y_true, y_pred):
        mean_true = tf.reduce_mean(y_true)
        numerator = tf.reduce_sum(tf.square(y_true - y_pred))
        denominator = tf.reduce_sum(tf.square(y_true - mean_true))
        nse = 1 - numerator / (denominator + K.epsilon())
        return 1 - nse

    def get_config(self):
        base_config = super().get_config()
        return {**base_config}

class ExpectileLoss(Loss):
    def __init__(self, expectile=0.5, name='expectile_loss'):
        super().__init__(name=name)
        self.expectile = expectile

    def call(self, y_true, y_pred):
        e = y_true - y_pred
        loss = tf.reduce_mean(
            tf.where(
                e >= 0,
                self.expectile * tf.square(e),
                (1 - self.expectile) * tf.square(e)
            )
        )
        return loss

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "expectile": self.expectile}

    @classmethod
    def from_config(cls, config):
        return cls(**config)

class QuantileLoss(Loss):
    def __init__(self, quantile=0.5, name='quantile_loss'):
        super().__init__(name=name)
        self.quantile = quantile

    def call(self, y_true, y_pred):
        e = y_true - y_pred
        loss = tf.reduce_mean(
            tf.maximum(self.quantile * e, (self.quantile - 1) * e)
        )
        return loss

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "quantile": self.quantile}

    @classmethod
    def from_config(cls, config):
        return cls(**config)

class HuberLoss(Loss):
    def __init__(self, delta=1.0, name='huber_loss'):
        super().__init__(name=name)
        self.delta = delta

    def call(self, y_true, y_pred):
        return tf.reduce_mean(tf.keras.losses.huber(y_true, y_pred, delta=self.delta))

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "delta": self.delta}

    @classmethod
    def from_config(cls, config):
        return cls(**config)

class WeightedMSELoss(Loss):
    def __init__(self, weights=None, name='weighted_mse_loss'):
        super().__init__(name=name)
        self.weights = weights

    def call(self, y_true, y_pred):
        if self.weights is None:
            weights = tf.ones_like(y_true)
        else:
            weights = tf.cast(self.weights, dtype=tf.float32)
        return tf.reduce_mean(weights * tf.square(y_true - y_pred))

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "weights": self.weights}

    @classmethod
    def from_config(cls, config):
        return cls(**config)

class MaeLoss(Loss):
    def __init__(self, name='mae_loss'):
        super().__init__(name=name)

    def call(self, y_true, y_pred):
        return tf.reduce_mean(tf.abs(y_true - y_pred))

    def get_config(self):
        base_config = super().get_config()
        return {**base_config}

class LogNseLoss(Loss):
    def __init__(self, epsilon=1e-6, name='lognse_loss'):
        super().__init__(name=name)
        self.epsilon = epsilon  # Small constant to avoid log(0)

    def call(self, y_true, y_pred):
        # Ensure y_true and y_pred are non-negative
        y_true = tf.maximum(y_true, 0.0)
        y_pred = tf.maximum(y_pred, 0.0)

        # Add epsilon to avoid log(0)
        y_true_log = tf.math.log(y_true + self.epsilon)
        y_pred_log = tf.math.log(y_pred + self.epsilon)

        # Handle cases where y_true_log has zero variance
        mean_true_log = tf.reduce_mean(y_true_log)
        denominator = tf.reduce_sum(tf.square(y_true_log - mean_true_log))
        # Replace zero denominators with a small constant
        denominator = tf.where(tf.equal(denominator, 0.0), K.epsilon(), denominator)

        numerator = tf.reduce_sum(tf.square(y_true_log - y_pred_log))

        nse = 1 - numerator / denominator
        # Ensure nse is within a valid range [-1, 1]
        nse = tf.clip_by_value(nse, -1.0, 1.0)

        # Return the loss
        loss = 1 - nse  # Subtract from 1 to convert to a loss
        return loss

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "epsilon": self.epsilon}


def kge_metric(y_true, y_pred):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    
    mean_true = tf.reduce_mean(y_true)
    mean_pred = tf.reduce_mean(y_pred)
    std_true = tf.math.reduce_std(y_true)
    std_pred = tf.math.reduce_std(y_pred)
    covariance = tf.reduce_mean((y_true - mean_true) * (y_pred - mean_pred))
    correlation = covariance / (std_true * std_pred + K.epsilon())
    std_ratio = std_pred / (std_true + K.epsilon())
    bias_ratio = mean_pred / (mean_true + K.epsilon())
    kge = 1 - tf.sqrt(
        tf.square(correlation - 1) + tf.square(std_ratio - 1) + tf.square(bias_ratio - 1)
    )
    return kge

def nse_metric(y_true, y_pred):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    
    mean_true = tf.reduce_mean(y_true)
    numerator = tf.reduce_sum(tf.square(y_true - y_pred))
    denominator = tf.reduce_sum(tf.square(y_true - mean_true))
    nse = 1 - numerator / (denominator + K.epsilon())
    return nse


def get_loss_function(loss_name, loss_params=None):
    available_loss_functions = {
        'nse_loss': NseLoss,
        'kge_loss': KgeLoss,
        'expectile_loss': ExpectileLoss,
        'quantile_loss': QuantileLoss,
        'huber_loss': HuberLoss,
        'weighted_mse_loss': WeightedMSELoss,
        'mae_loss': MaeLoss,
        'log_nse':LogNseLoss
    }

    if loss_name not in available_loss_functions:
        raise ValueError(f"Invalid loss function name: {loss_name}")

    loss_cls = available_loss_functions[loss_name]

    if loss_params:
        return loss_cls(**loss_params)
    else:
        return loss_cls()

def get_metric_function(metric_name):
    available_metrics = {
        'nse_metric': nse_metric,
        'kge_metric': kge_metric,
        'mae': tf.keras.metrics.MeanAbsoluteError(),
        'mse': tf.keras.metrics.MeanSquaredError(),
        # Add more metrics if needed
    }

    if metric_name not in available_metrics:
        raise ValueError(f"Invalid metric name: {metric_name}")

    return available_metrics[metric_name]



================================================
File: .ipynb_checkpoints/metrics-checkpoint.py
================================================
import logging
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from scipy import stats, signal
from xarray.core.dataarray import DataArray

import utils
from errors import AllNaNError

LOGGER = logging.getLogger(__name__)


def _check_all_nan(obs, sim):
    # Use np.isnan to check for missing values in NumPy arrays
    if np.all(np.isnan(obs)):
        raise ValueError("Observed data contains all NaN values.")
    if np.all(np.isnan(sim)):
        raise ValueError("Simulated data contains all NaN values.")


def get_available_metrics() -> List[str]:
    """Get list of available metrics.

    Returns
    -------
    List[str]
        List of implemented metric names.
    """
    metrics = [
        "NSE", "MSE", "RMSE", "KGE", "Alpha-NSE", "Pearson-r", "Beta-KGE", "Beta-NSE", "FHV", "FMS", "FLV",
        "Peak-Timing", "Missed-Peaks", "Peak-MAPE"
    ]
    return metrics


def _validate_inputs(obs: DataArray, sim: DataArray):
    if obs.shape != sim.shape:
        raise RuntimeError("Shapes of observations and simulations must match")

    if (len(obs.shape) > 1) and (obs.shape[1] > 1):
        raise RuntimeError("Metrics only defined for time series (1d or 2d with second dimension 1)")


def _mask_valid(obs: DataArray, sim: DataArray) -> Tuple[DataArray, DataArray]:
    # mask of invalid entries. NaNs in simulations can happen during validation/testing
    idx = (~sim.isnull()) & (~obs.isnull())

    obs = obs[idx]
    sim = sim[idx]

    return obs, sim


def _get_fdc(da: DataArray) -> np.ndarray:
    return da.sortby(da, ascending=False).values


def nse(obs: DataArray, sim: DataArray) -> float:
    r"""Calculate Nash-Sutcliffe Efficiency [#]_
    
    Nash-Sutcliffe Efficiency is the R-square between observed and simulated discharge.
    
    .. math:: \text{NSE} = 1 - \frac{\sum_{t=1}^{T}(Q_m^t - Q_o^t)^2}{\sum_{t=1}^T(Q_o^t - \overline{Q}_o)^2},
    
    where :math:`Q_m` are the simulations (here, `sim`) and :math:`Q_o` are observations (here, `obs`).
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.

    Returns
    -------
    float
        Nash-Sutcliffe Efficiency 
        
    References
    ----------
    .. [#] Nash, J. E.; Sutcliffe, J. V. (1970). "River flow forecasting through conceptual models part I - A 
        discussion of principles". Journal of Hydrology. 10 (3): 282-290. doi:10.1016/0022-1694(70)90255-6.

    """

    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    denominator = ((obs - obs.mean())**2).sum()
    numerator = ((sim - obs)**2).sum()

    value = 1 - numerator / denominator

    return float(value)


def mse(obs: DataArray, sim: DataArray) -> float:
    r"""Calculate mean squared error.
    
    .. math:: \text{MSE} = \frac{1}{T}\sum_{t=1}^T (\widehat{y}_t - y_t)^2,
    
    where :math:`\widehat{y}` are the simulations (here, `sim`) and :math:`y` are observations 
    (here, `obs`).
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.

    Returns
    -------
    float
        Mean squared error. 

    """

    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    return float(((sim - obs)**2).mean())


def rmse(obs: DataArray, sim: DataArray) -> float:
    r"""Calculate root mean squared error.
    
    .. math:: \text{RMSE} = \sqrt{\frac{1}{T}\sum_{t=1}^T (\widehat{y}_t - y_t)^2},
    
    where :math:`\widehat{y}` are the simulations (here, `sim`) and :math:`y` are observations 
    (here, `obs`).
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.

    Returns
    -------
    float
        Root mean sqaured error.

    """

    return np.sqrt(mse(obs, sim))


def alpha_nse(obs: DataArray, sim: DataArray) -> float:
    r"""Calculate the alpha NSE decomposition [#]_
    
    The alpha NSE decomposition is the fraction of the standard deviations of simulations and observations.
    
    .. math:: \alpha = \frac{\sigma_s}{\sigma_o},
    
    where :math:`\sigma_s` is the standard deviation of the simulations (here, `sim`) and :math:`\sigma_o` is the 
    standard deviation of the observations (here, `obs`).
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.

    Returns
    -------
    float
        Alpha NSE decomposition.
        
    References
    ----------
    .. [#] Gupta, H. V., Kling, H., Yilmaz, K. K., & Martinez, G. F. (2009). Decomposition of the mean squared error 
        and NSE performance criteria: Implications for improving hydrological modelling. Journal of hydrology, 377(1-2),
        80-91.

    """

    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    return float(sim.std() / obs.std())


def beta_nse(obs: DataArray, sim: DataArray) -> float:
    r"""Calculate the beta NSE decomposition [#]_

    The beta NSE decomposition is the difference of the mean simulation and mean observation divided by the standard 
    deviation of the observations.

    .. math:: \beta = \frac{\mu_s - \mu_o}{\sigma_o},
    
    where :math:`\mu_s` is the mean of the simulations (here, `sim`), :math:`\mu_o` is the mean of the observations 
    (here, `obs`) and :math:`\sigma_o` the standard deviation of the observations.

    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.

    Returns
    -------
    float
        Beta NSE decomposition.

    References
    ----------
    .. [#] Gupta, H. V., Kling, H., Yilmaz, K. K., & Martinez, G. F. (2009). Decomposition of the mean squared error 
        and NSE performance criteria: Implications for improving hydrological modelling. Journal of hydrology, 377(1-2),
        80-91.

    """
    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    return float((sim.mean() - obs.mean()) / obs.std())


def beta_kge(obs: DataArray, sim: DataArray) -> float:
    r"""Calculate the beta KGE term [#]_
    
    The beta term of the Kling-Gupta Efficiency is defined as the fraction of the means.
    
    .. math:: \beta_{\text{KGE}} = \frac{\mu_s}{\mu_o},
    
    where :math:`\mu_s` is the mean of the simulations (here, `sim`) and :math:`\mu_o` is the mean of the observations 
    (here, `obs`).
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.

    Returns
    -------
    float
        Beta NSE decomposition.

    References
    ----------
    .. [#] Gupta, H. V., Kling, H., Yilmaz, K. K., & Martinez, G. F. (2009). Decomposition of the mean squared error 
        and NSE performance criteria: Implications for improving hydrological modelling. Journal of hydrology, 377(1-2),
        80-91.

    """
    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    return float(sim.mean() / obs.mean())


def kge(obs: DataArray, sim: DataArray, weights: List[float] = [1., 1., 1.]) -> float:
    r"""Calculate the Kling-Gupta Efficieny [#]_
    
    .. math:: 
        \text{KGE} = 1 - \sqrt{[ s_r (r - 1)]^2 + [s_\alpha ( \alpha - 1)]^2 + 
            [s_\beta(\beta_{\text{KGE}} - 1)]^2},
            
    where :math:`r` is the correlation coefficient, :math:`\alpha` the :math:`\alpha`-NSE decomposition, 
    :math:`\beta_{\text{KGE}}` the fraction of the means and :math:`s_r, s_\alpha, s_\beta` the corresponding weights
    (here the three float values in the `weights` parameter).
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.
    weights : List[float]
        Weighting factors of the 3 KGE parts, by default each part has a weight of 1.

    Returns
    -------
    float
        Kling-Gupta Efficiency
    
    References
    ----------
    .. [#] Gupta, H. V., Kling, H., Yilmaz, K. K., & Martinez, G. F. (2009). Decomposition of the mean squared error 
        and NSE performance criteria: Implications for improving hydrological modelling. Journal of hydrology, 377(1-2),
        80-91.

    """
    if len(weights) != 3:
        raise ValueError("Weights of the KGE must be a list of three values")

    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    if len(obs) < 2:
        return np.nan

    r, _ = stats.pearsonr(obs.values, sim.values)

    alpha = sim.std() / obs.std()
    beta = sim.mean() / obs.mean()

    value = (weights[0] * (r - 1)**2 + weights[1] * (alpha - 1)**2 + weights[2] * (beta - 1)**2)

    return 1 - np.sqrt(float(value))


def pearsonr(obs: DataArray, sim: DataArray) -> float:
    """Calculate pearson correlation coefficient (using scipy.stats.pearsonr)

    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.

    Returns
    -------
    float
        Pearson correlation coefficient

    """

    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    if len(obs) < 2:
        return np.nan

    r, _ = stats.pearsonr(obs.values, sim.values)

    return float(r)


def fdc_fms(obs: DataArray, sim: DataArray, lower: float = 0.2, upper: float = 0.7) -> float:
    r"""Calculate the slope of the middle section of the flow duration curve [#]_
    
    .. math:: 
        \%\text{BiasFMS} = \frac{\left | \log(Q_{s,\text{lower}}) - \log(Q_{s,\text{upper}}) \right | - 
            \left | \log(Q_{o,\text{lower}}) - \log(Q_{o,\text{upper}}) \right |}{\left | 
            \log(Q_{s,\text{lower}}) - \log(Q_{s,\text{upper}}) \right |} \times 100,
            
    where :math:`Q_{s,\text{lower/upper}}` corresponds to the FDC of the simulations (here, `sim`) at the `lower` and
    `upper` bound of the middle section and :math:`Q_{o,\text{lower/upper}}` similarly for the observations (here, 
    `obs`).
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.
    lower : float, optional
        Lower bound of the middle section in range ]0,1[, by default 0.2
    upper : float, optional
        Upper bound of the middle section in range ]0,1[, by default 0.7
        
    Returns
    -------
    float
        Slope of the middle section of the flow duration curve.
    
    References
    ----------
    .. [#] Yilmaz, K. K., Gupta, H. V., and Wagener, T. ( 2008), A process-based diagnostic approach to model 
        evaluation: Application to the NWS distributed hydrologic model, Water Resour. Res., 44, W09417, 
        doi:10.1029/2007WR006716. 
    """
    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    if len(obs) < 1:
        return np.nan

    if any([(x <= 0) or (x >= 1) for x in [upper, lower]]):
        raise ValueError("upper and lower have to be in range ]0,1[")

    if lower >= upper:
        raise ValueError("The lower threshold has to be smaller than the upper.")

    # get arrays of sorted (descending) discharges
    obs = _get_fdc(obs)
    sim = _get_fdc(sim)

    # for numerical reasons change 0s to 1e-6. Simulations can still contain negatives, so also reset those.
    sim[sim <= 0] = 1e-6
    obs[obs == 0] = 1e-6

    # calculate fms part by part
    qsm_lower = np.log(sim[np.round(lower * len(sim)).astype(int)])
    qsm_upper = np.log(sim[np.round(upper * len(sim)).astype(int)])
    qom_lower = np.log(obs[np.round(lower * len(obs)).astype(int)])
    qom_upper = np.log(obs[np.round(upper * len(obs)).astype(int)])

    fms = ((qsm_lower - qsm_upper) - (qom_lower - qom_upper)) / (qom_lower - qom_upper + 1e-6)

    return fms * 100


def fdc_fhv(obs: DataArray, sim: DataArray, h: float = 0.02) -> float:
    r"""Calculate the peak flow bias of the flow duration curve [#]_
    
    .. math:: \%\text{BiasFHV} = \frac{\sum_{h=1}^{H}(Q_{s,h} - Q_{o,h})}{\sum_{h=1}^{H}Q_{o,h}} \times 100,
    
    where :math:`Q_s` are the simulations (here, `sim`), :math:`Q_o` the observations (here, `obs`) and `H` is the upper
    fraction of flows of the FDC (here, `h`). 
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.
    h : float, optional
        Fraction of upper flows to consider as peak flows of range ]0,1[, be default 0.02.
        
    Returns
    -------
    float
        Peak flow bias.
    
    References
    ----------
    .. [#] Yilmaz, K. K., Gupta, H. V., and Wagener, T. ( 2008), A process-based diagnostic approach to model 
        evaluation: Application to the NWS distributed hydrologic model, Water Resour. Res., 44, W09417, 
        doi:10.1029/2007WR006716. 
    """
    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    if len(obs) < 1:
        return np.nan

    if (h <= 0) or (h >= 1):
        raise ValueError("h has to be in range ]0,1[. Consider small values, e.g. 0.02 for 2% peak flows")

    # get arrays of sorted (descending) discharges
    obs = _get_fdc(obs)
    sim = _get_fdc(sim)

    # subset data to only top h flow values
    obs = obs[:np.round(h * len(obs)).astype(int)]
    sim = sim[:np.round(h * len(sim)).astype(int)]

    fhv = np.sum(sim - obs) / np.sum(obs)

    return fhv * 100


def fdc_flv(obs: DataArray, sim: DataArray, l: float = 0.3) -> float:
    r"""Calculate the low flow bias of the flow duration curve [#]_
    
    .. math:: 
        \%\text{BiasFMS} = -1 \frac{\sum_{l=1}^{L}[\log(Q_{s,l}) - \log(Q_{s,L})] - \sum_{l=1}^{L}[\log(Q_{o,l})
            - \log(Q_{o,L})]}{\sum_{l=1}^{L}[\log(Q_{o,l}) - \log(Q_{o,L})]} \times 100,
    
    where :math:`Q_s` are the simulations (here, `sim`), :math:`Q_o` the observations (here, `obs`) and `L` is the lower
    fraction of flows of the FDC (here, `l`). 
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.
    l : float, optional
        Fraction of lower flows to consider as low flows of range ]0,1[, be default 0.3.
        
    Returns
    -------
    float
        Low flow bias.
    
    References
    ----------
    .. [#] Yilmaz, K. K., Gupta, H. V., and Wagener, T. ( 2008), A process-based diagnostic approach to model 
        evaluation: Application to the NWS distributed hydrologic model, Water Resour. Res., 44, W09417, 
        doi:10.1029/2007WR006716. 
    """
    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    if len(obs) < 1:
        return np.nan

    if (l <= 0) or (l >= 1):
        raise ValueError("l has to be in range ]0,1[. Consider small values, e.g. 0.3 for 30% low flows")

    # get arrays of sorted (descending) discharges
    obs = _get_fdc(obs)
    sim = _get_fdc(sim)

    # for numerical reasons change 0s to 1e-6. Simulations can still contain negatives, so also reset those.
    sim[sim <= 0] = 1e-6
    obs[obs == 0] = 1e-6

    obs = obs[-np.round(l * len(obs)).astype(int):]
    sim = sim[-np.round(l * len(sim)).astype(int):]

    # transform values to log scale
    obs = np.log(obs)
    sim = np.log(sim)

    # calculate flv part by part
    qsl = np.sum(sim - sim.min())
    qol = np.sum(obs - obs.min())

    flv = -1 * (qsl - qol) / (qol + 1e-6)

    return flv * 100


def mean_peak_timing(obs: DataArray,
                     sim: DataArray,
                     window: int = None,
                     resolution: str = '1D',
                     datetime_coord: str = None) -> float:
    """Mean difference in peak flow timing.
    
    Uses scipy.find_peaks to find peaks in the observed time series. Starting with all observed peaks, those with a
    prominence of less than the standard deviation of the observed time series are discarded. Next, the lowest peaks
    are subsequently discarded until all remaining peaks have a distance of at least 100 steps. Finally, the
    corresponding peaks in the simulated time series are searched in a window of size `window` on either side of the
    observed peaks and the absolute time differences between observed and simulated peaks is calculated.
    The final metric is the mean absolute time difference across all peaks. For more details, see Appendix of [#]_
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.
    window : int, optional
        Size of window to consider on each side of the observed peak for finding the simulated peak. That is, the total
        window length to find the peak in the simulations is :math:`2 * \\text{window} + 1` centered at the observed
        peak. The default depends on the temporal resolution, e.g. for a resolution of '1D', a window of 3 is used and 
        for a resolution of '1H' the the window size is 12.
    resolution : str, optional
        Temporal resolution of the time series in pandas format, e.g. '1D' for daily and '1H' for hourly.
    datetime_coord : str, optional
        Name of datetime coordinate. Tried to infer automatically if not specified.
        

    Returns
    -------
    float
        Mean peak time difference.

    References
    ----------
    .. [#] Kratzert, F., Klotz, D., Hochreiter, S., and Nearing, G. S.: A note on leveraging synergy in multiple 
        meteorological datasets with deep learning for rainfall-runoff modeling, Hydrol. Earth Syst. Sci. Discuss., 
        https://doi.org/10.5194/hess-2020-221, in review, 2020. 
    """
    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations (scipy's find_peaks doesn't guarantee correctness with NaNs)
    obs, sim = _mask_valid(obs, sim)

    # heuristic to get indices of peaks and their corresponding height.
    peaks, _ = signal.find_peaks(obs.values, distance=100, prominence=np.std(obs.values))

    # infer name of datetime index
    if datetime_coord is None:
        datetime_coord = utils.infer_datetime_coord(obs)

    if window is None:
        # infer a reasonable window size
        window = max(int(utils.get_frequency_factor('12H', resolution)), 3)

    # evaluate timing
    timing_errors = []
    for idx in peaks:
        # skip peaks at the start and end of the sequence and peaks around missing observations
        # (NaNs that were removed in obs & sim would result in windows that span too much time).
        if (idx - window < 0) or (idx + window >= len(obs)) or (pd.date_range(obs[idx - window][datetime_coord].values,
                                                                              obs[idx + window][datetime_coord].values,
                                                                              freq=resolution).size != 2 * window + 1):
            continue

        # check if the value at idx is a peak (both neighbors must be smaller)
        if (sim[idx] > sim[idx - 1]) and (sim[idx] > sim[idx + 1]):
            peak_sim = sim[idx]
        else:
            # define peak around idx as the max value inside of the window
            values = sim[idx - window:idx + window + 1]
            peak_sim = values[values.argmax()]

        # get xarray object of qobs peak, for getting the date and calculating the datetime offset
        peak_obs = obs[idx]

        # calculate the time difference between the peaks
        delta = peak_obs.coords[datetime_coord] - peak_sim.coords[datetime_coord]

        timing_error = np.abs(delta.values / pd.to_timedelta(resolution))

        timing_errors.append(timing_error)

    return np.mean(timing_errors) if len(timing_errors) > 0 else np.nan


def missed_peaks(obs: DataArray,
                 sim: DataArray,
                 window: int = None,
                 resolution: str = '1D',
                 percentile: float = 80,
                 datetime_coord: str = None) -> float:
    """Fraction of missed peaks.
    
    Uses scipy.find_peaks to find peaks in the observed and simulated time series above a certain percentile. Counts
    the number of peaks in obs that do not exist in sim within the specified window.

    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.
    window : int, optional
        Size of window to consider on each side of the observed peak for finding the simulated peak. That is, the total
        window length to find the peak in the simulations is :math:`2 * \\text{window} + 1` centered at the observed
        peak. The default depends on the temporal resolution, e.g. for a resolution of '1D', a window of 1 is used and 
        for a resolution of '1H' the the window size is 12. Note that this is a different default window size than is
        used in the peak-timing metric for '1D'.
    resolution : str, optional
        Temporal resolution of the time series in pandas format, e.g. '1D' for daily and '1H' for hourly.
    percentile: float, optional
        Only consider peaks above this flow percentile (0, 100).
    datetime_coord : str, optional
        Name of datetime coordinate. Tried to infer automatically if not specified.

    Returns
    -------
    float
        Fraction of missed peaks.   
    """
    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations (scipy's find_peaks doesn't guarantee correctness with NaNs)
    obs, sim = _mask_valid(obs, sim)

    # minimum height of a peak, as defined by percentile, which can be passed
    min_obs_height = np.percentile(obs.values, percentile)
    min_sim_height = np.percentile(sim.values, percentile)

    # get time indices of peaks in obs and sim.
    peaks_obs_times, _ = signal.find_peaks(obs, distance=30, height=min_obs_height)
    peaks_sim_times, _ = signal.find_peaks(sim, distance=30, height=min_sim_height)

    if len(peaks_obs_times) == 0:
        return 0.

    # infer name of datetime index
    if datetime_coord is None:
        datetime_coord = utils.infer_datetime_coord(obs)

    # infer a reasonable window size
    if window is None:
        window = max(int(utils.get_frequency_factor('12H', resolution)), 1)

    # count missed peaks
    missed_events = 0

    for idx in peaks_obs_times:

        # skip peaks at the start and end of the sequence and peaks around missing observations
        # (NaNs that were removed in obs & sim would result in windows that span too much time).
        if (idx - window < 0) or (idx + window >= len(obs)) or (pd.date_range(obs[idx - window][datetime_coord].values,
                                                                              obs[idx + window][datetime_coord].values,
                                                                              freq=resolution).size != 2 * window + 1):
            continue

        nearby_peak_sim_index = np.where(np.abs(peaks_sim_times - idx) <= window)[0]
        if len(nearby_peak_sim_index) == 0:
            missed_events += 1

    return missed_events / len(peaks_obs_times)


def mean_absolute_percentage_peak_error(obs: DataArray, sim: DataArray) -> float:
    r"""Calculate the mean absolute percentage error (MAPE) for peaks

    .. math:: \text{MAPE}_\text{peak} = \frac{1}{P}\sum_{p=1}^{P} \left |\frac{Q_{s,p} - Q_{o,p}}{Q_{o,p}} \right | \times 100,

    where :math:`Q_{s,p}` are the simulated peaks (here, `sim`), :math:`Q_{o,p}` the observed peaks (here, `obs`) and
    `P` is the number of peaks.

    Uses scipy.find_peaks to find peaks in the observed time series. The observed peaks indices are used to subset
    observed and simulated flows. Finally, the MAPE metric is calculated as the mean absolute percentage error
    of observed peak flows and corresponding simulated flows.

    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.

    Returns
    -------
    float
        Mean absolute percentage error (MAPE) for peaks.
    """
    # verify inputs
    _validate_inputs(obs, sim)

    # get time series with only valid observations
    obs, sim = _mask_valid(obs, sim)

    # return np.nan if there are no valid observed or simulated values
    if obs.size == 0 or sim.size == 0:
        return np.nan

    # heuristic to get indices of peaks and their corresponding height.
    peaks, _ = signal.find_peaks(obs.values, distance=100, prominence=np.std(obs.values))

    # check if any peaks exist, otherwise return np.nan
    if peaks.size == 0:
        return np.nan

    # subset data to only peak values
    obs = obs[peaks].values
    sim = sim[peaks].values

    # calculate the mean absolute percentage peak error
    peak_mape = np.sum(np.abs((sim - obs) / obs)) / peaks.size * 100

    return peak_mape


def calculate_all_metrics(obs: DataArray,
                          sim: DataArray,
                          resolution: str = "1D",
                          datetime_coord: str = None) -> Dict[str, float]:
    """Calculate all metrics with default values.
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.
    resolution : str, optional
        Temporal resolution of the time series in pandas format, e.g. '1D' for daily and '1H' for hourly.
    datetime_coord : str, optional
        Datetime coordinate in the passed DataArray. Tried to infer automatically if not specified.
        
    Returns
    -------
    Dict[str, float]
        Dictionary with keys corresponding to metric name and values corresponding to metric values.

    Raises
    ------
    AllNaNError
        If all observations or all simulations are NaN.
    """
    _check_all_nan(obs, sim)

    results = {
        "NSE": nse(obs, sim),
        "MSE": mse(obs, sim),
        "RMSE": rmse(obs, sim),
        "KGE": kge(obs, sim),
        "Alpha-NSE": alpha_nse(obs, sim),
        "Beta-KGE": beta_kge(obs, sim),
        "Beta-NSE": beta_nse(obs, sim),
        "Pearson-r": pearsonr(obs, sim),
        "FHV": fdc_fhv(obs, sim),
        "FMS": fdc_fms(obs, sim),
        "FLV": fdc_flv(obs, sim),
        "Peak-Timing": mean_peak_timing(obs, sim, resolution=resolution, datetime_coord=datetime_coord),
        "Peak-MAPE": mean_absolute_percentage_peak_error(obs, sim)
    }

    return results


def calculate_metrics(obs: DataArray,
                      sim: DataArray,
                      metrics: List[str],
                      resolution: str = "1D",
                      datetime_coord: str = None) -> Dict[str, float]:
    """Calculate specific metrics with default values.
    
    Parameters
    ----------
    obs : DataArray
        Observed time series.
    sim : DataArray
        Simulated time series.
    metrics : List[str]
        List of metric names.
    resolution : str, optional
        Temporal resolution of the time series in pandas format, e.g. '1D' for daily and '1H' for hourly.
    datetime_coord : str, optional
        Datetime coordinate in the passed DataArray. Tried to infer automatically if not specified.

    Returns
    -------
    Dict[str, float]
        Dictionary with keys corresponding to metric name and values corresponding to metric values.

    Raises
    ------
    AllNaNError
        If all observations or all simulations are NaN.
    """
    if 'all' in metrics:
        return calculate_all_metrics(obs, sim, resolution=resolution)

    _check_all_nan(obs, sim)

    values = {}
    for metric in metrics:
        if metric.lower() == "nse":
            values["NSE"] = nse(obs, sim)
        elif metric.lower() == "mse":
            values["MSE"] = mse(obs, sim)
        elif metric.lower() == "rmse":
            values["RMSE"] = rmse(obs, sim)
        elif metric.lower() == "kge":
            values["KGE"] = kge(obs, sim)
        elif metric.lower() == "alpha-nse":
            values["Alpha-NSE"] = alpha_nse(obs, sim)
        elif metric.lower() == "beta-kge":
            values["Beta-KGE"] = beta_kge(obs, sim)
        elif metric.lower() == "beta-nse":
            values["Beta-NSE"] = beta_nse(obs, sim)
        elif metric.lower() == "pearson-r":
            values["Pearson-r"] = pearsonr(obs, sim)
        elif metric.lower() == "fhv":
            values["FHV"] = fdc_fhv(obs, sim)
        elif metric.lower() == "fms":
            values["FMS"] = fdc_fms(obs, sim)
        elif metric.lower() == "flv":
            values["FLV"] = fdc_flv(obs, sim)
        elif metric.lower() == "peak-timing":
            values["Peak-Timing"] = mean_peak_timing(obs, sim, resolution=resolution, datetime_coord=datetime_coord)
        elif metric.lower() == "missed-peaks":
            values["Missed-Peaks"] = missed_peaks(obs, sim, resolution=resolution, datetime_coord=datetime_coord)
        elif metric.lower() == "peak-mape":
            values["Peak-MAPE"] = mean_absolute_percentage_peak_error(obs, sim)
        else:
            raise RuntimeError(f"Unknown metric {metric}")

    return values


def _check_all_nan(obs: DataArray, sim: DataArray):
    """Check if all observations or simulations are NaN and raise an exception if this is the case.

    Raises
    ------
    AllNaNError
        If all observations or all simulations are NaN.
    """
    if all(obs.isnull()):
        raise AllNaNError("All observed values are NaN, thus metrics will be NaN, too.")
    if all(sim.isnull()):
        raise AllNaNError("All simulated values are NaN, thus metrics will be NaN, too.")



================================================
File: .ipynb_checkpoints/notes-checkpoint.txt
================================================
apptainer shell --nv --bind /scratch/:/scratch/ /scratch/kdahal3/streamflow-forecasting/streamflow.sif


================================================
File: .ipynb_checkpoints/run_all_models-checkpoint.sh
================================================
#!/bin/bash

# Define the different values for n_steps_out and corresponding output_prefix
n_steps_out_values=(7)
output_prefix_suffix=("7day")

# Define the Model types
model_types=("LSTM")  # All supported models

# Define the loss functions and parameters
declare -A loss_functions
loss_functions=(
    # ["nse_loss"]=""
    # ["kge_loss"]=""
    # ["huber_loss"]=""
    ["quantile_low"]="quantile: 0.15"
    ["quantile_high"]="quantile: 0.85"
    # ["expectile_low"]="expectile: 0.15"
    # ["expectile_high"]="expectile: 0.85"
)

# Map loss function keys to actual loss function names
declare -A loss_function_names
loss_function_names=(
    # ["nse_loss"]="nse_loss"
    # ["kge_loss"]="kge_loss"
    # ["huber_loss"]="huber_loss"
    ["quantile_low"]="quantile_loss"
    ["quantile_high"]="quantile_loss"
    # ["expectile_low"]="expectile_loss"
    # ["expectile_high"]="expectile_loss"
)

# Create a folder for the configuration files if it doesn't exist
config_folder="config_files"
mkdir -p $config_folder

# Path to the base config file
base_config="config.yml"

# Loop over the Model types, n_steps_out values, and loss functions
for model_type in "${model_types[@]}"; do
    for i in "${!n_steps_out_values[@]}"; do
        # Set the n_steps_out and output_prefix for this run
        n_steps_out=${n_steps_out_values[$i]}
        output_suffix=${output_prefix_suffix[$i]}
        
        for loss_key in "${!loss_functions[@]}"; do
            loss_function=${loss_function_names[$loss_key]}
            loss_params=${loss_functions[$loss_key]}
            
            # Construct the output prefix using Model type, loss function, and day suffix
            output_prefix="camels_${model_type}_${loss_key}_${output_suffix}"
            
            # Construct the filename for the config file
            config_file="${config_folder}/config_${output_prefix}_nsteps_${n_steps_out}.yml"
            
            # Create a new config file for this run by copying the base config
            cp "$base_config" "$config_file"
            
            # Modify the copied config file using | as the delimiter to avoid conflicts
            sed -i "s|^n_steps_out:.*|n_steps_out: $n_steps_out|" "$config_file"
            sed -i "s|^output_prefix:.*|output_prefix: \"$output_prefix\"|" "$config_file"
            sed -i "s|^model_type:.*|model_type: \"$model_type\"|" "$config_file"
            sed -i "s|^loss_function:.*|loss_function: \"$loss_function\"|" "$config_file"
            
            # Remove any existing loss_params section
            sed -i "/^loss_params:/,/^[^[:space:]]/{/^loss_params:/!{/^[^[:space:]]/!d}}" "$config_file"
            
            # If loss_params is not empty, add it
            if [ -n "$loss_params" ]; then
                echo -e "\nloss_params:\n  $loss_params" >> "$config_file"
            fi
            
            # Adjust epochs if model_type is TRANSFORMER
            if [ "$model_type" == "TRANSFORMER" ]; then
                sed -i "s|^epochs:.*|epochs: 150|" "$config_file"
            else
                # Assuming the base config has epochs set to default value
                sed -i "s|^epochs:.*|epochs: 50|" "$config_file"
            fi
            
            # Submit the job and pass the specific config file as an argument
            sbatch submit_jobs.sh "$config_file"
            
            echo "Submitted job with model_type: $model_type, n_steps_out: $n_steps_out, loss_function: $loss_function, and output_prefix: $output_prefix using $config_file"
        done
    done
done



================================================
File: .ipynb_checkpoints/run_evaluation-checkpoint.sh
================================================
#!/bin/bash

# This script automates the evaluation of all pre-trained models
# on the validation dataset. It reconstructs the output directory names
# from the training script and calls the evaluation python script for each model.

# --- Configuration: Match this to your training script and environment ---

# Base directory containing all your model run folders
BASE_MODEL_DIR="/scratch/kdahal3/camels_losses"

# Define the different values for n_steps_out and corresponding output_prefix
n_steps_out_values=(1 7 30)
output_prefix_suffix=("1day" "7day" "30day")

# Define the Model types
model_types=("LSTM") # Add other models like "TRANSFORMER" if you trained them

# Define the loss functions and parameters
declare -A loss_functions
loss_functions=(
    # ["nse_loss"]=""
    # ["kge_loss"]=""
    # ["huber_loss"]=""
    # ["quantile_low"]="quantile: 0.15"
    # ["quantile_high"]="quantile: 0.85"
    ["expectile_low"]="expectile: 0.15"
    ["expectile_high"]="expectile: 0.85"
)

# --- Paths (relative to BASE_MODEL_DIR or absolute) ---
CONFIG_FOLDER="${BASE_MODEL_DIR}/config_files"
VAL_DATA_FOLDER="/scratch/kdahal3/FF/val_data"

# !!! THIS IS THE CORRECTED LINE !!!
# Point to the actual location of your evaluation script
PYTHON_SCRIPT_PATH="/scratch/kdahal3/FF/evaluate_validation.py"

# --- Evaluation Loop ---

echo "Starting evaluation of all trained models..."
echo "Base model directory: ${BASE_MODEL_DIR}"

# Loop over the Model types, n_steps_out values, and loss functions
for model_type in "${model_types[@]}"; do
    for i in "${!n_steps_out_values[@]}"; do
        n_steps_out=${n_steps_out_values[$i]}
        output_suffix=${output_prefix_suffix[$i]}
        
        for loss_key in "${!loss_functions[@]}"; do
            # Reconstruct the output prefix to find the model directory
            output_prefix="camels_${model_type}_${loss_key}_${output_suffix}"
            
            echo "-----------------------------------------------------------------"
            echo "Looking for model: $output_prefix"
            
            # Define paths for this specific model run using the BASE_MODEL_DIR
            MODEL_DIR="${BASE_MODEL_DIR}/${output_prefix}"
            CONFIG_FILE="${CONFIG_FOLDER}/config_${output_prefix}_nsteps_${n_steps_out}.yml"
            MODEL_PATH="${MODEL_DIR}/combined_model.keras"
            MIN_MAX_PATH="${MODEL_DIR}/min_max.csv"
            OUTPUT_DIR="${MODEL_DIR}/validation_results" # Save results in a subfolder

            # Check if the model file actually exists before trying to evaluate
            if [ -f "$MODEL_PATH" ]; then
                echo "Found model. Starting evaluation for $output_prefix"
                
                # Call the Python evaluation script with all the necessary arguments
                python "$PYTHON_SCRIPT_PATH" \
                  --config "$CONFIG_FILE" \
                  --model-path "$MODEL_PATH" \
                  --min-max-path "$MIN_MAX_PATH" \
                  --val-data-folder "$VAL_DATA_FOLDER" \
                  --output-dir "$OUTPUT_DIR"
                  
                echo "Evaluation finished for $output_prefix"
            else
                echo "Warning: Model file not found at $MODEL_PATH. Skipping evaluation."
            fi
        done
    done
done

echo "-----------------------------------------------------------------"
echo "All evaluations complete."


================================================
File: .ipynb_checkpoints/spatial_uncertainty_analysis-checkpoint.py
================================================
import os
import pandas as pd
import glob
import numpy as np
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cfeature
from functools import reduce
import re # Import the regular expression module

# --- CONFIGURATION ---
base_dir = '/scratch/kdahal3/camels_losses'
log_file_path = os.path.join(base_dir, 'merge_log.txt')
forecast_horizons = [1, 7, 30]
loss_functions = [
    "nse_loss", "kge_loss", "huber_loss", "quantile_low",
    "quantile_high", "expectile_low", "expectile_high"
]
coords_file = '/scratch/kdahal3/Caravan/attributes/camels/attributes_other_camels.csv'

def clean_and_zfill(station_id_series):
    return station_id_series.astype(str).str.strip().str.zfill(8)

# Clear old log file
if os.path.exists(log_file_path):
    os.remove(log_file_path)

def log_message(message):
    print(message)
    with open(log_file_path, 'a') as f:
        f.write(message + '\n')

log_message("="*60)
log_message("Starting spatial uncertainty analysis...")
log_message("="*60)

# --- 1. LOAD AND PREPARE COORDINATES DATAFRAME ---
log_message("\n--- STEP 1: Loading and Cleaning Coordinate Data ---")
try:
    coords_df = pd.read_csv(coords_file)
    coords_df['Station_ID'] = coords_df['gauge_id'].apply(lambda x: str(x).split('_')[1])
    coords_df['Station_ID'] = clean_and_zfill(coords_df['Station_ID'])
    coords_df = coords_df[['Station_ID', 'gauge_lat', 'gauge_lon']].drop_duplicates(subset=['Station_ID'])
    log_message(f"  Cleaned and prepared coordinates for {len(coords_df)} unique stations.")
    log_message(f"  Example coord Station_IDs:\n{coords_df.head().to_string()}")
    log_message("-" * 50)
except Exception as e:
    log_message(f"[FATAL ERROR] Could not load or process coordinate file: {e}")
    exit()

# --- 2. PROCESS DATA FOR EACH HORIZON ---
all_horizon_data = []

for fh in forecast_horizons:
    log_message(f"\n{'='*25} Processing {fh}-Day Forecast {'='*25}")
    horizon_key = f'{fh}day'
    
    model_dfs = []
    
    for loss_func in loss_functions:
        # ... (This part is working correctly, so keeping it concise for brevity) ...
        folder_path = os.path.join(base_dir, f"camels_LSTM_{loss_func}_{horizon_key}/evaluation_metrics/")
        if not os.path.exists(folder_path): continue
        csv_files = glob.glob(os.path.join(folder_path, '*.csv'))
        if not csv_files: continue
        
        station_ids = []
        nse_values = []
        
        for file_path in csv_files:
            try:
                station_id_from_filename = os.path.basename(file_path).split('.')[0]
                temp_df = pd.read_csv(file_path, index_col=0).reset_index()
                temp_df.rename(columns={'index': 'Day'}, inplace=True)
                day_specific_series = temp_df[temp_df['Day'] == fh]
                if day_specific_series.empty or 'NSE' not in day_specific_series.columns: continue
                nse_value = day_specific_series['NSE'].iloc[0]
                station_ids.append(station_id_from_filename)
                nse_values.append(nse_value)
            except Exception:
                continue

        if not station_ids: continue
        model_df = pd.DataFrame({'Station_ID': station_ids, f'NSE_{loss_func}': nse_values})
        model_df['Station_ID'] = clean_and_zfill(model_df['Station_ID'])
        model_dfs.append(model_df)

    if not model_dfs:
        log_message(f"[CRITICAL] No individual model data loaded for {fh}-day. Skipping.")
        continue
    
    log_message(f"\n--- STEP 2: Merging {len(model_dfs)} individual models for {fh}-day ---")
    merged_individual_df = reduce(lambda left, right: pd.merge(left, right, on='Station_ID', how='outer'), model_dfs)
    
    log_message(f"  Merged individual models DataFrame shape: {merged_individual_df.shape}")
    log_message(f"  Example IDs in 'Station_ID' column RIGHT AFTER MERGE:\n{merged_individual_df.head().to_string()}")

    # #######################################################################
    # # --- THE BRUTE FORCE FIX ---
    # # If the 'Station_ID' column is messy, we will rebuild it.
    # #######################################################################
    
    # Define a regex to find an 8-digit number
    id_pattern = re.compile(r'(\d{8})')

    # Function to apply the regex
    def extract_clean_id(messy_id):
        match = id_pattern.search(str(messy_id))
        return match.group(1) if match else None

    log_message("  Forcibly cleaning the 'Station_ID' column using regex...")
    # Apply the function to create a new, clean column
    merged_individual_df['Clean_Station_ID'] = merged_individual_df['Station_ID'].apply(extract_clean_id)
    
    # Drop the old, messy column and rename the new one
    merged_individual_df = merged_individual_df.drop(columns=['Station_ID'])
    merged_individual_df = merged_individual_df.rename(columns={'Clean_Station_ID': 'Station_ID'})

    # Zfill just to be safe
    merged_individual_df['Station_ID'] = clean_and_zfill(merged_individual_df['Station_ID'])

    log_message(f"  Example IDs in 'Station_ID' column AFTER BRUTE FORCE FIX:\n{merged_individual_df.head().to_string()}")

    nse_cols = [col for col in merged_individual_df.columns if 'NSE_' in col]
    merged_individual_df['NSE_Range'] = merged_individual_df[nse_cols].max(axis=1) - merged_individual_df[nse_cols].min(axis=1)
    log_message("  Calculated NSE_Range (uncertainty metric).")
    log_message("-" * 50)

    # --- (The rest of the script is largely the same, but uses log_message) ---
    log_message(f"\n--- STEP 3: Loading BMA results for {fh}-day ---")
    bma_file = os.path.join(base_dir, f'bma_results_{fh}day_with_metrics.csv')
    if not os.path.isfile(bma_file):
        log_message(f"  [WARNING] BMA results file not found: {bma_file}")
        bma_df = pd.DataFrame(columns=['Station_ID', 'NSE_BMA'])
    else:
        log_message(f"  Loading BMA results from: {bma_file}")
        bma_df = pd.read_csv(bma_file)
        if 'station_id' in bma_df.columns:
             bma_df.rename(columns={'station_id': 'Station_ID'}, inplace=True)
        bma_df['Station_ID'] = clean_and_zfill(bma_df['Station_ID'])
        bma_df = bma_df[['Station_ID', 'NSE_BMA']].drop_duplicates(subset=['Station_ID'])
        log_message(f"  Cleaned BMA DataFrame. Shape: {bma_df.shape}")
        log_message(f"  Example BMA Station_IDs:\n{bma_df.head().to_string()}")
    log_message("-" * 50)

    log_message(f"\n--- STEP 4: Performing Final Merges for {fh}-day ---")
    
    # Log the head of each dataframe to the log file before merging
    log_message("\n--- PRE-MERGE DATAFRAME HEADS ---\n")
    log_message("Individual Models DF (to be merged):")
    log_message(merged_individual_df.head().to_string())
    log_message("\nBMA DF (to be merged):")
    log_message(bma_df.head().to_string())
    log_message("\nCoords DF (to be merged):")
    log_message(coords_df.head().to_string())
    log_message("\n----------------------------------\n")

    log_message("    Merging Individual models with BMA results...")
    merged1 = pd.merge(merged_individual_df, bma_df, on='Station_ID', how='inner')
    log_message(f"    ...Shape after merging with BMA: {merged1.shape}")

    log_message("\n    Merging result with Coordinates...")
    final_df = pd.merge(merged1, coords_df, on='Station_ID', how='inner')
    log_message(f"    ...Shape after merging with Coords: {final_df.shape}")

    if final_df.empty:
        log_message("\n  [FATAL ERROR] FINAL MERGE RESULTED IN AN EMPTY DATAFRAME!")
        # ... (error diagnostics) ...
        continue

    final_df['Forecast_Horizon'] = f"{fh}-Day"
    all_horizon_data.append(final_df)
    log_message(f"  SUCCESS! Final data for {fh}-day prepared.")
    log_message("-" * 50)


# --- 5. CREATE THE COMPARISON PLOT (with improved colormaps) ---
log_message(f"\n{'='*25} Generating Final Plot {'='*25}")
if not all_horizon_data:
    log_message("[FATAL] No data was successfully processed for ANY horizon. Cannot generate plot.")
else:
    master_df = pd.concat(all_horizon_data, ignore_index=True)
    master_df.dropna(subset=['NSE_BMA', 'NSE_Range', 'gauge_lat', 'gauge_lon'], inplace=True)
    log_message(f"  Successfully created master DataFrame for plotting. Total records: {len(master_df)}")
    
    fig, axes = plt.subplots(
        len(forecast_horizons), 2,
        figsize=(10, 11),
        subplot_kw={'projection': ccrs.PlateCarree()},
        constrained_layout=True
    )
    # fig.suptitle('Spatial Analysis of BMA Performance vs. Model Uncertainty', fontsize=16, y=1.03)

    for i, fh_str in enumerate([f"{fh}-Day" for fh in forecast_horizons]):
        fh_df = master_df[master_df['Forecast_Horizon'] == fh_str]
        if fh_df.empty:
            axes[i, 0].set_visible(False); axes[i, 1].set_visible(False)
            continue
        
        # --- Left Plot: BMA NSE Performance (using 'viridis') ---
        ax1 = axes[i, 0]
        ax1.set_extent([-125, -66.5, 24, 49.5], crs=ccrs.PlateCarree())
        ax1.add_feature(cfeature.COASTLINE); ax1.add_feature(cfeature.STATES, linestyle=':', linewidth=0.5)
        scatter1 = ax1.scatter(
            fh_df['gauge_lon'], fh_df['gauge_lat'], c=fh_df['NSE_BMA'],
            # ######################
            # ### CHANGE IS HERE ###
            # ######################
            cmap='viridis', 
            vmin=-0.5, vmax=1, s=20, edgecolor='k', alpha=0.9
        )
        cbar1 = fig.colorbar(scatter1, ax=ax1, orientation='vertical', pad=0.02, shrink=0.6)
        cbar1.set_label('NSE (BMA)'); ax1.set_title(f'{fh_str} NSE Forecast')

        # --- Right Plot: Uncertainty (NSE Range) (using 'plasma') ---
        ax2 = axes[i, 1]
        ax2.set_extent([-125, -66.5, 24, 49.5], crs=ccrs.PlateCarree())
        ax2.add_feature(cfeature.COASTLINE); ax2.add_feature(cfeature.STATES, linestyle=':', linewidth=0.5)
        
        vmax_val = fh_df['NSE_Range'].quantile(0.95) 
        scatter2 = ax2.scatter(
            fh_df['gauge_lon'], fh_df['gauge_lat'], c=fh_df['NSE_Range'],
            # ######################
            # ### CHANGE IS HERE ###
            # ######################
            cmap='plasma', 
            vmin=0, vmax=vmax_val, s=20, edgecolor='k', alpha=0.9
        )
        cbar2 = fig.colorbar(scatter2, ax=ax2, orientation='vertical', pad=0.02, shrink=0.6)
        cbar2.set_label('Uncertainty (NSE Range)'); ax2.set_title(f'{fh_str} Forecast Uncertainty')

    # Add gridlines (unchanged)
    for i in range(len(forecast_horizons)):
        for j in range(2):
            if i >= axes.shape[0] or not axes[i,j].get_visible(): continue
            ax = axes[i, j]
            gl = ax.gridlines(draw_labels=True, linestyle='--', alpha=0.5)
            gl.top_labels = False; gl.right_labels = False
            gl.left_labels = (j == 0); gl.bottom_labels = (i == len(forecast_horizons) - 1)
    
    # Save with a new name to avoid overwriting
    output_plot_path = os.path.join(base_dir, 'spatial_performance_vs_uncertainty_viridis.png')
    plt.savefig(output_plot_path, dpi=300, bbox_inches='tight')
    log_message(f"\nSUCCESS! Comparison plot with improved colormaps saved to {output_plot_path}")
    
    plt.show()


================================================
File: .ipynb_checkpoints/submit_jobs-checkpoint.sh
================================================
#!/bin/bash
#SBATCH -p htc
#SBATCH -N 1            # number of nodes
#SBATCH -c 16           # number of cores 
#SBATCH --gres=gpu:a100:1    # request 1 GPU
#SBATCH -t 0-04:00:00   # time in d-hh:mm:ss
#SBATCH --mem=128G      # memory for all cores in GB
#SBATCH -q public       # QOS
#SBATCH -o slurm.%j.out # file to save job's STDOUT (%j = JobId)
#SBATCH -e slurm.%j.err # file to save job's STDERR (%j = JobId)

# Load required modules for job's environment
source activate tf-gpu

# Run the Python script with the specific config file passed as an argument
python /scratch/kdahal3/camels_losses/MAIN2.py

